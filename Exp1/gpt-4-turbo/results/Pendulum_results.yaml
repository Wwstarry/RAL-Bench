project_name: Pendulum
task_file: D:\桌面\RealAppCodeBench_generic_eval\tasks\Pendulum\pendulum.yaml
generated_repo: D:\桌面\RealAppCodeBench_generic_eval\generation\Pendulum
timestamp: '2026-01-01 13:21:36'
functional_score: 0.0
non_functional_score: 0.4
non_functional_subscores:
  maintainability: 0.0
  security: 1.0
  robustness: 1.0
  performance: 0.0
  resource: 0.0
non_functional_weights:
  maintainability: 0.36
  security: 0.24
  robustness: 0.16
  performance: 0.12
  resource: 0.12
results:
  functional:
    returncode: 5
    stdout: '

      1 skipped in 0.13s

      '
    elapsed_time_s: 1.554255
    avg_memory_mb: 30.84
    avg_cpu_percent: 102.2
    passed: 0
    failed: 0
    skipped: 1
    total: 1
    score_inputs_passed: 0
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 5
    score_inputs_failed_suite: true
    score_inputs_baseline_keys:
    - functional_suite_time_s
    - functional_tests_total
  performance:
    returncode: 2
    stdout: "\n=================================== ERRORS ====================================\n\
      _____________ ERROR collecting tests/Pendulum/performance_test.py _____________\n\
      ImportError while importing test module 'D:\\桌面\\RealAppCodeBench_generic_eval\\\
      tests\\Pendulum\\performance_test.py'.\nHint: make sure your test modules/packages\
      \ have valid Python names.\nTraceback:\nC:\\Users\\86152\\AppData\\Local\\Programs\\\
      Python\\Python39\\lib\\importlib\\__init__.py:127: in import_module\n    return\
      \ _bootstrap._gcd_import(name[level:], package, level)\ntests\\Pendulum\\performance_test.py:30:\
      \ in <module>\n    import pendulum  # type: ignore  # noqa: E402\nE   ModuleNotFoundError:\
      \ No module named 'pendulum'\n=========================== short test summary\
      \ info ===========================\nERROR tests/Pendulum/performance_test.py\n\
      !!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!\n\
      1 error in 0.56s\n"
    elapsed_time_s: 1.940042
    avg_memory_mb: 35.04
    avg_cpu_percent: 96.6
    passed: 0
    failed: 0
    skipped: 0
    total: 1
    score_inputs_passed: 0
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 2
    score_inputs_failed_suite: true
    score_inputs_baseline_keys:
    - performance_suite_time_s
    - performance_tests_total
    score_inputs_baseline_time_s: 2.322455
    score_inputs_actual_time_s: 1.940042
  resource:
    returncode: 2
    stdout: "\n=================================== ERRORS ====================================\n\
      ______________ ERROR collecting tests/Pendulum/resource_test.py _______________\n\
      ImportError while importing test module 'D:\\桌面\\RealAppCodeBench_generic_eval\\\
      tests\\Pendulum\\resource_test.py'.\nHint: make sure your test modules/packages\
      \ have valid Python names.\nTraceback:\nC:\\Users\\86152\\AppData\\Local\\Programs\\\
      Python\\Python39\\lib\\importlib\\__init__.py:127: in import_module\n    return\
      \ _bootstrap._gcd_import(name[level:], package, level)\ntests\\Pendulum\\resource_test.py:29:\
      \ in <module>\n    import pendulum  # type: ignore  # noqa: E402\nE   ModuleNotFoundError:\
      \ No module named 'pendulum'\n=========================== short test summary\
      \ info ===========================\nERROR tests/Pendulum/resource_test.py\n\
      !!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!\n\
      1 error in 0.55s\n"
    elapsed_time_s: 1.852823
    avg_memory_mb: 34.84
    avg_cpu_percent: 98.2
    passed: 0
    failed: 0
    skipped: 0
    total: 1
    score_inputs_passed: 0
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 2
    score_inputs_failed_suite: true
    score_inputs_baseline_keys:
    - avg_cpu_percent
    - avg_memory_mb
    - resource_suite_time_s
    - resource_tests_total
    score_inputs_baseline_mem_mb: 32.89
    score_inputs_baseline_cpu_pct: 101.3
    score_inputs_actual_mem_mb: 34.84
    score_inputs_actual_cpu_pct: 98.2
  robustness:
    returncode: 0
    stdout: '.                                                                        [100%]

      1 passed in 0.51s

      '
    elapsed_time_s: 1.832684
    avg_memory_mb: 34.6
    avg_cpu_percent: 97.3
    passed: 1
    failed: 0
    skipped: 0
    total: 1
    score_inputs_passed: 1
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 0
    score_inputs_failed_suite: false
    score_inputs_baseline_keys:
    - robustness_suite_time_s
    - robustness_tests_total
  security:
    returncode: 0
    stdout: 'SECURITY_METRICS high_risk_count=0.0 files_scanned=0.0 total_loc=0.0

      .

      1 passed in 0.14s

      '
    elapsed_time_s: 1.758356
    avg_memory_mb: 30.98
    avg_cpu_percent: 96.2
    passed: 1
    failed: 0
    skipped: 0
    total: 1
    metrics:
      high_risk_count: 0.0
      files_scanned: 0.0
      total_loc: 0.0
    score_inputs_passed: 1
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 0
    score_inputs_failed_suite: false
    score_inputs_baseline_keys:
    - metrics
    - security_suite_time_s
    - security_tests_total
    score_inputs_baseline_high_risk_count: 0.0
    score_inputs_generated_high_risk_count: 0.0
  maintainability:
    returncode: 1
    stdout: "MAINT_METRICS mi_min=0.0000 files_scanned=0.0 total_loc=0.0 max_cc=0.0\n\
      F\n================================== FAILURES ===================================\n\
      _____________________ test_maintainability_metrics_smoke ______________________\n\
      \n    def test_maintainability_metrics_smoke() -> None:\n        root = _repo_root()\n\
      \    \n        # Optionally focus on a package subdir if provided and exists;\
      \ otherwise scan repo root.\n        pkg = os.environ.get(PKG_NAME_ENV, \"\"\
      ).strip()\n        scan_root = root\n        if pkg:\n            cand = root\
      \ / pkg\n            if cand.exists() and cand.is_dir():\n                scan_root\
      \ = cand\n    \n        files, summary = _scan_repo(scan_root)\n    \n     \
      \   # Print in a single line for easy parsing by measure_reference/measure_generated\n\
      \        print(\n            \"MAINT_METRICS \"\n            f\"mi_min={summary['mi_min']:.4f}\
      \ \"\n            f\"files_scanned={summary['files_scanned']:.1f} \"\n     \
      \       f\"total_loc={summary['total_loc']:.1f} \"\n            f\"max_cc={summary['max_cc']:.1f}\"\
      \n        )\n    \n        # Keep as a smoke test: must scan at least one python\
      \ file in real repos.\n>       assert summary[\"files_scanned\"] >= 1.0, f\"\
      No python files scanned under: {scan_root}\"\nE       AssertionError: No python\
      \ files scanned under: D:\\桌面\\RealAppCodeBench_generic_eval\\generation\\Pendulum\n\
      E       assert 0.0 >= 1.0\n\ntests\\_generic\\maintainability_test.py:380: AssertionError\n\
      =========================== short test summary info ===========================\n\
      FAILED tests/_generic/maintainability_test.py::test_maintainability_metrics_smoke\n\
      1 failed in 0.41s\n"
    elapsed_time_s: 1.872259
    avg_memory_mb: 31.95
    avg_cpu_percent: 99.2
    passed: 0
    failed: 1
    skipped: 0
    total: 1
    metrics:
      mi_min: 0.0
      files_scanned: 0.0
      total_loc: 0.0
      max_cc: 0.0
    score_inputs_passed: 0
    score_inputs_failed: 1
    score_inputs_total: 1
    score_inputs_returncode: 1
    score_inputs_failed_suite: true
    score_inputs_baseline_keys:
    - maintainability_suite_time_s
    - maintainability_tests_total
    - metrics
    score_inputs_baseline_mi_min: 0.029
    score_inputs_generated_mi_min: 0.0
    score_inputs_ratio_g_over_b: 0.0
baseline_metrics:
  performance:
    performance_suite_time_s: 2.322455
    performance_tests_total: 1
  resource:
    resource_suite_time_s: 1.330804
    resource_tests_total: 1
    avg_memory_mb: 32.89
    avg_cpu_percent: 101.3
  functional:
    functional_suite_time_s: 1.368165
    functional_tests_total: 13
  robustness:
    robustness_suite_time_s: 1.390624
    robustness_tests_total: 1
  security:
    security_suite_time_s: 1.430329
    security_tests_total: 1
    metrics:
      high_risk_count: 0.0
      files_scanned: 119.0
      total_loc: 11757.0
  maintainability:
    maintainability_suite_time_s: 3.35882
    maintainability_tests_total: 1
    metrics:
      mi_min: 0.029
      files_scanned: 172.0
      total_loc: 19627.0
      max_cc: 44.0
pytest_logs_dir: D:\桌面\RealAppCodeBench_generic_eval\results\Pendulum\pytest_logs

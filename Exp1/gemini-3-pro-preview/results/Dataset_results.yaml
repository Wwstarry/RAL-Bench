project_name: Dataset
task_file: D:\桌面\RealAppCodeBench_generic_eval\tasks\Dataset\dataset.yaml
generated_repo: D:\桌面\RealAppCodeBench_generic_eval\generation\Dataset
timestamp: '2025-12-31 16:10:02'
functional_score: 0.0
non_functional_score: 0.655
non_functional_subscores:
  maintainability: 0.7084
  security: 1.0
  robustness: 1.0
  performance: 0.0
  resource: 0.0
non_functional_weights:
  maintainability: 0.36
  security: 0.24
  robustness: 0.16
  performance: 0.12
  resource: 0.12
results:
  functional:
    returncode: 1
    stdout: "FFFFFFFFFFF                                                         \
      \     [100%]\n================================== FAILURES ===================================\n\
      ______________________ test_insert_and_query_basic_rows _______________________\n\
      \n    def test_insert_and_query_basic_rows() -> None:\n        db = create_in_memory_db()\n\
      \        table = db[\"users\"]\n    \n>       table.insert({\"name\": \"Alice\"\
      , \"age\": 30, \"country\": \"DE\"})\n\ntests\\Dataset\\functional_test.py:142:\
      \ \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _\ngeneration\\Dataset\\dataset\\table.py:100: in insert\n    return\
      \ self.insert_many([row])\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <dataset.table.Table object at 0x000002BA56113790>\n\
      rows = [{'age': 30, 'country': 'DE', 'name': 'Alice'}], chunk_size = None\n\n\
      \    def insert_many(self, rows, chunk_size=None):\n        \"\"\"\n       \
      \ Insert multiple rows.\n        \"\"\"\n        if not rows:\n            return\n\
      \    \n        # Normalize rows to dicts\n        rows = [dict(r) for r in rows]\n\
      \    \n        # 1. Ensure table exists based on the first row (or union of\
      \ keys if we were fancy,\n        #    but standard dataset often just looks\
      \ at what's coming in).\n        #    To be safe, we check schema against all\
      \ keys in the batch.\n        all_keys = set()\n        for r in rows:\n   \
      \         all_keys.update(r.keys())\n    \n        # Create a dummy row with\
      \ all keys to ensure schema\n        dummy_schema_row = {k: rows[0].get(k) for\
      \ k in all_keys}\n    \n        with self.db.lock:\n            self._ensure_table(dummy_schema_row)\n\
      \            self._ensure_columns(dummy_schema_row)\n    \n            # 2.\
      \ Construct Insert SQL\n            # We use the first row to determine the\
      \ parameterized query structure,\n            # but since we ensured columns\
      \ for ALL keys, we can just use all_keys.\n            keys = list(all_keys)\n\
      \            placeholders = [f':{k}' for k in keys]\n            columns_sql\
      \ = \", \".join(f'\"{k}\"' for k in keys)\n            values_sql = \", \".join(placeholders)\n\
      \    \n            sql = f'INSERT INTO \"{self.name}\" ({columns_sql}) VALUES\
      \ ({values_sql})'\n    \n            cursor = self.db._conn.cursor()\n     \
      \       try:\n>               cursor.executemany(sql, rows)\nE             \
      \  sqlite3.OperationalError: no such table: users\n\ngeneration\\Dataset\\dataset\\\
      table.py:138: OperationalError\n_______________________ test_update_upsert_and_indexes\
      \ ________________________\n\n    def test_update_upsert_and_indexes() -> None:\n\
      \        db = create_in_memory_db()\n        table = db[\"accounts\"]\n    \n\
      \        rows = [\n            {\"account_id\": 1, \"owner\": \"Alice\", \"\
      balance\": 100.0, \"currency\": \"EUR\"},\n            {\"account_id\": 2, \"\
      owner\": \"Bob\", \"balance\": 250.0, \"currency\": \"USD\"},\n        ]\n>\
      \       table.insert_many(rows)\n\ntests\\Dataset\\functional_test.py:175: \n\
      _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _\n\nself = <dataset.table.Table object at 0x000002BA5617DB50>\nrows = [{'account_id':\
      \ 1, 'balance': 100.0, 'currency': 'EUR', 'owner': 'Alice'}, {'account_id':\
      \ 2, 'balance': 250.0, 'currency': 'USD', 'owner': 'Bob'}]\nchunk_size = None\n\
      \n    def insert_many(self, rows, chunk_size=None):\n        \"\"\"\n      \
      \  Insert multiple rows.\n        \"\"\"\n        if not rows:\n           \
      \ return\n    \n        # Normalize rows to dicts\n        rows = [dict(r) for\
      \ r in rows]\n    \n        # 1. Ensure table exists based on the first row\
      \ (or union of keys if we were fancy,\n        #    but standard dataset often\
      \ just looks at what's coming in).\n        #    To be safe, we check schema\
      \ against all keys in the batch.\n        all_keys = set()\n        for r in\
      \ rows:\n            all_keys.update(r.keys())\n    \n        # Create a dummy\
      \ row with all keys to ensure schema\n        dummy_schema_row = {k: rows[0].get(k)\
      \ for k in all_keys}\n    \n        with self.db.lock:\n            self._ensure_table(dummy_schema_row)\n\
      \            self._ensure_columns(dummy_schema_row)\n    \n            # 2.\
      \ Construct Insert SQL\n            # We use the first row to determine the\
      \ parameterized query structure,\n            # but since we ensured columns\
      \ for ALL keys, we can just use all_keys.\n            keys = list(all_keys)\n\
      \            placeholders = [f':{k}' for k in keys]\n            columns_sql\
      \ = \", \".join(f'\"{k}\"' for k in keys)\n            values_sql = \", \".join(placeholders)\n\
      \    \n            sql = f'INSERT INTO \"{self.name}\" ({columns_sql}) VALUES\
      \ ({values_sql})'\n    \n            cursor = self.db._conn.cursor()\n     \
      \       try:\n>               cursor.executemany(sql, rows)\nE             \
      \  sqlite3.OperationalError: no such table: accounts\n\ngeneration\\Dataset\\\
      dataset\\table.py:138: OperationalError\n____________________ test_transactions_commit_and_rollback\
      \ ____________________\n\ntmp_path = WindowsPath('C:/Users/86152/AppData/Local/Temp/pytest-of-86152/pytest-308/test_transactions_commit_and_r0')\n\
      \n    def test_transactions_commit_and_rollback(tmp_path: Path) -> None:\n \
      \       db_path = tmp_path / \"tx_sample.db\"\n        db_url = \"sqlite:///%s\"\
      \ % str(db_path)\n        db = dataset.connect(db_url)\n        table = db[\"\
      events\"]\n    \n        db.begin()\n>       table.insert({\"name\": \"committed\"\
      , \"category\": \"ok\"})\n\ntests\\Dataset\\functional_test.py:203: \n_ _ _\
      \ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\
      generation\\Dataset\\dataset\\table.py:100: in insert\n    return self.insert_many([row])\n\
      _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _\n\nself = <dataset.table.Table object at 0x000002BA56113E20>\nrows = [{'category':\
      \ 'ok', 'name': 'committed'}], chunk_size = None\n\n    def insert_many(self,\
      \ rows, chunk_size=None):\n        \"\"\"\n        Insert multiple rows.\n \
      \       \"\"\"\n        if not rows:\n            return\n    \n        # Normalize\
      \ rows to dicts\n        rows = [dict(r) for r in rows]\n    \n        # 1.\
      \ Ensure table exists based on the first row (or union of keys if we were fancy,\n\
      \        #    but standard dataset often just looks at what's coming in).\n\
      \        #    To be safe, we check schema against all keys in the batch.\n \
      \       all_keys = set()\n        for r in rows:\n            all_keys.update(r.keys())\n\
      \    \n        # Create a dummy row with all keys to ensure schema\n       \
      \ dummy_schema_row = {k: rows[0].get(k) for k in all_keys}\n    \n        with\
      \ self.db.lock:\n            self._ensure_table(dummy_schema_row)\n        \
      \    self._ensure_columns(dummy_schema_row)\n    \n            # 2. Construct\
      \ Insert SQL\n            # We use the first row to determine the parameterized\
      \ query structure,\n            # but since we ensured columns for ALL keys,\
      \ we can just use all_keys.\n            keys = list(all_keys)\n           \
      \ placeholders = [f':{k}' for k in keys]\n            columns_sql = \", \".join(f'\"\
      {k}\"' for k in keys)\n            values_sql = \", \".join(placeholders)\n\
      \    \n            sql = f'INSERT INTO \"{self.name}\" ({columns_sql}) VALUES\
      \ ({values_sql})'\n    \n            cursor = self.db._conn.cursor()\n     \
      \       try:\n>               cursor.executemany(sql, rows)\nE             \
      \  sqlite3.OperationalError: no such table: events\n\ngeneration\\Dataset\\\
      dataset\\table.py:138: OperationalError\n___________________ test_insert_many_returns_ids_and_count\
      \ ____________________\n\n    def test_insert_many_returns_ids_and_count() ->\
      \ None:\n        db = create_in_memory_db()\n        table = db[\"items\"]\n\
      \    \n        rows = [{\"name\": \"A\"}, {\"name\": \"B\"}, {\"name\": \"C\"\
      }]\n>       ret = table.insert_many(rows)\n\ntests\\Dataset\\functional_test.py:223:\
      \ \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _\n\nself = <dataset.table.Table object at 0x000002BA5616BDF0>\nrows\
      \ = [{'name': 'A'}, {'name': 'B'}, {'name': 'C'}], chunk_size = None\n\n   \
      \ def insert_many(self, rows, chunk_size=None):\n        \"\"\"\n        Insert\
      \ multiple rows.\n        \"\"\"\n        if not rows:\n            return\n\
      \    \n        # Normalize rows to dicts\n        rows = [dict(r) for r in rows]\n\
      \    \n        # 1. Ensure table exists based on the first row (or union of\
      \ keys if we were fancy,\n        #    but standard dataset often just looks\
      \ at what's coming in).\n        #    To be safe, we check schema against all\
      \ keys in the batch.\n        all_keys = set()\n        for r in rows:\n   \
      \         all_keys.update(r.keys())\n    \n        # Create a dummy row with\
      \ all keys to ensure schema\n        dummy_schema_row = {k: rows[0].get(k) for\
      \ k in all_keys}\n    \n        with self.db.lock:\n            self._ensure_table(dummy_schema_row)\n\
      \            self._ensure_columns(dummy_schema_row)\n    \n            # 2.\
      \ Construct Insert SQL\n            # We use the first row to determine the\
      \ parameterized query structure,\n            # but since we ensured columns\
      \ for ALL keys, we can just use all_keys.\n            keys = list(all_keys)\n\
      \            placeholders = [f':{k}' for k in keys]\n            columns_sql\
      \ = \", \".join(f'\"{k}\"' for k in keys)\n            values_sql = \", \".join(placeholders)\n\
      \    \n            sql = f'INSERT INTO \"{self.name}\" ({columns_sql}) VALUES\
      \ ({values_sql})'\n    \n            cursor = self.db._conn.cursor()\n     \
      \       try:\n>               cursor.executemany(sql, rows)\nE             \
      \  sqlite3.OperationalError: no such table: items\n\ngeneration\\Dataset\\dataset\\\
      table.py:138: OperationalError\n_____________________ test_find_one_missing_returns_none\
      \ ______________________\n\n    def test_find_one_missing_returns_none() ->\
      \ None:\n        db = create_in_memory_db()\n        table = db[\"t\"]\n>  \
      \     table.insert({\"name\": \"only\"})\n\ntests\\Dataset\\functional_test.py:237:\
      \ \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _\ngeneration\\Dataset\\dataset\\table.py:100: in insert\n    return\
      \ self.insert_many([row])\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <dataset.table.Table object at 0x000002BA560D5E80>\n\
      rows = [{'name': 'only'}], chunk_size = None\n\n    def insert_many(self, rows,\
      \ chunk_size=None):\n        \"\"\"\n        Insert multiple rows.\n       \
      \ \"\"\"\n        if not rows:\n            return\n    \n        # Normalize\
      \ rows to dicts\n        rows = [dict(r) for r in rows]\n    \n        # 1.\
      \ Ensure table exists based on the first row (or union of keys if we were fancy,\n\
      \        #    but standard dataset often just looks at what's coming in).\n\
      \        #    To be safe, we check schema against all keys in the batch.\n \
      \       all_keys = set()\n        for r in rows:\n            all_keys.update(r.keys())\n\
      \    \n        # Create a dummy row with all keys to ensure schema\n       \
      \ dummy_schema_row = {k: rows[0].get(k) for k in all_keys}\n    \n        with\
      \ self.db.lock:\n            self._ensure_table(dummy_schema_row)\n        \
      \    self._ensure_columns(dummy_schema_row)\n    \n            # 2. Construct\
      \ Insert SQL\n            # We use the first row to determine the parameterized\
      \ query structure,\n            # but since we ensured columns for ALL keys,\
      \ we can just use all_keys.\n            keys = list(all_keys)\n           \
      \ placeholders = [f':{k}' for k in keys]\n            columns_sql = \", \".join(f'\"\
      {k}\"' for k in keys)\n            values_sql = \", \".join(placeholders)\n\
      \    \n            sql = f'INSERT INTO \"{self.name}\" ({columns_sql}) VALUES\
      \ ({values_sql})'\n    \n            cursor = self.db._conn.cursor()\n     \
      \       try:\n>               cursor.executemany(sql, rows)\nE             \
      \  sqlite3.OperationalError: no such table: t\n\ngeneration\\Dataset\\dataset\\\
      table.py:138: OperationalError\n_______________________ test_find_order_by_limit_offset\
      \ _______________________\n\n    def test_find_order_by_limit_offset() -> None:\n\
      \        db = create_in_memory_db()\n        table = db[\"nums\"]\n        for\
      \ i in range(10):\n>           table.insert({\"n\": i})\n\ntests\\Dataset\\\
      functional_test.py:246: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ngeneration\\Dataset\\dataset\\table.py:100:\
      \ in insert\n    return self.insert_many([row])\n_ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <dataset.table.Table\
      \ object at 0x000002BA5617D700>, rows = [{'n': 0}]\nchunk_size = None\n\n  \
      \  def insert_many(self, rows, chunk_size=None):\n        \"\"\"\n        Insert\
      \ multiple rows.\n        \"\"\"\n        if not rows:\n            return\n\
      \    \n        # Normalize rows to dicts\n        rows = [dict(r) for r in rows]\n\
      \    \n        # 1. Ensure table exists based on the first row (or union of\
      \ keys if we were fancy,\n        #    but standard dataset often just looks\
      \ at what's coming in).\n        #    To be safe, we check schema against all\
      \ keys in the batch.\n        all_keys = set()\n        for r in rows:\n   \
      \         all_keys.update(r.keys())\n    \n        # Create a dummy row with\
      \ all keys to ensure schema\n        dummy_schema_row = {k: rows[0].get(k) for\
      \ k in all_keys}\n    \n        with self.db.lock:\n            self._ensure_table(dummy_schema_row)\n\
      \            self._ensure_columns(dummy_schema_row)\n    \n            # 2.\
      \ Construct Insert SQL\n            # We use the first row to determine the\
      \ parameterized query structure,\n            # but since we ensured columns\
      \ for ALL keys, we can just use all_keys.\n            keys = list(all_keys)\n\
      \            placeholders = [f':{k}' for k in keys]\n            columns_sql\
      \ = \", \".join(f'\"{k}\"' for k in keys)\n            values_sql = \", \".join(placeholders)\n\
      \    \n            sql = f'INSERT INTO \"{self.name}\" ({columns_sql}) VALUES\
      \ ({values_sql})'\n    \n            cursor = self.db._conn.cursor()\n     \
      \       try:\n>               cursor.executemany(sql, rows)\nE             \
      \  sqlite3.OperationalError: no such table: nums\n\ngeneration\\Dataset\\dataset\\\
      table.py:138: OperationalError\n___________________ test_table_all_iteration_and_row_shape\
      \ ____________________\n\n    def test_table_all_iteration_and_row_shape() ->\
      \ None:\n        db = create_in_memory_db()\n        table = db[\"people\"]\n\
      >       table.insert({\"name\": \"Alice\", \"age\": 30})\n\ntests\\Dataset\\\
      functional_test.py:255: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ngeneration\\Dataset\\dataset\\table.py:100:\
      \ in insert\n    return self.insert_many([row])\n_ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <dataset.table.Table\
      \ object at 0x000002BA5617EA90>\nrows = [{'age': 30, 'name': 'Alice'}], chunk_size\
      \ = None\n\n    def insert_many(self, rows, chunk_size=None):\n        \"\"\"\
      \n        Insert multiple rows.\n        \"\"\"\n        if not rows:\n    \
      \        return\n    \n        # Normalize rows to dicts\n        rows = [dict(r)\
      \ for r in rows]\n    \n        # 1. Ensure table exists based on the first\
      \ row (or union of keys if we were fancy,\n        #    but standard dataset\
      \ often just looks at what's coming in).\n        #    To be safe, we check\
      \ schema against all keys in the batch.\n        all_keys = set()\n        for\
      \ r in rows:\n            all_keys.update(r.keys())\n    \n        # Create\
      \ a dummy row with all keys to ensure schema\n        dummy_schema_row = {k:\
      \ rows[0].get(k) for k in all_keys}\n    \n        with self.db.lock:\n    \
      \        self._ensure_table(dummy_schema_row)\n            self._ensure_columns(dummy_schema_row)\n\
      \    \n            # 2. Construct Insert SQL\n            # We use the first\
      \ row to determine the parameterized query structure,\n            # but since\
      \ we ensured columns for ALL keys, we can just use all_keys.\n            keys\
      \ = list(all_keys)\n            placeholders = [f':{k}' for k in keys]\n   \
      \         columns_sql = \", \".join(f'\"{k}\"' for k in keys)\n            values_sql\
      \ = \", \".join(placeholders)\n    \n            sql = f'INSERT INTO \"{self.name}\"\
      \ ({columns_sql}) VALUES ({values_sql})'\n    \n            cursor = self.db._conn.cursor()\n\
      \            try:\n>               cursor.executemany(sql, rows)\nE        \
      \       sqlite3.OperationalError: no such table: people\n\ngeneration\\Dataset\\\
      dataset\\table.py:138: OperationalError\n_______________________ test_delete_and_clear_all_rows\
      \ ________________________\n\n    def test_delete_and_clear_all_rows() -> None:\n\
      \        \"\"\"\n        Older dataset.Table may not expose truncate().\n  \
      \      Clear a table and end at 0 rows without relying on result iteration for\
      \ DML.\n        \"\"\"\n        db = create_in_memory_db()\n        table =\
      \ db[\"logs\"]\n>       table.insert_many([{\"kind\": \"a\"}, {\"kind\": \"\
      b\"}, {\"kind\": \"b\"}])\n\ntests\\Dataset\\functional_test.py:270: \n_ _ _\
      \ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\
      \nself = <dataset.table.Table object at 0x000002BA560D15B0>\nrows = [{'kind':\
      \ 'a'}, {'kind': 'b'}, {'kind': 'b'}], chunk_size = None\n\n    def insert_many(self,\
      \ rows, chunk_size=None):\n        \"\"\"\n        Insert multiple rows.\n \
      \       \"\"\"\n        if not rows:\n            return\n    \n        # Normalize\
      \ rows to dicts\n        rows = [dict(r) for r in rows]\n    \n        # 1.\
      \ Ensure table exists based on the first row (or union of keys if we were fancy,\n\
      \        #    but standard dataset often just looks at what's coming in).\n\
      \        #    To be safe, we check schema against all keys in the batch.\n \
      \       all_keys = set()\n        for r in rows:\n            all_keys.update(r.keys())\n\
      \    \n        # Create a dummy row with all keys to ensure schema\n       \
      \ dummy_schema_row = {k: rows[0].get(k) for k in all_keys}\n    \n        with\
      \ self.db.lock:\n            self._ensure_table(dummy_schema_row)\n        \
      \    self._ensure_columns(dummy_schema_row)\n    \n            # 2. Construct\
      \ Insert SQL\n            # We use the first row to determine the parameterized\
      \ query structure,\n            # but since we ensured columns for ALL keys,\
      \ we can just use all_keys.\n            keys = list(all_keys)\n           \
      \ placeholders = [f':{k}' for k in keys]\n            columns_sql = \", \".join(f'\"\
      {k}\"' for k in keys)\n            values_sql = \", \".join(placeholders)\n\
      \    \n            sql = f'INSERT INTO \"{self.name}\" ({columns_sql}) VALUES\
      \ ({values_sql})'\n    \n            cursor = self.db._conn.cursor()\n     \
      \       try:\n>               cursor.executemany(sql, rows)\nE             \
      \  sqlite3.OperationalError: no such table: logs\n\ngeneration\\Dataset\\dataset\\\
      table.py:138: OperationalError\n___________________ test_drop_table_removes_from_db_tables\
      \ ____________________\n\n    def test_drop_table_removes_from_db_tables() ->\
      \ None:\n        db = create_in_memory_db()\n        table = db[\"to_drop\"\
      ]\n>       table.insert({\"x\": 1})\n\ntests\\Dataset\\functional_test.py:299:\
      \ \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _\ngeneration\\Dataset\\dataset\\table.py:100: in insert\n    return\
      \ self.insert_many([row])\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <dataset.table.Table object at 0x000002BA561879A0>,\
      \ rows = [{'x': 1}]\nchunk_size = None\n\n    def insert_many(self, rows, chunk_size=None):\n\
      \        \"\"\"\n        Insert multiple rows.\n        \"\"\"\n        if not\
      \ rows:\n            return\n    \n        # Normalize rows to dicts\n     \
      \   rows = [dict(r) for r in rows]\n    \n        # 1. Ensure table exists based\
      \ on the first row (or union of keys if we were fancy,\n        #    but standard\
      \ dataset often just looks at what's coming in).\n        #    To be safe, we\
      \ check schema against all keys in the batch.\n        all_keys = set()\n  \
      \      for r in rows:\n            all_keys.update(r.keys())\n    \n       \
      \ # Create a dummy row with all keys to ensure schema\n        dummy_schema_row\
      \ = {k: rows[0].get(k) for k in all_keys}\n    \n        with self.db.lock:\n\
      \            self._ensure_table(dummy_schema_row)\n            self._ensure_columns(dummy_schema_row)\n\
      \    \n            # 2. Construct Insert SQL\n            # We use the first\
      \ row to determine the parameterized query structure,\n            # but since\
      \ we ensured columns for ALL keys, we can just use all_keys.\n            keys\
      \ = list(all_keys)\n            placeholders = [f':{k}' for k in keys]\n   \
      \         columns_sql = \", \".join(f'\"{k}\"' for k in keys)\n            values_sql\
      \ = \", \".join(placeholders)\n    \n            sql = f'INSERT INTO \"{self.name}\"\
      \ ({columns_sql}) VALUES ({values_sql})'\n    \n            cursor = self.db._conn.cursor()\n\
      \            try:\n>               cursor.executemany(sql, rows)\nE        \
      \       sqlite3.OperationalError: no such table: to_drop\n\ngeneration\\Dataset\\\
      dataset\\table.py:138: OperationalError\n_____________________ test_raw_sql_query_with_parameters\
      \ ______________________\n\ntmp_path = WindowsPath('C:/Users/86152/AppData/Local/Temp/pytest-of-86152/pytest-308/test_raw_sql_query_with_parame0')\n\
      \n    def test_raw_sql_query_with_parameters(tmp_path: Path) -> None:\n    \
      \    db_path = tmp_path / \"param.db\"\n        db = dataset.connect(\"sqlite:///%s\"\
      \ % str(db_path))\n        table = db[\"kv\"]\n>       table.insert_many([{\"\
      k\": \"a\", \"v\": 1}, {\"k\": \"b\", \"v\": 2}])\n\ntests\\Dataset\\functional_test.py:319:\
      \ \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _\n\nself = <dataset.table.Table object at 0x000002BA561B2610>\nrows\
      \ = [{'k': 'a', 'v': 1}, {'k': 'b', 'v': 2}], chunk_size = None\n\n    def insert_many(self,\
      \ rows, chunk_size=None):\n        \"\"\"\n        Insert multiple rows.\n \
      \       \"\"\"\n        if not rows:\n            return\n    \n        # Normalize\
      \ rows to dicts\n        rows = [dict(r) for r in rows]\n    \n        # 1.\
      \ Ensure table exists based on the first row (or union of keys if we were fancy,\n\
      \        #    but standard dataset often just looks at what's coming in).\n\
      \        #    To be safe, we check schema against all keys in the batch.\n \
      \       all_keys = set()\n        for r in rows:\n            all_keys.update(r.keys())\n\
      \    \n        # Create a dummy row with all keys to ensure schema\n       \
      \ dummy_schema_row = {k: rows[0].get(k) for k in all_keys}\n    \n        with\
      \ self.db.lock:\n            self._ensure_table(dummy_schema_row)\n        \
      \    self._ensure_columns(dummy_schema_row)\n    \n            # 2. Construct\
      \ Insert SQL\n            # We use the first row to determine the parameterized\
      \ query structure,\n            # but since we ensured columns for ALL keys,\
      \ we can just use all_keys.\n            keys = list(all_keys)\n           \
      \ placeholders = [f':{k}' for k in keys]\n            columns_sql = \", \".join(f'\"\
      {k}\"' for k in keys)\n            values_sql = \", \".join(placeholders)\n\
      \    \n            sql = f'INSERT INTO \"{self.name}\" ({columns_sql}) VALUES\
      \ ({values_sql})'\n    \n            cursor = self.db._conn.cursor()\n     \
      \       try:\n>               cursor.executemany(sql, rows)\nE             \
      \  sqlite3.OperationalError: no such table: kv\n\ngeneration\\Dataset\\dataset\\\
      table.py:138: OperationalError\n_____________________ test_distinct_returns_unique_values\
      \ _____________________\n\n    def test_distinct_returns_unique_values() ->\
      \ None:\n        db = create_in_memory_db()\n        table = db[\"colors\"]\n\
      >       table.insert_many([{\"c\": \"red\"}, {\"c\": \"red\"}, {\"c\": \"blue\"\
      }])\n\ntests\\Dataset\\functional_test.py:330: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <dataset.table.Table\
      \ object at 0x000002BA560CEC40>\nrows = [{'c': 'red'}, {'c': 'red'}, {'c': 'blue'}],\
      \ chunk_size = None\n\n    def insert_many(self, rows, chunk_size=None):\n \
      \       \"\"\"\n        Insert multiple rows.\n        \"\"\"\n        if not\
      \ rows:\n            return\n    \n        # Normalize rows to dicts\n     \
      \   rows = [dict(r) for r in rows]\n    \n        # 1. Ensure table exists based\
      \ on the first row (or union of keys if we were fancy,\n        #    but standard\
      \ dataset often just looks at what's coming in).\n        #    To be safe, we\
      \ check schema against all keys in the batch.\n        all_keys = set()\n  \
      \      for r in rows:\n            all_keys.update(r.keys())\n    \n       \
      \ # Create a dummy row with all keys to ensure schema\n        dummy_schema_row\
      \ = {k: rows[0].get(k) for k in all_keys}\n    \n        with self.db.lock:\n\
      \            self._ensure_table(dummy_schema_row)\n            self._ensure_columns(dummy_schema_row)\n\
      \    \n            # 2. Construct Insert SQL\n            # We use the first\
      \ row to determine the parameterized query structure,\n            # but since\
      \ we ensured columns for ALL keys, we can just use all_keys.\n            keys\
      \ = list(all_keys)\n            placeholders = [f':{k}' for k in keys]\n   \
      \         columns_sql = \", \".join(f'\"{k}\"' for k in keys)\n            values_sql\
      \ = \", \".join(placeholders)\n    \n            sql = f'INSERT INTO \"{self.name}\"\
      \ ({columns_sql}) VALUES ({values_sql})'\n    \n            cursor = self.db._conn.cursor()\n\
      \            try:\n>               cursor.executemany(sql, rows)\nE        \
      \       sqlite3.OperationalError: no such table: colors\n\ngeneration\\Dataset\\\
      dataset\\table.py:138: OperationalError\n=========================== short test\
      \ summary info ===========================\nFAILED tests/Dataset/functional_test.py::test_insert_and_query_basic_rows\
      \ - s...\nFAILED tests/Dataset/functional_test.py::test_update_upsert_and_indexes\
      \ - sql...\nFAILED tests/Dataset/functional_test.py::test_transactions_commit_and_rollback\n\
      FAILED tests/Dataset/functional_test.py::test_insert_many_returns_ids_and_count\n\
      FAILED tests/Dataset/functional_test.py::test_find_one_missing_returns_none\n\
      FAILED tests/Dataset/functional_test.py::test_find_order_by_limit_offset - sq...\n\
      FAILED tests/Dataset/functional_test.py::test_table_all_iteration_and_row_shape\n\
      FAILED tests/Dataset/functional_test.py::test_delete_and_clear_all_rows - sql...\n\
      FAILED tests/Dataset/functional_test.py::test_drop_table_removes_from_db_tables\n\
      FAILED tests/Dataset/functional_test.py::test_raw_sql_query_with_parameters\n\
      FAILED tests/Dataset/functional_test.py::test_distinct_returns_unique_values\n\
      11 failed in 54.44s\n"
    elapsed_time_s: 55.754882
    avg_memory_mb: 34.57
    avg_cpu_percent: 0.27
    passed: 0
    failed: 11
    skipped: 0
    total: 11
    score_inputs_passed: 0
    score_inputs_failed: 11
    score_inputs_total: 11
    score_inputs_returncode: 1
    score_inputs_failed_suite: true
    score_inputs_baseline_keys:
    - functional_suite_time_s
    - functional_tests_total
  performance:
    returncode: 2
    stdout: "\n=================================== ERRORS ====================================\n\
      _____________ ERROR collecting tests/Dataset/performance_test.py ______________\n\
      tests\\Dataset\\performance_test.py:18: in <module>\n    raise RuntimeError(f\"\
      Unsupported DATASET_TARGET value: {TARGET_ENV}\")\nE   RuntimeError: Unsupported\
      \ DATASET_TARGET value: generated\n=========================== short test summary\
      \ info ===========================\nERROR tests/Dataset/performance_test.py\
      \ - RuntimeError: Unsupported DATASET_T...\n!!!!!!!!!!!!!!!!!!! Interrupted:\
      \ 1 error during collection !!!!!!!!!!!!!!!!!!!!\n1 error in 0.44s\n"
    elapsed_time_s: 1.624613
    avg_memory_mb: 35.84
    avg_cpu_percent: 99.0
    passed: 0
    failed: 0
    skipped: 0
    total: 1
    score_inputs_passed: 0
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 2
    score_inputs_failed_suite: true
    score_inputs_baseline_keys:
    - performance_suite_time_s
    - performance_tests_total
    score_inputs_baseline_time_s: 2.781374
    score_inputs_actual_time_s: 1.624613
  resource:
    returncode: 2
    stdout: "\n=================================== ERRORS ====================================\n\
      _______________ ERROR collecting tests/Dataset/resource_test.py _______________\n\
      tests\\Dataset\\resource_test.py:18: in <module>\n    raise RuntimeError(f\"\
      Unsupported DATASET_TARGET value: {TARGET_ENV}\")\nE   RuntimeError: Unsupported\
      \ DATASET_TARGET value: generated\n=========================== short test summary\
      \ info ===========================\nERROR tests/Dataset/resource_test.py - RuntimeError:\
      \ Unsupported DATASET_TARG...\n!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during\
      \ collection !!!!!!!!!!!!!!!!!!!!\n1 error in 0.45s\n"
    elapsed_time_s: 1.610865
    avg_memory_mb: 35.36
    avg_cpu_percent: 96.9
    passed: 0
    failed: 0
    skipped: 0
    total: 1
    score_inputs_passed: 0
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 2
    score_inputs_failed_suite: true
    score_inputs_baseline_keys:
    - avg_cpu_percent
    - avg_memory_mb
    - resource_suite_time_s
    - resource_tests_total
    score_inputs_baseline_mem_mb: 55.98
    score_inputs_baseline_cpu_pct: 100.6
    score_inputs_actual_mem_mb: 35.36
    score_inputs_actual_cpu_pct: 96.9
  robustness:
    returncode: 0
    stdout: "......                                                              \
      \     [100%]\n============================== warnings summary ===============================\n\
      tests\\Dataset\\robustness_test.py:92\n  D:\\桌面\\RealAppCodeBench_generic_eval\\\
      tests\\Dataset\\robustness_test.py:92: PytestUnknownMarkWarning: Unknown pytest.mark.timeout\
      \ - is this a typo?  You can register custom marks to avoid this warning - for\
      \ details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.timeout(10)\n\
      \ntests\\Dataset\\robustness_test.py:102\n  D:\\桌面\\RealAppCodeBench_generic_eval\\\
      tests\\Dataset\\robustness_test.py:102: PytestUnknownMarkWarning: Unknown pytest.mark.timeout\
      \ - is this a typo?  You can register custom marks to avoid this warning - for\
      \ details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.timeout(10)\n\
      \ntests\\Dataset\\robustness_test.py:120\n  D:\\桌面\\RealAppCodeBench_generic_eval\\\
      tests\\Dataset\\robustness_test.py:120: PytestUnknownMarkWarning: Unknown pytest.mark.timeout\
      \ - is this a typo?  You can register custom marks to avoid this warning - for\
      \ details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.timeout(10)\n\
      \ntests\\Dataset\\robustness_test.py:135\n  D:\\桌面\\RealAppCodeBench_generic_eval\\\
      tests\\Dataset\\robustness_test.py:135: PytestUnknownMarkWarning: Unknown pytest.mark.timeout\
      \ - is this a typo?  You can register custom marks to avoid this warning - for\
      \ details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.timeout(10)\n\
      \ntests\\Dataset\\robustness_test.py:149\n  D:\\桌面\\RealAppCodeBench_generic_eval\\\
      tests\\Dataset\\robustness_test.py:149: PytestUnknownMarkWarning: Unknown pytest.mark.timeout\
      \ - is this a typo?  You can register custom marks to avoid this warning - for\
      \ details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.timeout(10)\n\
      \ntests\\Dataset\\robustness_test.py:163\n  D:\\桌面\\RealAppCodeBench_generic_eval\\\
      tests\\Dataset\\robustness_test.py:163: PytestUnknownMarkWarning: Unknown pytest.mark.timeout\
      \ - is this a typo?  You can register custom marks to avoid this warning - for\
      \ details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.timeout(10)\n\
      \n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n\
      6 passed, 6 warnings in 0.13s\n"
    elapsed_time_s: 1.353675
    avg_memory_mb: 31.78
    avg_cpu_percent: 100.0
    passed: 6
    failed: 0
    skipped: 0
    total: 6
    score_inputs_passed: 6
    score_inputs_failed: 0
    score_inputs_total: 6
    score_inputs_returncode: 0
    score_inputs_failed_suite: false
    score_inputs_baseline_keys:
    - robustness_suite_time_s
    - robustness_tests_total
  security:
    returncode: 0
    stdout: 'SECURITY_METRICS high_risk_count=0.0 files_scanned=3.0 total_loc=349.0

      .

      1 passed in 0.11s

      '
    elapsed_time_s: 1.240168
    avg_memory_mb: 31.78
    avg_cpu_percent: 100.0
    passed: 1
    failed: 0
    skipped: 0
    total: 1
    metrics:
      high_risk_count: 0.0
      files_scanned: 3.0
      total_loc: 349.0
    score_inputs_passed: 1
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 0
    score_inputs_failed_suite: false
    score_inputs_baseline_keys:
    - metrics
    - security_suite_time_s
    - security_tests_total
    score_inputs_baseline_high_risk_count: 0.0
    score_inputs_generated_high_risk_count: 0.0
  maintainability:
    returncode: 0
    stdout: 'MAINT_METRICS mi_min=18.7540 files_scanned=3.0 total_loc=349.0 max_cc=10.0

      .

      1 passed in 0.13s

      '
    elapsed_time_s: 1.242663
    avg_memory_mb: 31.64
    avg_cpu_percent: 97.2
    passed: 1
    failed: 0
    skipped: 0
    total: 1
    metrics:
      mi_min: 18.754
      files_scanned: 3.0
      total_loc: 349.0
      max_cc: 10.0
    score_inputs_passed: 1
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 0
    score_inputs_failed_suite: false
    score_inputs_baseline_keys:
    - maintainability_suite_time_s
    - maintainability_tests_total
    - metrics
    score_inputs_baseline_mi_min: 7.7184
    score_inputs_generated_mi_min: 18.754
    score_inputs_ratio_g_over_b: 2.4297781923714763
baseline_metrics:
  performance:
    performance_suite_time_s: 2.781374
    performance_tests_total: 2
  resource:
    resource_suite_time_s: 2.854474
    resource_tests_total: 2
    avg_memory_mb: 55.98
    avg_cpu_percent: 100.6
  functional:
    functional_suite_time_s: 4.552087
    functional_tests_total: 11
  robustness:
    robustness_suite_time_s: 2.083828
    robustness_tests_total: 6
  security:
    security_suite_time_s: 1.286846
    security_tests_total: 1
    metrics:
      high_risk_count: 0.0
      files_scanned: 6.0
      total_loc: 1131.0
  maintainability:
    maintainability_suite_time_s: 1.311875
    maintainability_tests_total: 1
    metrics:
      mi_min: 7.7184
      files_scanned: 6.0
      total_loc: 1131.0
      max_cc: 16.0
pytest_logs_dir: D:\桌面\RealAppCodeBench_generic_eval\results\Dataset\pytest_logs

project_name: Dataset
task_file: D:\桌面\RAL-Bench\tasks\Dataset\dataset.yaml
generated_repo: D:\桌面\RAL-Bench\generation\claude-4.5-sonnet\Dataset
timestamp: '2026-01-21 22:23:01'
functional_score: 0.5455
non_functional_score: 0.6607
non_functional_subscores:
  maintainability: 0.724
  security: 1.0
  robustness: 1.0
  performance: 0.0
  resource: 0.0
non_functional_weights:
  maintainability: 0.36
  security: 0.24
  robustness: 0.16
  performance: 0.12
  resource: 0.12
results:
  functional:
    returncode: 1
    stdout: "FF...F..F.F                                                         \
      \     [100%]\n================================== FAILURES ===================================\n\
      ______________________ test_insert_and_query_basic_rows _______________________\n\
      \n>   ???\n\nD:\\桌面\\RealAppCodeBench_generic_eval\\tests\\Dataset\\functional_test.py:155:\
      \ \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _\ngeneration\\claude-4.5-sonnet\\Dataset\\dataset\\table.py:233: in\
      \ find\n    cursor = self.database.execute(sql, values)\n_ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <dataset.database.Database\
      \ object at 0x000002EA38106940>\nsql = 'SELECT * FROM users WHERE age = ?',\
      \ params = [{'>=': 40}]\n\n    def execute(self, sql, params=None):\n      \
      \  \"\"\"\n        Execute a SQL statement.\n    \n        Args:\n         \
      \   sql: SQL statement\n            params: Parameters (dict or tuple)\n   \
      \ \n        Returns:\n            Cursor object\n        \"\"\"\n        if\
      \ params is None:\n            params = {}\n>       return self._connection.execute(sql,\
      \ params)\nE       sqlite3.InterfaceError: Error binding parameter 0 - probably\
      \ unsupported type.\n\ngeneration\\claude-4.5-sonnet\\Dataset\\dataset\\database.py:107:\
      \ InterfaceError\n_______________________ test_update_upsert_and_indexes ________________________\n\
      \n>   ???\nE   assert 150.0 == 150.0\nE     \nE     comparison failed\nE   \
      \  Obtained: 150.0\nE     Expected: 150.0\n\nD:\\桌面\\RealAppCodeBench_generic_eval\\\
      tests\\Dataset\\functional_test.py:184: AssertionError\n_______________________\
      \ test_find_order_by_limit_offset _______________________\n\n>   ???\n\nD:\\\
      桌面\\RealAppCodeBench_generic_eval\\tests\\Dataset\\functional_test.py:248: \n\
      _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _\ngeneration\\claude-4.5-sonnet\\Dataset\\dataset\\table.py:233: in find\n\
      \    cursor = self.database.execute(sql, values)\n_ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <dataset.database.Database\
      \ object at 0x000002EA38102EE0>\nsql = 'SELECT * FROM nums WHERE order_by =\
      \ ? AND _limit = ? AND _offset = ?'\nparams = ['n', 3, 4]\n\n    def execute(self,\
      \ sql, params=None):\n        \"\"\"\n        Execute a SQL statement.\n   \
      \ \n        Args:\n            sql: SQL statement\n            params: Parameters\
      \ (dict or tuple)\n    \n        Returns:\n            Cursor object\n     \
      \   \"\"\"\n        if params is None:\n            params = {}\n>       return\
      \ self._connection.execute(sql, params)\nE       sqlite3.OperationalError: no\
      \ such column: order_by\n\ngeneration\\claude-4.5-sonnet\\Dataset\\dataset\\\
      database.py:107: OperationalError\n___________________ test_drop_table_removes_from_db_tables\
      \ ____________________\n\n>   ???\nE   AssertionError: assert 'to_drop' in []\n\
      E    +  where [] = _db_tables(<dataset.database.Database object at 0x000002EA38189CA0>)\n\
      \nD:\\桌面\\RealAppCodeBench_generic_eval\\tests\\Dataset\\functional_test.py:301:\
      \ AssertionError\n_____________________ test_distinct_returns_unique_values\
      \ _____________________\n\n>   ???\n\nD:\\桌面\\RealAppCodeBench_generic_eval\\\
      tests\\Dataset\\functional_test.py:333: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n.0 = <list_iterator object\
      \ at 0x000002EA381CFAC0>\n\n>   ???\nE   TypeError: string indices must be integers\n\
      \nD:\\桌面\\RealAppCodeBench_generic_eval\\tests\\Dataset\\functional_test.py:333:\
      \ TypeError\n=========================== short test summary info ===========================\n\
      FAILED tests/Dataset/functional_test.py::test_insert_and_query_basic_rows -\
      \ s...\nFAILED tests/Dataset/functional_test.py::test_update_upsert_and_indexes\
      \ - ass...\nFAILED tests/Dataset/functional_test.py::test_find_order_by_limit_offset\
      \ - sq...\nFAILED tests/Dataset/functional_test.py::test_drop_table_removes_from_db_tables\n\
      FAILED tests/Dataset/functional_test.py::test_distinct_returns_unique_values\n\
      5 failed, 6 passed in 3.52s\n"
    elapsed_time_s: 4.787475
    avg_memory_mb: 33.71
    avg_cpu_percent: 98.7
    passed: 6
    failed: 5
    skipped: 0
    total: 11
    score_inputs_passed: 6
    score_inputs_failed: 5
    score_inputs_total: 11
    score_inputs_returncode: 1
    score_inputs_failed_suite: true
    score_inputs_baseline_keys:
    - functional_suite_time_s
    - functional_tests_total
  performance:
    returncode: 2
    stdout: "\n=================================== ERRORS ====================================\n\
      _____________ ERROR collecting tests/Dataset/performance_test.py ______________\n\
      C:\\Users\\86152\\AppData\\Local\\Programs\\Python\\Python39\\lib\\importlib\\\
      __init__.py:127: in import_module\n    return _bootstrap._gcd_import(name[level:],\
      \ package, level)\n<frozen importlib._bootstrap>:1030: in _gcd_import\n    ???\n\
      <frozen importlib._bootstrap>:1007: in _find_and_load\n    ???\n<frozen importlib._bootstrap>:986:\
      \ in _find_and_load_unlocked\n    ???\n<frozen importlib._bootstrap>:680: in\
      \ _load_unlocked\n    ???\nC:\\Users\\86152\\AppData\\Local\\Programs\\Python\\\
      Python39\\lib\\site-packages\\_pytest\\assertion\\rewrite.py:186: in exec_module\n\
      \    exec(co, module.__dict__)\nD:\\桌面\\RealAppCodeBench_generic_eval\\tests\\\
      Dataset\\performance_test.py:18: in <module>\n    ???\nE   RuntimeError: Unsupported\
      \ DATASET_TARGET value: generated\n=========================== short test summary\
      \ info ===========================\nERROR tests/Dataset/performance_test.py\
      \ - RuntimeError: Unsupported DATASET_T...\n!!!!!!!!!!!!!!!!!!! Interrupted:\
      \ 1 error during collection !!!!!!!!!!!!!!!!!!!!\n1 error in 0.49s\n"
    elapsed_time_s: 1.725994
    avg_memory_mb: 35.45
    avg_cpu_percent: 99.1
    passed: 0
    failed: 0
    skipped: 0
    total: 1
    score_inputs_passed: 0
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 2
    score_inputs_failed_suite: true
    score_inputs_baseline_keys:
    - performance_suite_time_s
    - performance_tests_total
    score_inputs_baseline_time_s: 2.781374
    score_inputs_actual_time_s: 1.725994
  resource:
    returncode: 2
    stdout: "\n=================================== ERRORS ====================================\n\
      _______________ ERROR collecting tests/Dataset/resource_test.py _______________\n\
      C:\\Users\\86152\\AppData\\Local\\Programs\\Python\\Python39\\lib\\importlib\\\
      __init__.py:127: in import_module\n    return _bootstrap._gcd_import(name[level:],\
      \ package, level)\n<frozen importlib._bootstrap>:1030: in _gcd_import\n    ???\n\
      <frozen importlib._bootstrap>:1007: in _find_and_load\n    ???\n<frozen importlib._bootstrap>:986:\
      \ in _find_and_load_unlocked\n    ???\n<frozen importlib._bootstrap>:680: in\
      \ _load_unlocked\n    ???\nC:\\Users\\86152\\AppData\\Local\\Programs\\Python\\\
      Python39\\lib\\site-packages\\_pytest\\assertion\\rewrite.py:186: in exec_module\n\
      \    exec(co, module.__dict__)\nD:\\桌面\\RealAppCodeBench_generic_eval\\tests\\\
      Dataset\\resource_test.py:18: in <module>\n    ???\nE   RuntimeError: Unsupported\
      \ DATASET_TARGET value: generated\n=========================== short test summary\
      \ info ===========================\nERROR tests/Dataset/resource_test.py - RuntimeError:\
      \ Unsupported DATASET_TARG...\n!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during\
      \ collection !!!!!!!!!!!!!!!!!!!!\n1 error in 0.49s\n"
    elapsed_time_s: 1.678846
    avg_memory_mb: 36.34
    avg_cpu_percent: 99.0
    passed: 0
    failed: 0
    skipped: 0
    total: 1
    score_inputs_passed: 0
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 2
    score_inputs_failed_suite: true
    score_inputs_baseline_keys:
    - avg_cpu_percent
    - avg_memory_mb
    - resource_suite_time_s
    - resource_tests_total
    score_inputs_baseline_mem_mb: 55.98
    score_inputs_baseline_cpu_pct: 100.6
    score_inputs_actual_mem_mb: 36.34
    score_inputs_actual_cpu_pct: 99.0
  robustness:
    returncode: 0
    stdout: "......                                                              \
      \     [100%]\n============================== warnings summary ===============================\n\
      ..\\RealAppCodeBench_generic_eval\\tests\\Dataset\\robustness_test.py:92\n \
      \ D:\\桌面\\RealAppCodeBench_generic_eval\\tests\\Dataset\\robustness_test.py:92:\
      \ PytestUnknownMarkWarning: Unknown pytest.mark.timeout - is this a typo?  You\
      \ can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n\
      \n..\\RealAppCodeBench_generic_eval\\tests\\Dataset\\robustness_test.py:102\n\
      \  D:\\桌面\\RealAppCodeBench_generic_eval\\tests\\Dataset\\robustness_test.py:102:\
      \ PytestUnknownMarkWarning: Unknown pytest.mark.timeout - is this a typo?  You\
      \ can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n\
      \n..\\RealAppCodeBench_generic_eval\\tests\\Dataset\\robustness_test.py:120\n\
      \  D:\\桌面\\RealAppCodeBench_generic_eval\\tests\\Dataset\\robustness_test.py:120:\
      \ PytestUnknownMarkWarning: Unknown pytest.mark.timeout - is this a typo?  You\
      \ can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n\
      \n..\\RealAppCodeBench_generic_eval\\tests\\Dataset\\robustness_test.py:135\n\
      \  D:\\桌面\\RealAppCodeBench_generic_eval\\tests\\Dataset\\robustness_test.py:135:\
      \ PytestUnknownMarkWarning: Unknown pytest.mark.timeout - is this a typo?  You\
      \ can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n\
      \n..\\RealAppCodeBench_generic_eval\\tests\\Dataset\\robustness_test.py:149\n\
      \  D:\\桌面\\RealAppCodeBench_generic_eval\\tests\\Dataset\\robustness_test.py:149:\
      \ PytestUnknownMarkWarning: Unknown pytest.mark.timeout - is this a typo?  You\
      \ can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n\
      \n..\\RealAppCodeBench_generic_eval\\tests\\Dataset\\robustness_test.py:163\n\
      \  D:\\桌面\\RealAppCodeBench_generic_eval\\tests\\Dataset\\robustness_test.py:163:\
      \ PytestUnknownMarkWarning: Unknown pytest.mark.timeout - is this a typo?  You\
      \ can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n\
      \n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n\
      6 passed, 6 warnings in 0.14s\n"
    elapsed_time_s: 1.405271
    avg_memory_mb: 31.95
    avg_cpu_percent: 98.9
    passed: 6
    failed: 0
    skipped: 0
    total: 6
    score_inputs_passed: 6
    score_inputs_failed: 0
    score_inputs_total: 6
    score_inputs_returncode: 0
    score_inputs_failed_suite: false
    score_inputs_baseline_keys:
    - robustness_suite_time_s
    - robustness_tests_total
  security:
    returncode: 0
    stdout: 'SECURITY_METRICS high_risk_count=0.0 files_scanned=3.0 total_loc=382.0

      .

      1 passed in 0.10s

      '
    elapsed_time_s: 1.255864
    avg_memory_mb: 32.07
    avg_cpu_percent: 98.6
    passed: 1
    failed: 0
    skipped: 0
    total: 1
    metrics:
      high_risk_count: 0.0
      files_scanned: 3.0
      total_loc: 382.0
    score_inputs_passed: 1
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 0
    score_inputs_failed_suite: false
    score_inputs_baseline_keys:
    - metrics
    - security_suite_time_s
    - security_tests_total
    score_inputs_baseline_high_risk_count: 0.0
    score_inputs_generated_high_risk_count: 0.0
  maintainability:
    returncode: 0
    stdout: 'MAINT_METRICS mi_min=20.2500 files_scanned=3.0 total_loc=382.0 max_cc=10.0

      .

      1 passed in 0.13s

      '
    elapsed_time_s: 1.298767
    avg_memory_mb: 32.14
    avg_cpu_percent: 100.0
    passed: 1
    failed: 0
    skipped: 0
    total: 1
    metrics:
      mi_min: 20.25
      files_scanned: 3.0
      total_loc: 382.0
      max_cc: 10.0
    score_inputs_passed: 1
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 0
    score_inputs_failed_suite: false
    score_inputs_baseline_keys:
    - maintainability_suite_time_s
    - maintainability_tests_total
    - metrics
    score_inputs_baseline_mi_min: 7.7184
    score_inputs_generated_mi_min: 20.25
    score_inputs_ratio_g_over_b: 2.623600746268657
baseline_metrics:
  performance:
    performance_suite_time_s: 2.781374
    performance_tests_total: 2
  resource:
    resource_suite_time_s: 2.854474
    resource_tests_total: 2
    avg_memory_mb: 55.98
    avg_cpu_percent: 100.6
  functional:
    functional_suite_time_s: 4.552087
    functional_tests_total: 11
  robustness:
    robustness_suite_time_s: 2.083828
    robustness_tests_total: 6
  security:
    security_suite_time_s: 1.286846
    security_tests_total: 1
    metrics:
      high_risk_count: 0.0
      files_scanned: 6.0
      total_loc: 1131.0
  maintainability:
    maintainability_suite_time_s: 1.311875
    maintainability_tests_total: 1
    metrics:
      mi_min: 7.7184
      files_scanned: 6.0
      total_loc: 1131.0
      max_cc: 16.0
pytest_logs_dir: D:\桌面\RAL-Bench\results\Dataset\pytest_logs

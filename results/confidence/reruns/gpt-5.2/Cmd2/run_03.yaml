project_name: Cmd2
task_file: D:\桌面\RAL-Bench\tasks\Cmd2\cmd2.yaml
generated_repo: D:\桌面\RAL-Bench\generation\gpt-5.2\Cmd2
timestamp: '2026-01-21 22:14:18'
functional_score: 1.0
non_functional_score: 0.24
non_functional_subscores:
  maintainability: 0.0
  security: 1.0
  robustness: 0.0
  performance: 0.0
  resource: 0.0
non_functional_weights:
  maintainability: 0.36
  security: 0.24
  robustness: 0.16
  performance: 0.12
  resource: 0.12
results:
  functional:
    returncode: 0
    stdout: '...........                                                              [100%]

      11 passed in 3.00s

      '
    elapsed_time_s: 4.183381
    avg_memory_mb: 30.91
    avg_cpu_percent: 100.0
    passed: 11
    failed: 0
    skipped: 0
    total: 11
    score_inputs_passed: 11
    score_inputs_failed: 0
    score_inputs_total: 11
    score_inputs_returncode: 0
    score_inputs_failed_suite: false
    score_inputs_baseline_keys:
    - functional_suite_time_s
    - functional_tests_total
  performance:
    returncode: 2
    stdout: "\n=================================== ERRORS ====================================\n\
      _______________ ERROR collecting tests/Cmd2/performance_test.py _______________\n\
      C:\\Users\\86152\\AppData\\Local\\Programs\\Python\\Python39\\lib\\importlib\\\
      __init__.py:127: in import_module\n    return _bootstrap._gcd_import(name[level:],\
      \ package, level)\n<frozen importlib._bootstrap>:1030: in _gcd_import\n    ???\n\
      <frozen importlib._bootstrap>:1007: in _find_and_load\n    ???\n<frozen importlib._bootstrap>:986:\
      \ in _find_and_load_unlocked\n    ???\n<frozen importlib._bootstrap>:680: in\
      \ _load_unlocked\n    ???\nC:\\Users\\86152\\AppData\\Local\\Programs\\Python\\\
      Python39\\lib\\site-packages\\_pytest\\assertion\\rewrite.py:186: in exec_module\n\
      \    exec(co, module.__dict__)\nD:\\桌面\\RealAppCodeBench_generic_eval\\tests\\\
      Cmd2\\performance_test.py:20: in <module>\n    ???\nE   RuntimeError: Target\
      \ repository does not exist: D:\\桌面\\RAL-Bench\\generation\\cmd2\n===========================\
      \ short test summary info ===========================\nERROR tests/Cmd2/performance_test.py\
      \ - RuntimeError: Target repository does n...\n!!!!!!!!!!!!!!!!!!! Interrupted:\
      \ 1 error during collection !!!!!!!!!!!!!!!!!!!!\n1 error in 0.51s\n"
    elapsed_time_s: 1.810971
    avg_memory_mb: 35.66
    avg_cpu_percent: 97.3
    passed: 0
    failed: 0
    skipped: 0
    total: 1
    score_inputs_passed: 0
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 2
    score_inputs_failed_suite: true
    score_inputs_baseline_keys:
    - performance_suite_time_s
    - performance_tests_total
    score_inputs_baseline_time_s: 1.911562
    score_inputs_actual_time_s: 1.810971
  resource:
    returncode: 2
    stdout: "\n=================================== ERRORS ====================================\n\
      ________________ ERROR collecting tests/Cmd2/resource_test.py _________________\n\
      C:\\Users\\86152\\AppData\\Local\\Programs\\Python\\Python39\\lib\\importlib\\\
      __init__.py:127: in import_module\n    return _bootstrap._gcd_import(name[level:],\
      \ package, level)\n<frozen importlib._bootstrap>:1030: in _gcd_import\n    ???\n\
      <frozen importlib._bootstrap>:1007: in _find_and_load\n    ???\n<frozen importlib._bootstrap>:986:\
      \ in _find_and_load_unlocked\n    ???\n<frozen importlib._bootstrap>:680: in\
      \ _load_unlocked\n    ???\nC:\\Users\\86152\\AppData\\Local\\Programs\\Python\\\
      Python39\\lib\\site-packages\\_pytest\\assertion\\rewrite.py:186: in exec_module\n\
      \    exec(co, module.__dict__)\nD:\\桌面\\RealAppCodeBench_generic_eval\\tests\\\
      Cmd2\\resource_test.py:21: in <module>\n    ???\nE   RuntimeError: Target repository\
      \ does not exist: D:\\桌面\\RAL-Bench\\generation\\cmd2\n===========================\
      \ short test summary info ===========================\nERROR tests/Cmd2/resource_test.py\
      \ - RuntimeError: Target repository does not ...\n!!!!!!!!!!!!!!!!!!! Interrupted:\
      \ 1 error during collection !!!!!!!!!!!!!!!!!!!!\n1 error in 0.50s\n"
    elapsed_time_s: 1.691078
    avg_memory_mb: 38.05
    avg_cpu_percent: 99.0
    passed: 0
    failed: 0
    skipped: 0
    total: 1
    score_inputs_passed: 0
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 2
    score_inputs_failed_suite: true
    score_inputs_baseline_keys:
    - avg_cpu_percent
    - avg_memory_mb
    - resource_suite_time_s
    - resource_tests_total
    score_inputs_baseline_mem_mb: 44.14
    score_inputs_baseline_cpu_pct: 100.9
    score_inputs_actual_mem_mb: 38.05
    score_inputs_actual_cpu_pct: 99.0
  robustness:
    returncode: 1
    stdout: "FFF                                                                 \
      \     [100%]\n================================== FAILURES ===================================\n\
      ___________ test_cmd2_robustness_import_or_graceful_incompatibility ___________\n\
      \n>   ???\n\nD:\\桌面\\RealAppCodeBench_generic_eval\\tests\\Cmd2\\robustness_test.py:138:\
      \ \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _\nD:\\桌面\\RealAppCodeBench_generic_eval\\tests\\Cmd2\\robustness_test.py:97:\
      \ in _try_import_cmd2\n    ???\ngeneration\\gpt-5.2\\cmd2\\cmd2\\__init__.py:8:\
      \ in <module>\n    from .cmd2 import Cmd2, CommandResult, StdSim, with_default_category\n\
      _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _\n\n>   from .cmd2 import Cmd2, CommandResult, StdSim, with_default_category\n\
      E   ImportError: cannot import name 'Cmd2' from partially initialized module\
      \ 'cmd2.cmd2' (most likely due to a circular import) (D:\\桌面\\RAL-Bench\\generation\\\
      gpt-5.2\\cmd2\\cmd2\\cmd2.py)\n\ngeneration\\gpt-5.2\\cmd2\\cmd2\\cmd2.py:2:\
      \ ImportError\n___________ test_cmd2_robustness_minimal_instantiation_if_possible\
      \ ____________\n\n>   ???\n\nD:\\桌面\\RealAppCodeBench_generic_eval\\tests\\\
      Cmd2\\robustness_test.py:155: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nD:\\桌面\\RealAppCodeBench_generic_eval\\\
      tests\\Cmd2\\robustness_test.py:97: in _try_import_cmd2\n    ???\ngeneration\\\
      gpt-5.2\\cmd2\\cmd2\\__init__.py:8: in <module>\n    from .cmd2 import Cmd2,\
      \ CommandResult, StdSim, with_default_category\n_ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n>   from .cmd2 import\
      \ Cmd2, CommandResult, StdSim, with_default_category\nE   ImportError: cannot\
      \ import name 'Cmd2' from partially initialized module 'cmd2.cmd2' (most likely\
      \ due to a circular import) (D:\\桌面\\RAL-Bench\\generation\\gpt-5.2\\cmd2\\\
      cmd2\\cmd2.py)\n\ngeneration\\gpt-5.2\\cmd2\\cmd2\\cmd2.py:2: ImportError\n\
      __________ test_cmd2_robustness_basic_command_execution_if_possible ___________\n\
      \n>   ???\n\nD:\\桌面\\RealAppCodeBench_generic_eval\\tests\\Cmd2\\robustness_test.py:173:\
      \ \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _\nD:\\桌面\\RealAppCodeBench_generic_eval\\tests\\Cmd2\\robustness_test.py:97:\
      \ in _try_import_cmd2\n    ???\ngeneration\\gpt-5.2\\cmd2\\cmd2\\__init__.py:8:\
      \ in <module>\n    from .cmd2 import Cmd2, CommandResult, StdSim, with_default_category\n\
      _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _\n\n>   from .cmd2 import Cmd2, CommandResult, StdSim, with_default_category\n\
      E   ImportError: cannot import name 'Cmd2' from partially initialized module\
      \ 'cmd2.cmd2' (most likely due to a circular import) (D:\\桌面\\RAL-Bench\\generation\\\
      gpt-5.2\\cmd2\\cmd2\\cmd2.py)\n\ngeneration\\gpt-5.2\\cmd2\\cmd2\\cmd2.py:2:\
      \ ImportError\n============================== warnings summary ===============================\n\
      ..\\RealAppCodeBench_generic_eval\\tests\\Cmd2\\robustness_test.py:129\n  D:\\\
      桌面\\RealAppCodeBench_generic_eval\\tests\\Cmd2\\robustness_test.py:129: PytestUnknownMarkWarning:\
      \ Unknown pytest.mark.timeout - is this a typo?  You can register custom marks\
      \ to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n\
      \n..\\RealAppCodeBench_generic_eval\\tests\\Cmd2\\robustness_test.py:148\n \
      \ D:\\桌面\\RealAppCodeBench_generic_eval\\tests\\Cmd2\\robustness_test.py:148:\
      \ PytestUnknownMarkWarning: Unknown pytest.mark.timeout - is this a typo?  You\
      \ can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n\
      \n..\\RealAppCodeBench_generic_eval\\tests\\Cmd2\\robustness_test.py:165\n \
      \ D:\\桌面\\RealAppCodeBench_generic_eval\\tests\\Cmd2\\robustness_test.py:165:\
      \ PytestUnknownMarkWarning: Unknown pytest.mark.timeout - is this a typo?  You\
      \ can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n\
      \n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n\
      =========================== short test summary info ===========================\n\
      FAILED tests/Cmd2/robustness_test.py::test_cmd2_robustness_import_or_graceful_incompatibility\n\
      FAILED tests/Cmd2/robustness_test.py::test_cmd2_robustness_minimal_instantiation_if_possible\n\
      FAILED tests/Cmd2/robustness_test.py::test_cmd2_robustness_basic_command_execution_if_possible\n\
      3 failed, 3 warnings in 0.35s\n"
    elapsed_time_s: 1.568678
    avg_memory_mb: 31.55
    avg_cpu_percent: 100.0
    passed: 0
    failed: 3
    skipped: 0
    total: 3
    score_inputs_passed: 0
    score_inputs_failed: 3
    score_inputs_total: 3
    score_inputs_returncode: 1
    score_inputs_failed_suite: true
    score_inputs_baseline_keys:
    - robustness_suite_time_s
    - robustness_tests_total
  security:
    returncode: 0
    stdout: 'SECURITY_METRICS high_risk_count=0.0 files_scanned=4.0 total_loc=174.0

      .

      1 passed in 0.10s

      '
    elapsed_time_s: 1.250852
    avg_memory_mb: 31.57
    avg_cpu_percent: 101.3
    passed: 1
    failed: 0
    skipped: 0
    total: 1
    metrics:
      high_risk_count: 0.0
      files_scanned: 4.0
      total_loc: 174.0
    score_inputs_passed: 1
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 0
    score_inputs_failed_suite: false
    score_inputs_baseline_keys:
    - metrics
    - security_suite_time_s
    - security_tests_total
    score_inputs_baseline_high_risk_count: 3.0
    score_inputs_generated_high_risk_count: 0.0
  maintainability:
    returncode: 0
    stdout: 'MAINT_METRICS mi_min=29.8956 files_scanned=4.0 total_loc=174.0 max_cc=8.0

      .

      1 passed in 0.13s

      '
    elapsed_time_s: 1.28897
    avg_memory_mb: 31.67
    avg_cpu_percent: 98.7
    passed: 1
    failed: 0
    skipped: 0
    total: 1
    metrics:
      mi_min: 29.8956
      files_scanned: 4.0
      total_loc: 174.0
      max_cc: 8.0
    score_inputs_passed: 1
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 0
    score_inputs_failed_suite: false
    score_inputs_baseline_keys:
    - maintainability_suite_time_s
    - maintainability_tests_total
    - metrics
    score_inputs_baseline_mi_min: 0.0
    score_inputs_generated_mi_min: 29.8956
baseline_metrics:
  performance:
    performance_suite_time_s: 1.911562
    performance_tests_total: 1
  resource:
    resource_suite_time_s: 1.91871
    resource_tests_total: 1
    avg_memory_mb: 44.14
    avg_cpu_percent: 100.9
  functional:
    functional_suite_time_s: 3.166834
    functional_tests_total: 11
  robustness:
    robustness_suite_time_s: 1.488449
    robustness_tests_total: 3
  security:
    security_suite_time_s: 1.400502
    security_tests_total: 1
    metrics:
      high_risk_count: 3.0
      files_scanned: 21.0
      total_loc: 9076.0
  maintainability:
    maintainability_suite_time_s: 1.998324
    maintainability_tests_total: 1
    metrics:
      mi_min: 0.0
      files_scanned: 21.0
      total_loc: 9076.0
      max_cc: 69.0
pytest_logs_dir: D:\桌面\RAL-Bench\results\Cmd2\pytest_logs

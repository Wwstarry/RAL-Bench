project_name: Dataset
task_file: D:\桌面\RAL-Bench\tasks\Dataset\dataset.yaml
generated_repo: D:\桌面\RAL-Bench\generation\gemini-3-pro-preview\Dataset
timestamp: '2026-01-21 22:20:47'
functional_score: 0.0
non_functional_score: 0.655
non_functional_subscores:
  maintainability: 0.7084
  security: 1.0
  robustness: 1.0
  performance: 0.0
  resource: 0.0
non_functional_weights:
  maintainability: 0.36
  security: 0.24
  robustness: 0.16
  performance: 0.12
  resource: 0.12
results:
  functional:
    returncode: 1
    stdout: "FFFFFFFFFFF                                                         \
      \     [100%]\n================================== FAILURES ===================================\n\
      ______________________ test_insert_and_query_basic_rows _______________________\n\
      \n>   ???\n\nD:\\桌面\\RealAppCodeBench_generic_eval\\tests\\Dataset\\functional_test.py:142:\
      \ \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _\ngeneration\\gemini-3-pro-preview\\Dataset\\dataset\\table.py:100:\
      \ in insert\n    return self.insert_many([row])\n_ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <dataset.table.Table\
      \ object at 0x0000021731415400>\nrows = [{'age': 30, 'country': 'DE', 'name':\
      \ 'Alice'}], chunk_size = None\n\n    def insert_many(self, rows, chunk_size=None):\n\
      \        \"\"\"\n        Insert multiple rows.\n        \"\"\"\n        if not\
      \ rows:\n            return\n    \n        # Normalize rows to dicts\n     \
      \   rows = [dict(r) for r in rows]\n    \n        # 1. Ensure table exists based\
      \ on the first row (or union of keys if we were fancy,\n        #    but standard\
      \ dataset often just looks at what's coming in).\n        #    To be safe, we\
      \ check schema against all keys in the batch.\n        all_keys = set()\n  \
      \      for r in rows:\n            all_keys.update(r.keys())\n    \n       \
      \ # Create a dummy row with all keys to ensure schema\n        dummy_schema_row\
      \ = {k: rows[0].get(k) for k in all_keys}\n    \n        with self.db.lock:\n\
      \            self._ensure_table(dummy_schema_row)\n            self._ensure_columns(dummy_schema_row)\n\
      \    \n            # 2. Construct Insert SQL\n            # We use the first\
      \ row to determine the parameterized query structure,\n            # but since\
      \ we ensured columns for ALL keys, we can just use all_keys.\n            keys\
      \ = list(all_keys)\n            placeholders = [f':{k}' for k in keys]\n   \
      \         columns_sql = \", \".join(f'\"{k}\"' for k in keys)\n            values_sql\
      \ = \", \".join(placeholders)\n    \n            sql = f'INSERT INTO \"{self.name}\"\
      \ ({columns_sql}) VALUES ({values_sql})'\n    \n            cursor = self.db._conn.cursor()\n\
      \            try:\n>               cursor.executemany(sql, rows)\nE        \
      \       sqlite3.OperationalError: no such table: users\n\ngeneration\\gemini-3-pro-preview\\\
      Dataset\\dataset\\table.py:138: OperationalError\n_______________________ test_update_upsert_and_indexes\
      \ ________________________\n\n>   ???\n\nD:\\桌面\\RealAppCodeBench_generic_eval\\\
      tests\\Dataset\\functional_test.py:175: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <dataset.table.Table\
      \ object at 0x000002173149CB20>\nrows = [{'account_id': 1, 'balance': 100.0,\
      \ 'currency': 'EUR', 'owner': 'Alice'}, {'account_id': 2, 'balance': 250.0,\
      \ 'currency': 'USD', 'owner': 'Bob'}]\nchunk_size = None\n\n    def insert_many(self,\
      \ rows, chunk_size=None):\n        \"\"\"\n        Insert multiple rows.\n \
      \       \"\"\"\n        if not rows:\n            return\n    \n        # Normalize\
      \ rows to dicts\n        rows = [dict(r) for r in rows]\n    \n        # 1.\
      \ Ensure table exists based on the first row (or union of keys if we were fancy,\n\
      \        #    but standard dataset often just looks at what's coming in).\n\
      \        #    To be safe, we check schema against all keys in the batch.\n \
      \       all_keys = set()\n        for r in rows:\n            all_keys.update(r.keys())\n\
      \    \n        # Create a dummy row with all keys to ensure schema\n       \
      \ dummy_schema_row = {k: rows[0].get(k) for k in all_keys}\n    \n        with\
      \ self.db.lock:\n            self._ensure_table(dummy_schema_row)\n        \
      \    self._ensure_columns(dummy_schema_row)\n    \n            # 2. Construct\
      \ Insert SQL\n            # We use the first row to determine the parameterized\
      \ query structure,\n            # but since we ensured columns for ALL keys,\
      \ we can just use all_keys.\n            keys = list(all_keys)\n           \
      \ placeholders = [f':{k}' for k in keys]\n            columns_sql = \", \".join(f'\"\
      {k}\"' for k in keys)\n            values_sql = \", \".join(placeholders)\n\
      \    \n            sql = f'INSERT INTO \"{self.name}\" ({columns_sql}) VALUES\
      \ ({values_sql})'\n    \n            cursor = self.db._conn.cursor()\n     \
      \       try:\n>               cursor.executemany(sql, rows)\nE             \
      \  sqlite3.OperationalError: no such table: accounts\n\ngeneration\\gemini-3-pro-preview\\\
      Dataset\\dataset\\table.py:138: OperationalError\n____________________ test_transactions_commit_and_rollback\
      \ ____________________\n\ntmp_path = WindowsPath('C:/Users/86152/AppData/Local/Temp/pytest-of-86152/pytest-559/test_transactions_commit_and_r0')\n\
      \n>   ???\n\nD:\\桌面\\RealAppCodeBench_generic_eval\\tests\\Dataset\\functional_test.py:203:\
      \ \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _\ngeneration\\gemini-3-pro-preview\\Dataset\\dataset\\table.py:100:\
      \ in insert\n    return self.insert_many([row])\n_ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <dataset.table.Table\
      \ object at 0x000002173142CF10>\nrows = [{'category': 'ok', 'name': 'committed'}],\
      \ chunk_size = None\n\n    def insert_many(self, rows, chunk_size=None):\n \
      \       \"\"\"\n        Insert multiple rows.\n        \"\"\"\n        if not\
      \ rows:\n            return\n    \n        # Normalize rows to dicts\n     \
      \   rows = [dict(r) for r in rows]\n    \n        # 1. Ensure table exists based\
      \ on the first row (or union of keys if we were fancy,\n        #    but standard\
      \ dataset often just looks at what's coming in).\n        #    To be safe, we\
      \ check schema against all keys in the batch.\n        all_keys = set()\n  \
      \      for r in rows:\n            all_keys.update(r.keys())\n    \n       \
      \ # Create a dummy row with all keys to ensure schema\n        dummy_schema_row\
      \ = {k: rows[0].get(k) for k in all_keys}\n    \n        with self.db.lock:\n\
      \            self._ensure_table(dummy_schema_row)\n            self._ensure_columns(dummy_schema_row)\n\
      \    \n            # 2. Construct Insert SQL\n            # We use the first\
      \ row to determine the parameterized query structure,\n            # but since\
      \ we ensured columns for ALL keys, we can just use all_keys.\n            keys\
      \ = list(all_keys)\n            placeholders = [f':{k}' for k in keys]\n   \
      \         columns_sql = \", \".join(f'\"{k}\"' for k in keys)\n            values_sql\
      \ = \", \".join(placeholders)\n    \n            sql = f'INSERT INTO \"{self.name}\"\
      \ ({columns_sql}) VALUES ({values_sql})'\n    \n            cursor = self.db._conn.cursor()\n\
      \            try:\n>               cursor.executemany(sql, rows)\nE        \
      \       sqlite3.OperationalError: no such table: events\n\ngeneration\\gemini-3-pro-preview\\\
      Dataset\\dataset\\table.py:138: OperationalError\n___________________ test_insert_many_returns_ids_and_count\
      \ ____________________\n\n>   ???\n\nD:\\桌面\\RealAppCodeBench_generic_eval\\\
      tests\\Dataset\\functional_test.py:223: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <dataset.table.Table\
      \ object at 0x000002172FE20B20>\nrows = [{'name': 'A'}, {'name': 'B'}, {'name':\
      \ 'C'}], chunk_size = None\n\n    def insert_many(self, rows, chunk_size=None):\n\
      \        \"\"\"\n        Insert multiple rows.\n        \"\"\"\n        if not\
      \ rows:\n            return\n    \n        # Normalize rows to dicts\n     \
      \   rows = [dict(r) for r in rows]\n    \n        # 1. Ensure table exists based\
      \ on the first row (or union of keys if we were fancy,\n        #    but standard\
      \ dataset often just looks at what's coming in).\n        #    To be safe, we\
      \ check schema against all keys in the batch.\n        all_keys = set()\n  \
      \      for r in rows:\n            all_keys.update(r.keys())\n    \n       \
      \ # Create a dummy row with all keys to ensure schema\n        dummy_schema_row\
      \ = {k: rows[0].get(k) for k in all_keys}\n    \n        with self.db.lock:\n\
      \            self._ensure_table(dummy_schema_row)\n            self._ensure_columns(dummy_schema_row)\n\
      \    \n            # 2. Construct Insert SQL\n            # We use the first\
      \ row to determine the parameterized query structure,\n            # but since\
      \ we ensured columns for ALL keys, we can just use all_keys.\n            keys\
      \ = list(all_keys)\n            placeholders = [f':{k}' for k in keys]\n   \
      \         columns_sql = \", \".join(f'\"{k}\"' for k in keys)\n            values_sql\
      \ = \", \".join(placeholders)\n    \n            sql = f'INSERT INTO \"{self.name}\"\
      \ ({columns_sql}) VALUES ({values_sql})'\n    \n            cursor = self.db._conn.cursor()\n\
      \            try:\n>               cursor.executemany(sql, rows)\nE        \
      \       sqlite3.OperationalError: no such table: items\n\ngeneration\\gemini-3-pro-preview\\\
      Dataset\\dataset\\table.py:138: OperationalError\n_____________________ test_find_one_missing_returns_none\
      \ ______________________\n\n>   ???\n\nD:\\桌面\\RealAppCodeBench_generic_eval\\\
      tests\\Dataset\\functional_test.py:237: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ngeneration\\gemini-3-pro-preview\\\
      Dataset\\dataset\\table.py:100: in insert\n    return self.insert_many([row])\n\
      _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _\n\nself = <dataset.table.Table object at 0x0000021731498D60>\nrows = [{'name':\
      \ 'only'}], chunk_size = None\n\n    def insert_many(self, rows, chunk_size=None):\n\
      \        \"\"\"\n        Insert multiple rows.\n        \"\"\"\n        if not\
      \ rows:\n            return\n    \n        # Normalize rows to dicts\n     \
      \   rows = [dict(r) for r in rows]\n    \n        # 1. Ensure table exists based\
      \ on the first row (or union of keys if we were fancy,\n        #    but standard\
      \ dataset often just looks at what's coming in).\n        #    To be safe, we\
      \ check schema against all keys in the batch.\n        all_keys = set()\n  \
      \      for r in rows:\n            all_keys.update(r.keys())\n    \n       \
      \ # Create a dummy row with all keys to ensure schema\n        dummy_schema_row\
      \ = {k: rows[0].get(k) for k in all_keys}\n    \n        with self.db.lock:\n\
      \            self._ensure_table(dummy_schema_row)\n            self._ensure_columns(dummy_schema_row)\n\
      \    \n            # 2. Construct Insert SQL\n            # We use the first\
      \ row to determine the parameterized query structure,\n            # but since\
      \ we ensured columns for ALL keys, we can just use all_keys.\n            keys\
      \ = list(all_keys)\n            placeholders = [f':{k}' for k in keys]\n   \
      \         columns_sql = \", \".join(f'\"{k}\"' for k in keys)\n            values_sql\
      \ = \", \".join(placeholders)\n    \n            sql = f'INSERT INTO \"{self.name}\"\
      \ ({columns_sql}) VALUES ({values_sql})'\n    \n            cursor = self.db._conn.cursor()\n\
      \            try:\n>               cursor.executemany(sql, rows)\nE        \
      \       sqlite3.OperationalError: no such table: t\n\ngeneration\\gemini-3-pro-preview\\\
      Dataset\\dataset\\table.py:138: OperationalError\n_______________________ test_find_order_by_limit_offset\
      \ _______________________\n\n>   ???\n\nD:\\桌面\\RealAppCodeBench_generic_eval\\\
      tests\\Dataset\\functional_test.py:246: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ngeneration\\gemini-3-pro-preview\\\
      Dataset\\dataset\\table.py:100: in insert\n    return self.insert_many([row])\n\
      _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _\n\nself = <dataset.table.Table object at 0x00000217314C0D00>, rows = [{'n':\
      \ 0}]\nchunk_size = None\n\n    def insert_many(self, rows, chunk_size=None):\n\
      \        \"\"\"\n        Insert multiple rows.\n        \"\"\"\n        if not\
      \ rows:\n            return\n    \n        # Normalize rows to dicts\n     \
      \   rows = [dict(r) for r in rows]\n    \n        # 1. Ensure table exists based\
      \ on the first row (or union of keys if we were fancy,\n        #    but standard\
      \ dataset often just looks at what's coming in).\n        #    To be safe, we\
      \ check schema against all keys in the batch.\n        all_keys = set()\n  \
      \      for r in rows:\n            all_keys.update(r.keys())\n    \n       \
      \ # Create a dummy row with all keys to ensure schema\n        dummy_schema_row\
      \ = {k: rows[0].get(k) for k in all_keys}\n    \n        with self.db.lock:\n\
      \            self._ensure_table(dummy_schema_row)\n            self._ensure_columns(dummy_schema_row)\n\
      \    \n            # 2. Construct Insert SQL\n            # We use the first\
      \ row to determine the parameterized query structure,\n            # but since\
      \ we ensured columns for ALL keys, we can just use all_keys.\n            keys\
      \ = list(all_keys)\n            placeholders = [f':{k}' for k in keys]\n   \
      \         columns_sql = \", \".join(f'\"{k}\"' for k in keys)\n            values_sql\
      \ = \", \".join(placeholders)\n    \n            sql = f'INSERT INTO \"{self.name}\"\
      \ ({columns_sql}) VALUES ({values_sql})'\n    \n            cursor = self.db._conn.cursor()\n\
      \            try:\n>               cursor.executemany(sql, rows)\nE        \
      \       sqlite3.OperationalError: no such table: nums\n\ngeneration\\gemini-3-pro-preview\\\
      Dataset\\dataset\\table.py:138: OperationalError\n___________________ test_table_all_iteration_and_row_shape\
      \ ____________________\n\n>   ???\n\nD:\\桌面\\RealAppCodeBench_generic_eval\\\
      tests\\Dataset\\functional_test.py:255: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ngeneration\\gemini-3-pro-preview\\\
      Dataset\\dataset\\table.py:100: in insert\n    return self.insert_many([row])\n\
      _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _\n\nself = <dataset.table.Table object at 0x000002173149A190>\nrows = [{'age':\
      \ 30, 'name': 'Alice'}], chunk_size = None\n\n    def insert_many(self, rows,\
      \ chunk_size=None):\n        \"\"\"\n        Insert multiple rows.\n       \
      \ \"\"\"\n        if not rows:\n            return\n    \n        # Normalize\
      \ rows to dicts\n        rows = [dict(r) for r in rows]\n    \n        # 1.\
      \ Ensure table exists based on the first row (or union of keys if we were fancy,\n\
      \        #    but standard dataset often just looks at what's coming in).\n\
      \        #    To be safe, we check schema against all keys in the batch.\n \
      \       all_keys = set()\n        for r in rows:\n            all_keys.update(r.keys())\n\
      \    \n        # Create a dummy row with all keys to ensure schema\n       \
      \ dummy_schema_row = {k: rows[0].get(k) for k in all_keys}\n    \n        with\
      \ self.db.lock:\n            self._ensure_table(dummy_schema_row)\n        \
      \    self._ensure_columns(dummy_schema_row)\n    \n            # 2. Construct\
      \ Insert SQL\n            # We use the first row to determine the parameterized\
      \ query structure,\n            # but since we ensured columns for ALL keys,\
      \ we can just use all_keys.\n            keys = list(all_keys)\n           \
      \ placeholders = [f':{k}' for k in keys]\n            columns_sql = \", \".join(f'\"\
      {k}\"' for k in keys)\n            values_sql = \", \".join(placeholders)\n\
      \    \n            sql = f'INSERT INTO \"{self.name}\" ({columns_sql}) VALUES\
      \ ({values_sql})'\n    \n            cursor = self.db._conn.cursor()\n     \
      \       try:\n>               cursor.executemany(sql, rows)\nE             \
      \  sqlite3.OperationalError: no such table: people\n\ngeneration\\gemini-3-pro-preview\\\
      Dataset\\dataset\\table.py:138: OperationalError\n_______________________ test_delete_and_clear_all_rows\
      \ ________________________\n\n>   ???\n\nD:\\桌面\\RealAppCodeBench_generic_eval\\\
      tests\\Dataset\\functional_test.py:270: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <dataset.table.Table\
      \ object at 0x000002172FE37370>\nrows = [{'kind': 'a'}, {'kind': 'b'}, {'kind':\
      \ 'b'}], chunk_size = None\n\n    def insert_many(self, rows, chunk_size=None):\n\
      \        \"\"\"\n        Insert multiple rows.\n        \"\"\"\n        if not\
      \ rows:\n            return\n    \n        # Normalize rows to dicts\n     \
      \   rows = [dict(r) for r in rows]\n    \n        # 1. Ensure table exists based\
      \ on the first row (or union of keys if we were fancy,\n        #    but standard\
      \ dataset often just looks at what's coming in).\n        #    To be safe, we\
      \ check schema against all keys in the batch.\n        all_keys = set()\n  \
      \      for r in rows:\n            all_keys.update(r.keys())\n    \n       \
      \ # Create a dummy row with all keys to ensure schema\n        dummy_schema_row\
      \ = {k: rows[0].get(k) for k in all_keys}\n    \n        with self.db.lock:\n\
      \            self._ensure_table(dummy_schema_row)\n            self._ensure_columns(dummy_schema_row)\n\
      \    \n            # 2. Construct Insert SQL\n            # We use the first\
      \ row to determine the parameterized query structure,\n            # but since\
      \ we ensured columns for ALL keys, we can just use all_keys.\n            keys\
      \ = list(all_keys)\n            placeholders = [f':{k}' for k in keys]\n   \
      \         columns_sql = \", \".join(f'\"{k}\"' for k in keys)\n            values_sql\
      \ = \", \".join(placeholders)\n    \n            sql = f'INSERT INTO \"{self.name}\"\
      \ ({columns_sql}) VALUES ({values_sql})'\n    \n            cursor = self.db._conn.cursor()\n\
      \            try:\n>               cursor.executemany(sql, rows)\nE        \
      \       sqlite3.OperationalError: no such table: logs\n\ngeneration\\gemini-3-pro-preview\\\
      Dataset\\dataset\\table.py:138: OperationalError\n___________________ test_drop_table_removes_from_db_tables\
      \ ____________________\n\n>   ???\n\nD:\\桌面\\RealAppCodeBench_generic_eval\\\
      tests\\Dataset\\functional_test.py:299: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ngeneration\\gemini-3-pro-preview\\\
      Dataset\\dataset\\table.py:100: in insert\n    return self.insert_many([row])\n\
      _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _\n\nself = <dataset.table.Table object at 0x0000021731416FD0>, rows = [{'x':\
      \ 1}]\nchunk_size = None\n\n    def insert_many(self, rows, chunk_size=None):\n\
      \        \"\"\"\n        Insert multiple rows.\n        \"\"\"\n        if not\
      \ rows:\n            return\n    \n        # Normalize rows to dicts\n     \
      \   rows = [dict(r) for r in rows]\n    \n        # 1. Ensure table exists based\
      \ on the first row (or union of keys if we were fancy,\n        #    but standard\
      \ dataset often just looks at what's coming in).\n        #    To be safe, we\
      \ check schema against all keys in the batch.\n        all_keys = set()\n  \
      \      for r in rows:\n            all_keys.update(r.keys())\n    \n       \
      \ # Create a dummy row with all keys to ensure schema\n        dummy_schema_row\
      \ = {k: rows[0].get(k) for k in all_keys}\n    \n        with self.db.lock:\n\
      \            self._ensure_table(dummy_schema_row)\n            self._ensure_columns(dummy_schema_row)\n\
      \    \n            # 2. Construct Insert SQL\n            # We use the first\
      \ row to determine the parameterized query structure,\n            # but since\
      \ we ensured columns for ALL keys, we can just use all_keys.\n            keys\
      \ = list(all_keys)\n            placeholders = [f':{k}' for k in keys]\n   \
      \         columns_sql = \", \".join(f'\"{k}\"' for k in keys)\n            values_sql\
      \ = \", \".join(placeholders)\n    \n            sql = f'INSERT INTO \"{self.name}\"\
      \ ({columns_sql}) VALUES ({values_sql})'\n    \n            cursor = self.db._conn.cursor()\n\
      \            try:\n>               cursor.executemany(sql, rows)\nE        \
      \       sqlite3.OperationalError: no such table: to_drop\n\ngeneration\\gemini-3-pro-preview\\\
      Dataset\\dataset\\table.py:138: OperationalError\n_____________________ test_raw_sql_query_with_parameters\
      \ ______________________\n\ntmp_path = WindowsPath('C:/Users/86152/AppData/Local/Temp/pytest-of-86152/pytest-559/test_raw_sql_query_with_parame0')\n\
      \n>   ???\n\nD:\\桌面\\RealAppCodeBench_generic_eval\\tests\\Dataset\\functional_test.py:319:\
      \ \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _\n\nself = <dataset.table.Table object at 0x000002172FE37280>\nrows\
      \ = [{'k': 'a', 'v': 1}, {'k': 'b', 'v': 2}], chunk_size = None\n\n    def insert_many(self,\
      \ rows, chunk_size=None):\n        \"\"\"\n        Insert multiple rows.\n \
      \       \"\"\"\n        if not rows:\n            return\n    \n        # Normalize\
      \ rows to dicts\n        rows = [dict(r) for r in rows]\n    \n        # 1.\
      \ Ensure table exists based on the first row (or union of keys if we were fancy,\n\
      \        #    but standard dataset often just looks at what's coming in).\n\
      \        #    To be safe, we check schema against all keys in the batch.\n \
      \       all_keys = set()\n        for r in rows:\n            all_keys.update(r.keys())\n\
      \    \n        # Create a dummy row with all keys to ensure schema\n       \
      \ dummy_schema_row = {k: rows[0].get(k) for k in all_keys}\n    \n        with\
      \ self.db.lock:\n            self._ensure_table(dummy_schema_row)\n        \
      \    self._ensure_columns(dummy_schema_row)\n    \n            # 2. Construct\
      \ Insert SQL\n            # We use the first row to determine the parameterized\
      \ query structure,\n            # but since we ensured columns for ALL keys,\
      \ we can just use all_keys.\n            keys = list(all_keys)\n           \
      \ placeholders = [f':{k}' for k in keys]\n            columns_sql = \", \".join(f'\"\
      {k}\"' for k in keys)\n            values_sql = \", \".join(placeholders)\n\
      \    \n            sql = f'INSERT INTO \"{self.name}\" ({columns_sql}) VALUES\
      \ ({values_sql})'\n    \n            cursor = self.db._conn.cursor()\n     \
      \       try:\n>               cursor.executemany(sql, rows)\nE             \
      \  sqlite3.OperationalError: no such table: kv\n\ngeneration\\gemini-3-pro-preview\\\
      Dataset\\dataset\\table.py:138: OperationalError\n_____________________ test_distinct_returns_unique_values\
      \ _____________________\n\n>   ???\n\nD:\\桌面\\RealAppCodeBench_generic_eval\\\
      tests\\Dataset\\functional_test.py:330: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <dataset.table.Table\
      \ object at 0x00000217314B64F0>\nrows = [{'c': 'red'}, {'c': 'red'}, {'c': 'blue'}],\
      \ chunk_size = None\n\n    def insert_many(self, rows, chunk_size=None):\n \
      \       \"\"\"\n        Insert multiple rows.\n        \"\"\"\n        if not\
      \ rows:\n            return\n    \n        # Normalize rows to dicts\n     \
      \   rows = [dict(r) for r in rows]\n    \n        # 1. Ensure table exists based\
      \ on the first row (or union of keys if we were fancy,\n        #    but standard\
      \ dataset often just looks at what's coming in).\n        #    To be safe, we\
      \ check schema against all keys in the batch.\n        all_keys = set()\n  \
      \      for r in rows:\n            all_keys.update(r.keys())\n    \n       \
      \ # Create a dummy row with all keys to ensure schema\n        dummy_schema_row\
      \ = {k: rows[0].get(k) for k in all_keys}\n    \n        with self.db.lock:\n\
      \            self._ensure_table(dummy_schema_row)\n            self._ensure_columns(dummy_schema_row)\n\
      \    \n            # 2. Construct Insert SQL\n            # We use the first\
      \ row to determine the parameterized query structure,\n            # but since\
      \ we ensured columns for ALL keys, we can just use all_keys.\n            keys\
      \ = list(all_keys)\n            placeholders = [f':{k}' for k in keys]\n   \
      \         columns_sql = \", \".join(f'\"{k}\"' for k in keys)\n            values_sql\
      \ = \", \".join(placeholders)\n    \n            sql = f'INSERT INTO \"{self.name}\"\
      \ ({columns_sql}) VALUES ({values_sql})'\n    \n            cursor = self.db._conn.cursor()\n\
      \            try:\n>               cursor.executemany(sql, rows)\nE        \
      \       sqlite3.OperationalError: no such table: colors\n\ngeneration\\gemini-3-pro-preview\\\
      Dataset\\dataset\\table.py:138: OperationalError\n===========================\
      \ short test summary info ===========================\nFAILED tests/Dataset/functional_test.py::test_insert_and_query_basic_rows\
      \ - s...\nFAILED tests/Dataset/functional_test.py::test_update_upsert_and_indexes\
      \ - sql...\nFAILED tests/Dataset/functional_test.py::test_transactions_commit_and_rollback\n\
      FAILED tests/Dataset/functional_test.py::test_insert_many_returns_ids_and_count\n\
      FAILED tests/Dataset/functional_test.py::test_find_one_missing_returns_none\n\
      FAILED tests/Dataset/functional_test.py::test_find_order_by_limit_offset - sq...\n\
      FAILED tests/Dataset/functional_test.py::test_table_all_iteration_and_row_shape\n\
      FAILED tests/Dataset/functional_test.py::test_delete_and_clear_all_rows - sql...\n\
      FAILED tests/Dataset/functional_test.py::test_drop_table_removes_from_db_tables\n\
      FAILED tests/Dataset/functional_test.py::test_raw_sql_query_with_parameters\n\
      FAILED tests/Dataset/functional_test.py::test_distinct_returns_unique_values\n\
      11 failed in 27.34s\n"
    elapsed_time_s: 52.860487
    avg_memory_mb: 33.84
    avg_cpu_percent: 0.38
    passed: 0
    failed: 11
    skipped: 0
    total: 11
    score_inputs_passed: 0
    score_inputs_failed: 11
    score_inputs_total: 11
    score_inputs_returncode: 1
    score_inputs_failed_suite: true
    score_inputs_baseline_keys:
    - functional_suite_time_s
    - functional_tests_total
  performance:
    returncode: 2
    stdout: "\n=================================== ERRORS ====================================\n\
      _____________ ERROR collecting tests/Dataset/performance_test.py ______________\n\
      C:\\Users\\86152\\AppData\\Local\\Programs\\Python\\Python39\\lib\\importlib\\\
      __init__.py:127: in import_module\n    return _bootstrap._gcd_import(name[level:],\
      \ package, level)\n<frozen importlib._bootstrap>:1030: in _gcd_import\n    ???\n\
      <frozen importlib._bootstrap>:1007: in _find_and_load\n    ???\n<frozen importlib._bootstrap>:986:\
      \ in _find_and_load_unlocked\n    ???\n<frozen importlib._bootstrap>:680: in\
      \ _load_unlocked\n    ???\nC:\\Users\\86152\\AppData\\Local\\Programs\\Python\\\
      Python39\\lib\\site-packages\\_pytest\\assertion\\rewrite.py:186: in exec_module\n\
      \    exec(co, module.__dict__)\nD:\\桌面\\RealAppCodeBench_generic_eval\\tests\\\
      Dataset\\performance_test.py:18: in <module>\n    ???\nE   RuntimeError: Unsupported\
      \ DATASET_TARGET value: generated\n=========================== short test summary\
      \ info ===========================\nERROR tests/Dataset/performance_test.py\
      \ - RuntimeError: Unsupported DATASET_T...\n!!!!!!!!!!!!!!!!!!! Interrupted:\
      \ 1 error during collection !!!!!!!!!!!!!!!!!!!!\n1 error in 0.49s\n"
    elapsed_time_s: 1.667493
    avg_memory_mb: 35.76
    avg_cpu_percent: 99.0
    passed: 0
    failed: 0
    skipped: 0
    total: 1
    score_inputs_passed: 0
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 2
    score_inputs_failed_suite: true
    score_inputs_baseline_keys:
    - performance_suite_time_s
    - performance_tests_total
    score_inputs_baseline_time_s: 2.781374
    score_inputs_actual_time_s: 1.667493
  resource:
    returncode: 2
    stdout: "\n=================================== ERRORS ====================================\n\
      _______________ ERROR collecting tests/Dataset/resource_test.py _______________\n\
      C:\\Users\\86152\\AppData\\Local\\Programs\\Python\\Python39\\lib\\importlib\\\
      __init__.py:127: in import_module\n    return _bootstrap._gcd_import(name[level:],\
      \ package, level)\n<frozen importlib._bootstrap>:1030: in _gcd_import\n    ???\n\
      <frozen importlib._bootstrap>:1007: in _find_and_load\n    ???\n<frozen importlib._bootstrap>:986:\
      \ in _find_and_load_unlocked\n    ???\n<frozen importlib._bootstrap>:680: in\
      \ _load_unlocked\n    ???\nC:\\Users\\86152\\AppData\\Local\\Programs\\Python\\\
      Python39\\lib\\site-packages\\_pytest\\assertion\\rewrite.py:186: in exec_module\n\
      \    exec(co, module.__dict__)\nD:\\桌面\\RealAppCodeBench_generic_eval\\tests\\\
      Dataset\\resource_test.py:18: in <module>\n    ???\nE   RuntimeError: Unsupported\
      \ DATASET_TARGET value: generated\n=========================== short test summary\
      \ info ===========================\nERROR tests/Dataset/resource_test.py - RuntimeError:\
      \ Unsupported DATASET_TARG...\n!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during\
      \ collection !!!!!!!!!!!!!!!!!!!!\n1 error in 0.50s\n"
    elapsed_time_s: 1.701195
    avg_memory_mb: 36.16
    avg_cpu_percent: 99.0
    passed: 0
    failed: 0
    skipped: 0
    total: 1
    score_inputs_passed: 0
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 2
    score_inputs_failed_suite: true
    score_inputs_baseline_keys:
    - avg_cpu_percent
    - avg_memory_mb
    - resource_suite_time_s
    - resource_tests_total
    score_inputs_baseline_mem_mb: 55.98
    score_inputs_baseline_cpu_pct: 100.6
    score_inputs_actual_mem_mb: 36.16
    score_inputs_actual_cpu_pct: 99.0
  robustness:
    returncode: 0
    stdout: "......                                                              \
      \     [100%]\n============================== warnings summary ===============================\n\
      ..\\RealAppCodeBench_generic_eval\\tests\\Dataset\\robustness_test.py:92\n \
      \ D:\\桌面\\RealAppCodeBench_generic_eval\\tests\\Dataset\\robustness_test.py:92:\
      \ PytestUnknownMarkWarning: Unknown pytest.mark.timeout - is this a typo?  You\
      \ can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n\
      \n..\\RealAppCodeBench_generic_eval\\tests\\Dataset\\robustness_test.py:102\n\
      \  D:\\桌面\\RealAppCodeBench_generic_eval\\tests\\Dataset\\robustness_test.py:102:\
      \ PytestUnknownMarkWarning: Unknown pytest.mark.timeout - is this a typo?  You\
      \ can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n\
      \n..\\RealAppCodeBench_generic_eval\\tests\\Dataset\\robustness_test.py:120\n\
      \  D:\\桌面\\RealAppCodeBench_generic_eval\\tests\\Dataset\\robustness_test.py:120:\
      \ PytestUnknownMarkWarning: Unknown pytest.mark.timeout - is this a typo?  You\
      \ can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n\
      \n..\\RealAppCodeBench_generic_eval\\tests\\Dataset\\robustness_test.py:135\n\
      \  D:\\桌面\\RealAppCodeBench_generic_eval\\tests\\Dataset\\robustness_test.py:135:\
      \ PytestUnknownMarkWarning: Unknown pytest.mark.timeout - is this a typo?  You\
      \ can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n\
      \n..\\RealAppCodeBench_generic_eval\\tests\\Dataset\\robustness_test.py:149\n\
      \  D:\\桌面\\RealAppCodeBench_generic_eval\\tests\\Dataset\\robustness_test.py:149:\
      \ PytestUnknownMarkWarning: Unknown pytest.mark.timeout - is this a typo?  You\
      \ can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n\
      \n..\\RealAppCodeBench_generic_eval\\tests\\Dataset\\robustness_test.py:163\n\
      \  D:\\桌面\\RealAppCodeBench_generic_eval\\tests\\Dataset\\robustness_test.py:163:\
      \ PytestUnknownMarkWarning: Unknown pytest.mark.timeout - is this a typo?  You\
      \ can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n\
      \n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n\
      6 passed, 6 warnings in 0.14s\n"
    elapsed_time_s: 1.426756
    avg_memory_mb: 31.95
    avg_cpu_percent: 101.2
    passed: 6
    failed: 0
    skipped: 0
    total: 6
    score_inputs_passed: 6
    score_inputs_failed: 0
    score_inputs_total: 6
    score_inputs_returncode: 0
    score_inputs_failed_suite: false
    score_inputs_baseline_keys:
    - robustness_suite_time_s
    - robustness_tests_total
  security:
    returncode: 0
    stdout: 'SECURITY_METRICS high_risk_count=0.0 files_scanned=3.0 total_loc=349.0

      .

      1 passed in 0.11s

      '
    elapsed_time_s: 1.252183
    avg_memory_mb: 32.16
    avg_cpu_percent: 100.0
    passed: 1
    failed: 0
    skipped: 0
    total: 1
    metrics:
      high_risk_count: 0.0
      files_scanned: 3.0
      total_loc: 349.0
    score_inputs_passed: 1
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 0
    score_inputs_failed_suite: false
    score_inputs_baseline_keys:
    - metrics
    - security_suite_time_s
    - security_tests_total
    score_inputs_baseline_high_risk_count: 0.0
    score_inputs_generated_high_risk_count: 0.0
  maintainability:
    returncode: 0
    stdout: 'MAINT_METRICS mi_min=18.7540 files_scanned=3.0 total_loc=349.0 max_cc=10.0

      .

      1 passed in 0.13s

      '
    elapsed_time_s: 1.305258
    avg_memory_mb: 32.37
    avg_cpu_percent: 98.7
    passed: 1
    failed: 0
    skipped: 0
    total: 1
    metrics:
      mi_min: 18.754
      files_scanned: 3.0
      total_loc: 349.0
      max_cc: 10.0
    score_inputs_passed: 1
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 0
    score_inputs_failed_suite: false
    score_inputs_baseline_keys:
    - maintainability_suite_time_s
    - maintainability_tests_total
    - metrics
    score_inputs_baseline_mi_min: 7.7184
    score_inputs_generated_mi_min: 18.754
    score_inputs_ratio_g_over_b: 2.4297781923714763
baseline_metrics:
  performance:
    performance_suite_time_s: 2.781374
    performance_tests_total: 2
  resource:
    resource_suite_time_s: 2.854474
    resource_tests_total: 2
    avg_memory_mb: 55.98
    avg_cpu_percent: 100.6
  functional:
    functional_suite_time_s: 4.552087
    functional_tests_total: 11
  robustness:
    robustness_suite_time_s: 2.083828
    robustness_tests_total: 6
  security:
    security_suite_time_s: 1.286846
    security_tests_total: 1
    metrics:
      high_risk_count: 0.0
      files_scanned: 6.0
      total_loc: 1131.0
  maintainability:
    maintainability_suite_time_s: 1.311875
    maintainability_tests_total: 1
    metrics:
      mi_min: 7.7184
      files_scanned: 6.0
      total_loc: 1131.0
      max_cc: 16.0
pytest_logs_dir: D:\桌面\RAL-Bench\results\Dataset\pytest_logs

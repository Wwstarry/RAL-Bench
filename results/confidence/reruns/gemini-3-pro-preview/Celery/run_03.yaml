project_name: Celery
task_file: D:\桌面\RAL-Bench\tasks\Celery\celery.yaml
generated_repo: D:\桌面\RAL-Bench\generation\gemini-3-pro-preview\Celery
timestamp: '2026-01-21 22:09:02'
functional_score: 1.0
non_functional_score: 0.6276
non_functional_subscores:
  maintainability: 0.0
  security: 1.0
  robustness: 1.0
  performance: 0.903
  resource: 0.9937
non_functional_weights:
  maintainability: 0.36
  security: 0.24
  robustness: 0.16
  performance: 0.12
  resource: 0.12
results:
  functional:
    returncode: 0
    stdout: '..........                                                               [100%]

      10 passed in 1.80s

      '
    elapsed_time_s: 3.101276
    avg_memory_mb: 43.17
    avg_cpu_percent: 99.5
    passed: 10
    failed: 0
    skipped: 0
    total: 10
    score_inputs_passed: 10
    score_inputs_failed: 0
    score_inputs_total: 10
    score_inputs_returncode: 0
    score_inputs_failed_suite: false
    score_inputs_baseline_keys:
    - functional_suite_time_s
    - functional_tests_total
  performance:
    returncode: 0
    stdout: "..                                                                  \
      \     [100%]\n============================== warnings summary ===============================\n\
      ..\\RealAppCodeBench_generic_eval\\tests\\Celery\\performance_test.py:47\n \
      \ D:\\桌面\\RealAppCodeBench_generic_eval\\tests\\Celery\\performance_test.py:47:\
      \ PytestUnknownMarkWarning: Unknown pytest.mark.performance - is this a typo?\
      \  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n\
      \n..\\RealAppCodeBench_generic_eval\\tests\\Celery\\performance_test.py:69\n\
      \  D:\\桌面\\RealAppCodeBench_generic_eval\\tests\\Celery\\performance_test.py:69:\
      \ PytestUnknownMarkWarning: Unknown pytest.mark.performance - is this a typo?\
      \  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n\
      \n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n\
      2 passed, 2 warnings in 2.96s\n"
    elapsed_time_s: 4.174056
    avg_memory_mb: 44.12
    avg_cpu_percent: 99.6
    passed: 2
    failed: 0
    skipped: 0
    total: 2
    score_inputs_passed: 2
    score_inputs_failed: 0
    score_inputs_total: 2
    score_inputs_returncode: 0
    score_inputs_failed_suite: false
    score_inputs_baseline_keys:
    - performance_suite_time_s
    - performance_tests_total
    score_inputs_baseline_time_s: 3.769039
    score_inputs_actual_time_s: 4.174056
  resource:
    returncode: 0
    stdout: "..                                                                  \
      \     [100%]\n============================== warnings summary ===============================\n\
      ..\\RealAppCodeBench_generic_eval\\tests\\Celery\\resource_test.py:48\n  D:\\\
      桌面\\RealAppCodeBench_generic_eval\\tests\\Celery\\resource_test.py:48: PytestUnknownMarkWarning:\
      \ Unknown pytest.mark.resource - is this a typo?  You can register custom marks\
      \ to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n\
      \n..\\RealAppCodeBench_generic_eval\\tests\\Celery\\resource_test.py:85\n  D:\\\
      桌面\\RealAppCodeBench_generic_eval\\tests\\Celery\\resource_test.py:85: PytestUnknownMarkWarning:\
      \ Unknown pytest.mark.resource - is this a typo?  You can register custom marks\
      \ to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n\
      \n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n\
      2 passed, 2 warnings in 8.71s\n"
    elapsed_time_s: 9.95891
    avg_memory_mb: 44.79
    avg_cpu_percent: 99.5
    passed: 2
    failed: 0
    skipped: 0
    total: 2
    score_inputs_passed: 2
    score_inputs_failed: 0
    score_inputs_total: 2
    score_inputs_returncode: 0
    score_inputs_failed_suite: false
    score_inputs_baseline_keys:
    - avg_cpu_percent
    - avg_memory_mb
    - resource_suite_time_s
    - resource_tests_total
    score_inputs_baseline_mem_mb: 44.36
    score_inputs_baseline_cpu_pct: 99.2
    score_inputs_actual_mem_mb: 44.79
    score_inputs_actual_cpu_pct: 99.5
  robustness:
    returncode: 0
    stdout: "......                                                              \
      \     [100%]\n============================== warnings summary ===============================\n\
      tests/Celery/robustness_test.py::test_002_unknown_task_name_signature_errors_cleanly\n\
      \  D:\\桌面\\RAL-Bench\\repositories\\celery\\celery\\canvas.py:400: AlwaysEagerIgnored:\
      \ task_always_eager has no effect on send_task\n    return _apply(args, kwargs,\
      \ **options)\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n\
      6 passed, 1 warning in 3.61s\n"
    elapsed_time_s: 4.90715
    avg_memory_mb: 43.78
    avg_cpu_percent: 57.3
    passed: 6
    failed: 0
    skipped: 0
    total: 6
    score_inputs_passed: 6
    score_inputs_failed: 0
    score_inputs_total: 6
    score_inputs_returncode: 0
    score_inputs_failed_suite: false
    score_inputs_baseline_keys:
    - robustness_suite_time_s
    - robustness_tests_total
  security:
    returncode: 0
    stdout: 'SECURITY_METRICS high_risk_count=0.0 files_scanned=0.0 total_loc=0.0

      .

      1 passed in 0.10s

      '
    elapsed_time_s: 1.257454
    avg_memory_mb: 31.05
    avg_cpu_percent: 95.9
    passed: 1
    failed: 0
    skipped: 0
    total: 1
    metrics:
      high_risk_count: 0.0
      files_scanned: 0.0
      total_loc: 0.0
    score_inputs_passed: 1
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 0
    score_inputs_failed_suite: false
    score_inputs_baseline_keys:
    - metrics
    - security_suite_time_s
    - security_tests_total
    score_inputs_baseline_high_risk_count: 4.0
    score_inputs_generated_high_risk_count: 0.0
  maintainability:
    returncode: 1
    stdout: 'MAINT_METRICS mi_min=0.0000 files_scanned=0.0 total_loc=0.0 max_cc=0.0

      F

      ================================== FAILURES ===================================

      _____________________ test_maintainability_metrics_smoke ______________________


      >   ???

      E   AssertionError: No python files scanned under: D:\桌面\RAL-Bench\generation\gemini-3-pro-preview\Celery

      E   assert 0.0 >= 1.0


      D:\桌面\RealAppCodeBench_generic_eval\tests\_generic\maintainability_test.py:380:
      AssertionError

      =========================== short test summary info ===========================

      FAILED tests/_generic/maintainability_test.py::test_maintainability_metrics_smoke

      1 failed in 0.29s

      '
    elapsed_time_s: 1.449845
    avg_memory_mb: 31.45
    avg_cpu_percent: 100.0
    passed: 0
    failed: 1
    skipped: 0
    total: 1
    metrics:
      mi_min: 0.0
      files_scanned: 0.0
      total_loc: 0.0
      max_cc: 0.0
    score_inputs_passed: 0
    score_inputs_failed: 1
    score_inputs_total: 1
    score_inputs_returncode: 1
    score_inputs_failed_suite: true
    score_inputs_baseline_keys:
    - maintainability_suite_time_s
    - maintainability_tests_total
    - metrics
    score_inputs_baseline_mi_min: 0.0
    score_inputs_generated_mi_min: 0.0
baseline_metrics:
  functional:
    functional_suite_time_s: 2.734527
    functional_tests_total: 10
  performance:
    performance_suite_time_s: 3.769039
    performance_tests_total: 2
  resource:
    resource_suite_time_s: 9.788972
    resource_tests_total: 2
    avg_memory_mb: 44.36
    avg_cpu_percent: 99.2
  robustness:
    robustness_suite_time_s: 4.859838
    robustness_tests_total: 6
  security:
    security_suite_time_s: 2.146028
    security_tests_total: 1
    metrics:
      high_risk_count: 4.0
      files_scanned: 161.0
      total_loc: 32227.0
  maintainability:
    maintainability_suite_time_s: 4.314273
    maintainability_tests_total: 1
    metrics:
      mi_min: 0.0
      files_scanned: 156.0
      total_loc: 32227.0
      max_cc: 66.0
pytest_logs_dir: D:\桌面\RAL-Bench\results\Celery\pytest_logs

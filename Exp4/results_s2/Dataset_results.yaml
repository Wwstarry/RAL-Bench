project_name: Dataset
task_file: D:\桌面\RealAppCodeBench_generic_eval\tasks\Dataset\dataset.yaml
generated_repo: D:\桌面\RealAppCodeBench_generic_eval\generation_m3\Dataset
timestamp: '2026-01-15 11:09:12'
functional_score: 0.6364
non_functional_score: 0.6497
non_functional_subscores:
  maintainability: 0.6935
  security: 1.0
  robustness: 1.0
  performance: 0.0
  resource: 0.0
non_functional_weights:
  maintainability: 0.36
  security: 0.24
  robustness: 0.16
  performance: 0.12
  resource: 0.12
results:
  functional:
    returncode: 1
    stdout: "F....F..F.F                                                         \
      \     [100%]\n================================== FAILURES ===================================\n\
      ______________________ test_insert_and_query_basic_rows _______________________\n\
      \n    def test_insert_and_query_basic_rows() -> None:\n        db = create_in_memory_db()\n\
      \        table = db[\"users\"]\n    \n        table.insert({\"name\": \"Alice\"\
      , \"age\": 30, \"country\": \"DE\"})\n        table.insert({\"name\": \"Bob\"\
      , \"age\": 41, \"country\": \"US\", \"active\": True})\n        table.insert({\"\
      name\": \"Charlie\", \"age\": 41, \"country\": \"US\", \"active\": False})\n\
      \    \n        assert \"id\" in _table_columns(table)\n        assert \"name\"\
      \ in _table_columns(table)\n        assert \"country\" in _table_columns(table)\n\
      \        assert len(table) == 3\n    \n        alice = table.find_one(name=\"\
      Alice\")\n        assert alice is not None\n        assert alice[\"country\"\
      ] == \"DE\"\n    \n>       older = list(table.find(age={\">=\": 40}))\n\ntests\\\
      Dataset\\functional_test.py:155: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ngeneration_m3\\Dataset\\dataset\\table.py:193:\
      \ in find\n    cur = self.db.execute(f\"SELECT * FROM {self._quoted}{where_sql}\"\
      , params)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _ _ _ _\n\nself = <dataset.database.Database object at 0x0000024E985B1CD0>\n\
      sql = 'SELECT * FROM \"users\" WHERE \"age\" = :w_0', params = {'w_0': {'>=':\
      \ 40}}\n\n    def execute(self, sql: str, params: Optional[Dict[str, Any]] =\
      \ None) -> sqlite3.Cursor:\n        with self._lock:\n>           return self._conn.execute(sql,\
      \ params or {})\nE           sqlite3.InterfaceError: Error binding parameter\
      \ :w_0 - probably unsupported type.\n\ngeneration_m3\\Dataset\\dataset\\database.py:138:\
      \ InterfaceError\n_______________________ test_find_order_by_limit_offset _______________________\n\
      \n    def test_find_order_by_limit_offset() -> None:\n        db = create_in_memory_db()\n\
      \        table = db[\"nums\"]\n        for i in range(10):\n            table.insert({\"\
      n\": i})\n    \n        rows = list(table.find(order_by=\"n\", _limit=3, _offset=4))\n\
      >       assert [r[\"n\"] for r in rows] == [4, 5, 6]\nE       assert [] == [4,\
      \ 5, 6]\nE         \nE         Right contains 3 more items, first extra item:\
      \ 4\nE         Use -v to get more diff\n\ntests\\Dataset\\functional_test.py:249:\
      \ AssertionError\n___________________ test_drop_table_removes_from_db_tables\
      \ ____________________\n\n    def test_drop_table_removes_from_db_tables() ->\
      \ None:\n        db = create_in_memory_db()\n        table = db[\"to_drop\"\
      ]\n        table.insert({\"x\": 1})\n    \n>       assert \"to_drop\" in _db_tables(db)\n\
      E       AssertionError: assert 'to_drop' in []\nE        +  where [] = _db_tables(<dataset.database.Database\
      \ object at 0x0000024E96AE1AC0>)\n\ntests\\Dataset\\functional_test.py:301:\
      \ AssertionError\n_____________________ test_distinct_returns_unique_values\
      \ _____________________\n\n    def test_distinct_returns_unique_values() ->\
      \ None:\n        db = create_in_memory_db()\n        table = db[\"colors\"]\n\
      \        table.insert_many([{\"c\": \"red\"}, {\"c\": \"red\"}, {\"c\": \"blue\"\
      }])\n    \n        distinct = list(table.distinct(\"c\"))\n>       values =\
      \ {r[\"c\"] for r in distinct}\n\ntests\\Dataset\\functional_test.py:333: \n\
      _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _\n\n.0 = <list_iterator object at 0x0000024E986173A0>\n\n>   values = {r[\"\
      c\"] for r in distinct}\nE   TypeError: string indices must be integers\n\n\
      tests\\Dataset\\functional_test.py:333: TypeError\n===========================\
      \ short test summary info ===========================\nFAILED tests/Dataset/functional_test.py::test_insert_and_query_basic_rows\
      \ - s...\nFAILED tests/Dataset/functional_test.py::test_find_order_by_limit_offset\
      \ - as...\nFAILED tests/Dataset/functional_test.py::test_drop_table_removes_from_db_tables\n\
      FAILED tests/Dataset/functional_test.py::test_distinct_returns_unique_values\n\
      4 failed, 7 passed in 3.87s\n"
    elapsed_time_s: 5.242748
    avg_memory_mb: 34.81
    avg_cpu_percent: 98.2
    passed: 7
    failed: 4
    skipped: 0
    total: 11
    score_inputs_passed: 7
    score_inputs_failed: 4
    score_inputs_total: 11
    score_inputs_returncode: 1
    score_inputs_failed_suite: true
    score_inputs_baseline_keys:
    - functional_suite_time_s
    - functional_tests_total
  performance:
    returncode: 2
    stdout: "\n=================================== ERRORS ====================================\n\
      _____________ ERROR collecting tests/Dataset/performance_test.py ______________\n\
      tests\\Dataset\\performance_test.py:18: in <module>\n    raise RuntimeError(f\"\
      Unsupported DATASET_TARGET value: {TARGET_ENV}\")\nE   RuntimeError: Unsupported\
      \ DATASET_TARGET value: generated\n=========================== short test summary\
      \ info ===========================\nERROR tests/Dataset/performance_test.py\
      \ - RuntimeError: Unsupported DATASET_T...\n!!!!!!!!!!!!!!!!!!! Interrupted:\
      \ 1 error during collection !!!!!!!!!!!!!!!!!!!!\n1 error in 0.56s\n"
    elapsed_time_s: 2.035805
    avg_memory_mb: 35.79
    avg_cpu_percent: 100.8
    passed: 0
    failed: 0
    skipped: 0
    total: 1
    score_inputs_passed: 0
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 2
    score_inputs_failed_suite: true
    score_inputs_baseline_keys:
    - performance_suite_time_s
    - performance_tests_total
    score_inputs_baseline_time_s: 2.781374
    score_inputs_actual_time_s: 2.035805
  resource:
    returncode: 2
    stdout: "\n=================================== ERRORS ====================================\n\
      _______________ ERROR collecting tests/Dataset/resource_test.py _______________\n\
      tests\\Dataset\\resource_test.py:18: in <module>\n    raise RuntimeError(f\"\
      Unsupported DATASET_TARGET value: {TARGET_ENV}\")\nE   RuntimeError: Unsupported\
      \ DATASET_TARGET value: generated\n=========================== short test summary\
      \ info ===========================\nERROR tests/Dataset/resource_test.py - RuntimeError:\
      \ Unsupported DATASET_TARG...\n!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during\
      \ collection !!!!!!!!!!!!!!!!!!!!\n1 error in 0.59s\n"
    elapsed_time_s: 1.983211
    avg_memory_mb: 34.98
    avg_cpu_percent: 103.3
    passed: 0
    failed: 0
    skipped: 0
    total: 1
    score_inputs_passed: 0
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 2
    score_inputs_failed_suite: true
    score_inputs_baseline_keys:
    - avg_cpu_percent
    - avg_memory_mb
    - resource_suite_time_s
    - resource_tests_total
    score_inputs_baseline_mem_mb: 55.98
    score_inputs_baseline_cpu_pct: 100.6
    score_inputs_actual_mem_mb: 34.98
    score_inputs_actual_cpu_pct: 103.3
  robustness:
    returncode: 0
    stdout: "......                                                              \
      \     [100%]\n============================== warnings summary ===============================\n\
      tests\\Dataset\\robustness_test.py:92\n  D:\\桌面\\RealAppCodeBench_generic_eval\\\
      tests\\Dataset\\robustness_test.py:92: PytestUnknownMarkWarning: Unknown pytest.mark.timeout\
      \ - is this a typo?  You can register custom marks to avoid this warning - for\
      \ details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.timeout(10)\n\
      \ntests\\Dataset\\robustness_test.py:102\n  D:\\桌面\\RealAppCodeBench_generic_eval\\\
      tests\\Dataset\\robustness_test.py:102: PytestUnknownMarkWarning: Unknown pytest.mark.timeout\
      \ - is this a typo?  You can register custom marks to avoid this warning - for\
      \ details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.timeout(10)\n\
      \ntests\\Dataset\\robustness_test.py:120\n  D:\\桌面\\RealAppCodeBench_generic_eval\\\
      tests\\Dataset\\robustness_test.py:120: PytestUnknownMarkWarning: Unknown pytest.mark.timeout\
      \ - is this a typo?  You can register custom marks to avoid this warning - for\
      \ details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.timeout(10)\n\
      \ntests\\Dataset\\robustness_test.py:135\n  D:\\桌面\\RealAppCodeBench_generic_eval\\\
      tests\\Dataset\\robustness_test.py:135: PytestUnknownMarkWarning: Unknown pytest.mark.timeout\
      \ - is this a typo?  You can register custom marks to avoid this warning - for\
      \ details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.timeout(10)\n\
      \ntests\\Dataset\\robustness_test.py:149\n  D:\\桌面\\RealAppCodeBench_generic_eval\\\
      tests\\Dataset\\robustness_test.py:149: PytestUnknownMarkWarning: Unknown pytest.mark.timeout\
      \ - is this a typo?  You can register custom marks to avoid this warning - for\
      \ details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.timeout(10)\n\
      \ntests\\Dataset\\robustness_test.py:163\n  D:\\桌面\\RealAppCodeBench_generic_eval\\\
      tests\\Dataset\\robustness_test.py:163: PytestUnknownMarkWarning: Unknown pytest.mark.timeout\
      \ - is this a typo?  You can register custom marks to avoid this warning - for\
      \ details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.timeout(10)\n\
      \n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n\
      6 passed, 6 warnings in 0.20s\n"
    elapsed_time_s: 1.677937
    avg_memory_mb: 32.19
    avg_cpu_percent: 99.0
    passed: 6
    failed: 0
    skipped: 0
    total: 6
    score_inputs_passed: 6
    score_inputs_failed: 0
    score_inputs_total: 6
    score_inputs_returncode: 0
    score_inputs_failed_suite: false
    score_inputs_baseline_keys:
    - robustness_suite_time_s
    - robustness_tests_total
  security:
    returncode: 0
    stdout: 'SECURITY_METRICS high_risk_count=0.0 files_scanned=3.0 total_loc=383.0

      .

      1 passed in 0.15s

      '
    elapsed_time_s: 1.527572
    avg_memory_mb: 31.73
    avg_cpu_percent: 98.9
    passed: 1
    failed: 0
    skipped: 0
    total: 1
    metrics:
      high_risk_count: 0.0
      files_scanned: 3.0
      total_loc: 383.0
    score_inputs_passed: 1
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 0
    score_inputs_failed_suite: false
    score_inputs_baseline_keys:
    - metrics
    - security_suite_time_s
    - security_tests_total
    score_inputs_baseline_high_risk_count: 0.0
    score_inputs_generated_high_risk_count: 0.0
  maintainability:
    returncode: 0
    stdout: 'MAINT_METRICS mi_min=17.4666 files_scanned=3.0 total_loc=383.0 max_cc=21.0

      .

      1 passed in 0.19s

      '
    elapsed_time_s: 1.549361
    avg_memory_mb: 32.01
    avg_cpu_percent: 100.0
    passed: 1
    failed: 0
    skipped: 0
    total: 1
    metrics:
      mi_min: 17.4666
      files_scanned: 3.0
      total_loc: 383.0
      max_cc: 21.0
    score_inputs_passed: 1
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 0
    score_inputs_failed_suite: false
    score_inputs_baseline_keys:
    - maintainability_suite_time_s
    - maintainability_tests_total
    - metrics
    score_inputs_baseline_mi_min: 7.7184
    score_inputs_generated_mi_min: 17.4666
    score_inputs_ratio_g_over_b: 2.2629819651741294
baseline_metrics:
  performance:
    performance_suite_time_s: 2.781374
    performance_tests_total: 2
  resource:
    resource_suite_time_s: 2.854474
    resource_tests_total: 2
    avg_memory_mb: 55.98
    avg_cpu_percent: 100.6
  functional:
    functional_suite_time_s: 4.552087
    functional_tests_total: 11
  robustness:
    robustness_suite_time_s: 2.083828
    robustness_tests_total: 6
  security:
    security_suite_time_s: 1.286846
    security_tests_total: 1
    metrics:
      high_risk_count: 0.0
      files_scanned: 6.0
      total_loc: 1131.0
  maintainability:
    maintainability_suite_time_s: 1.311875
    maintainability_tests_total: 1
    metrics:
      mi_min: 7.7184
      files_scanned: 6.0
      total_loc: 1131.0
      max_cc: 16.0
pytest_logs_dir: D:\桌面\RealAppCodeBench_generic_eval\results\Dataset\pytest_logs

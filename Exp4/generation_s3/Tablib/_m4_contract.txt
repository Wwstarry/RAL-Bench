1) Repository layout
- Provide a top-level Python package directory named: tablib/
- Required modules/files (must exist and be importable):
  - tablib/__init__.py
  - tablib/core.py
  - tablib/formats/_csv.py
  - tablib/formats/_json.py
- Import behavior requirements:
  - `import tablib` must work.
  - `import tablib.core` must work.
  - `import tablib.formats._csv` and `import tablib.formats._json` must work.
  - `tablib.Dataset` and `tablib.Databook` must be available at top-level (re-exported from tablib/__init__.py).

2) Public API surface
2.1 tablib (package)
- Expose:
  - class tablib.Dataset (defined in tablib.core)
  - class tablib.Databook (defined in tablib.core)
- Optional: expose version metadata (not required by tests).

2.2 tablib.core
- class Dataset:
  - Constructor:
    - Dataset(*rows, headers=None)
      - rows: zero or more row iterables (each row is an iterable of values)
      - headers: optional iterable of column names
  - Attributes / properties:
    - headers (get/set): list-like sequence of column names (or None/empty when unset)
    - height (property): int number of rows
    - width (property): int number of columns
    - title (optional attribute, default None): used by Databook JSON export/import
    - dict (property): list of dicts mapping header->cell for each row
  - Row access:
    - __getitem__(key):
      - if key is slice: return list (or tuple) of row tuples for the slice
      - if key is int: (optional) may return a single row tuple; not required unless tests use it
      - if key is str: return an iterable (e.g., list) of values for that column (dictionary-style column access)
  - Row/column mutation:
    - append(row): add one row
    - append_col(values, header=None): add one column
  - Serialization:
    - export(fmt): return string; must support fmt='csv' and fmt='json'
    - csv property:
      - getter: return CSV string of dataset
      - setter: accept CSV string, replace dataset content (headers + rows)
    - json property:
      - getter: return JSON string of dataset
      - setter: accept JSON string, replace dataset content (headers + rows)

- class Databook:
  - Constructor:
    - Databook(datasets)
      - datasets: iterable of Dataset instances
  - Attributes / properties:
    - size: int number of sheets
  - Sheet access:
    - sheets(): iterable of Dataset objects in stored order
    - __iter__ (optional but recommended): iterate over datasets in order
  - Serialization:
    - export(fmt): must support fmt='json' returning JSON string for entire book
    - json property setter:
      - setter: accept JSON string produced by export('json') and replace internal datasets with restored sheets (titles/headers/rows)
    - json property getter (optional but recommended): return same as export('json') if implemented; tests explicitly require setter and export.

2.3 tablib.formats._csv
- Provide pure-python helper functions used by Dataset core:
  - export_dataset(dataset) -> str (or similar name; internal use)
  - import_dataset(dataset, csv_text) -> None, populating dataset
- Must not require external dependencies.

2.4 tablib.formats._json
- Provide pure-python helper functions used by Dataset/Databook core:
  - export_dataset(dataset) -> str
  - import_dataset(dataset, json_text) -> None
  - export_databook(book) -> str
  - import_databook(book, json_text) -> None
- Must not require external dependencies.

3) Behavioral contract (I/O, invariants, edge cases, error handling)
3.1 Core data model and invariants
- Dataset stores:
  - headers: ordered list of column names (strings recommended). May be empty list when unspecified.
  - rows: ordered sequence of rows; each row is stored/returned as a tuple to ensure stable ordering/immutability when exposed.
- Shape:
  - height == number of stored rows.
  - width == number of columns:
    - if headers is set and non-empty: width == len(headers)
    - else if no headers but rows exist: width == max row length at time of insertion OR consistent inferred width (preferred: infer from first row length and normalize others)
  - After any mutation (append/append_col/import), height/width must reflect current content immediately.
- Normalization:
  - Rows may be provided as any iterable; they are stored as tuples.
  - If appended row length is shorter than current width: pad with None to width.
  - If appended row length is longer than current width:
    - If headers exist: extend headers with autogenerated names (e.g., "colN") OR allow width growth without headers by extending width; recommended: extend headers with "col{index}" to maintain dict property.
    - Ensure existing rows are padded to new width.
  - If headers are set to a new sequence:
    - width becomes len(new_headers)
    - existing rows are padded/truncated to match new width to keep stable rectangular shape.

3.2 Dataset constructor behavior
- Dataset(*rows, headers=None):
  - If headers is provided:
    - store headers as list(headers)
    - set width to len(headers)
  - Insert given rows in order using same normalization rules as append (padding/truncation as needed).
  - If headers not provided:
    - start with empty headers list
    - infer width from first row length (if any rows), then create default headers to support .dict property (recommended): "col1", "col2", ...
    - Alternative acceptable only if tests always provide headers: keep headers empty and ensure dict uses positional keys; however tests require header->cell mapping, so default headers should be created when missing.

3.3 headers attribute (get/set)
- Getter returns the stored ordered list of header names (not a generator).
- Setter accepts:
  - None: treat as empty header list; width then inferred from current rows (preferred: keep current width and create default headers for dict export) OR set headers to [] and keep width unchanged.
  - Iterable of names: store as list; adjust width and normalize rows to match.
- Dict representation must always map header names to values, so if headers are empty but width>0, implementation must generate stable default headers for dict export (either at set-time or on-demand).

3.4 Row access and slicing
- dataset[start:stop] must return a sequence (list or tuple) of row tuples in insertion order, reflecting the slice.
- dataset['column_name'] returns a sequence (list recommended) of values for that column across all rows in row order.
  - Column lookup uses headers to find index; KeyError is acceptable if column_name not found.
  - If headers contain duplicates, first match is used (acceptable).
- dataset[i] integer access is optional; if implemented, return row tuple.

3.5 append(row)
- Adds a single row at the end.
- Must update height.
- Must ensure rectangular shape:
  - pad/truncate to width as defined by headers/width rules.
- If dataset currently has width=0 and no headers:
  - infer width from row length
  - create default headers to width to keep dict functioning.

3.6 append_col(values, header=None)
- Adds a new column at the end.
- values is an iterable; must be materialized to list to support length checks.
- Length handling:
  - If dataset has existing rows (height > 0):
    - values length must equal height; if shorter, pad missing with None; if longer, ignore extras OR raise (preferred: pad/ignore rather than raise to be test-friendly).
  - If dataset has no rows (height == 0):
    - adding a column should create rows if values has items:
      - height becomes len(values)
      - create one-cell rows for each value, and width becomes previous width+1.
- Header handling:
  - If headers exist or are expected for dict:
    - if header is provided, append it to headers
    - else append autogenerated header name (e.g., "col{new_index}") to keep width aligned
- Must update width and keep dict representation coherent.

3.7 dict property
- Returns list of dicts, one per row, in row order.
- Each dict maps header name -> corresponding cell value.
- Ordering stability:
  - Keys should appear in header order (Python 3.7+ dict preserves insertion order); create dict by iterating headers in order.
- If headers length < width (should not happen after normalization), generate missing header names deterministically.
- Values may be of any type; importers may coerce to strings (acceptable).
- If there are no headers but width>0, dict must still return meaningful keys (default header generation required).

3.8 Dataset export/import (CSV and JSON)
- dataset.export(fmt):
  - fmt case-insensitive recommended; must accept 'csv' and 'json'.
  - Return type: str (text).
  - For unsupported fmt: raise ValueError (or KeyError) with a clear message.
- dataset.csv getter:
  - Return CSV string equivalent to export('csv').
- dataset.csv setter:
  - Accept CSV text (str), replace dataset headers and rows with parsed content.
  - CSV parsing expectations:
    - First row in CSV is treated as headers.
    - Remaining rows are data rows.
    - Use Python built-in csv module with newline handling; support quoted fields and commas.
    - Empty input results in empty dataset (headers=[], rows=[]).
    - Preserve row ordering.
    - Values may be returned as strings (no type inference required).
- dataset.json getter:
  - Return JSON string equivalent to export('json').
- dataset.json setter:
  - Accept JSON text (str), replace dataset content with parsed JSON.
  - JSON structure for dataset (must be stable and roundtrippable):
    - Recommended canonical shape:
      {
        "title": <optional title or null>,
        "headers": [..],
        "data": [[..], [..], ...]
      }
    - Importer must accept that exact structure.
  - Values may be kept as parsed JSON types or coerced; structural fidelity is primary.

3.9 Databook behavior
- Stores an ordered list of Dataset objects (sheets).
- book.size equals number of datasets.
- book.sheets() returns datasets in stored order (list or iterator).
- Iteration (if implemented) yields datasets in stored order.

3.10 Databook JSON export/import
- book.export('json') returns a JSON string representing entire workbook.
- JSON structure must preserve:
  - number of sheets
  - sheet titles (Dataset.title)
  - per-sheet headers
  - per-sheet row data order
- Recommended canonical shape:
  {
    "sheets": [
      {"title": "...", "headers": [...], "data": [[...], ...]},
      ...
    ]
  }
- book.json setter accepts JSON string produced by book.export('json') and replaces internal sheets accordingly.
  - Must create new Dataset instances for sheets (or clear and repopulate existing) preserving order.
  - After import, each sheet’s .dict, headers, height, width must match exported logical content.
- Error handling:
  - If JSON is invalid: propagate json.JSONDecodeError.
  - If structure missing expected keys: raise ValueError.

3.11 Performance/resource expectations (high-level)
- Avoid quadratic behavior for common operations:
  - export should iterate rows once and build output efficiently.
  - append and column access should be O(width) and O(height) respectively.
- Avoid storing redundant copies during repeated exports; however returning a new string each time is fine.
- Use only standard library (csv, json, io).

4) Acceptance checklist (verifiable bullets, map to test intent)
- Imports:
  - `import tablib` works.
  - `from tablib import Dataset, Databook` works.
  - `import tablib.core`, `import tablib.formats._csv`, `import tablib.formats._json` all work.
- Dataset construction and shape:
  - Dataset(headers=['a','b'], rows...) sets headers in order and width==2.
  - height equals number of rows inserted.
  - Slicing dataset[0:2] returns row tuples in correct order.
- Column access:
  - dataset['a'] returns values from column 'a' in row order.
  - Missing column name raises KeyError (acceptable).
- Mutation:
  - append(row) increases height by 1 and keeps rectangular shape.
  - append_col(values, header='x') increases width by 1, updates headers, and updates dict output accordingly.
- Dict representation:
  - dataset.dict returns list of dicts with keys in header order and one dict per row.
  - After mutations, dict reflects latest content.
- CSV:
  - dataset.csv getter returns a CSV string with headers as first row and correct number of data rows.
  - Assigning dataset.csv = <exported csv> restores headers, height, width, and row ordering (type coercion to strings allowed).
  - dataset.export('csv') matches dataset.csv output.
- JSON (Dataset):
  - dataset.json getter returns JSON string.
  - Assigning dataset.json = <exported json> restores headers, height, width, row ordering (types may vary).
  - dataset.export('json') matches dataset.json output.
- Databook:
  - Databook([ds1, ds2]).size == 2.
  - book.sheets() yields ds1 then ds2 (or their equivalents after import).
  - book.export('json') returns JSON that includes all sheets with titles/headers/data.
  - Setting book.json to that exported JSON restores size, titles, headers, and each sheet’s dict content equivalence.

5) Non-goals / constraints
- Pure Python only; use standard library only (no third-party dependencies).
- Implement only the core API required here; no need for XLS/XLSX/ODS/YAML/HTML formats.
- No need for advanced Tablib features (separators customization, formatting options, dataset sorting, filtering, transpose, etc.) unless required by tests.
- No external I/O; all import/export operate on in-memory strings.
- No strict type preservation required across CSV/JSON roundtrips; structural integrity (shape, headers, ordering) is required.
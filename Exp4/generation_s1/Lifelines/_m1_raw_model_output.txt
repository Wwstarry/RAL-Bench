<file:name=lifelines/__init__.py>
from __future__ import annotations

from .fitters.kaplan_meier_fitter import KaplanMeierFitter
from .fitters.coxph_fitter import CoxPHFitter
from . import datasets

__all__ = ["KaplanMeierFitter", "CoxPHFitter", "datasets"]
__version__ = "0.0.0"
</file>

<file:name=lifelines/fitters/__init__.py>
from .kaplan_meier_fitter import KaplanMeierFitter
from .coxph_fitter import CoxPHFitter

__all__ = ["KaplanMeierFitter", "CoxPHFitter"]
</file>

<file:name=lifelines/fitters/kaplan_meier_fitter.py>
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Iterable

import numpy as np
import pandas as pd


def _to_1d_array(x: Any, name: str) -> np.ndarray:
    if isinstance(x, (pd.Series, pd.Index)):
        arr = x.to_numpy()
    elif isinstance(x, pd.DataFrame):
        if x.shape[1] != 1:
            raise ValueError(f"{name} must be 1-dimensional.")
        arr = x.iloc[:, 0].to_numpy()
    else:
        arr = np.asarray(x)
    arr = np.asarray(arr).reshape(-1)
    return arr


@dataclass
class KaplanMeierFitter:
    label: str | None = None

    def __post_init__(self) -> None:
        if self.label is None:
            self.label = "KM_estimate"

        self.survival_function_: pd.DataFrame | None = None
        self.event_table_: pd.DataFrame | None = None
        self.timeline: pd.Index | None = None

    def fit(
        self,
        durations: Any,
        event_observed: Any | None = None,
        label: str | None = None,
    ) -> "KaplanMeierFitter":
        T = _to_1d_array(durations, "durations").astype(float)
        if event_observed is None:
            E = np.ones_like(T, dtype=int)
        else:
            E = _to_1d_array(event_observed, "event_observed")
            # allow bool, float, int
            E = (E.astype(float) > 0.0).astype(int)

        if T.shape[0] != E.shape[0]:
            raise ValueError("durations and event_observed must be the same length.")

        if label is not None:
            self.label = label

        mask = np.isfinite(T) & np.isfinite(E.astype(float))
        T = T[mask]
        E = E[mask].astype(int)

        if T.size == 0:
            raise ValueError("No valid observations.")

        # timeline includes all unique durations (events + censored) for robustness
        unique_times = np.unique(T)
        unique_times = unique_times[unique_times >= 0]
        unique_times.sort()
        if unique_times.size == 0 or unique_times[0] != 0.0:
            timeline = np.concatenate([[0.0], unique_times])
        else:
            timeline = unique_times

        # Precompute counts by time
        # at_risk at time t is number with T >= t
        # observed at time t is number with T == t and E==1
        # censored at time t is number with T == t and E==0
        observed = {}
        censored = {}
        for t in unique_times:
            at_t = (T == t)
            observed[t] = int(np.sum(at_t & (E == 1)))
            censored[t] = int(np.sum(at_t & (E == 0)))

        # KM estimate
        S = 1.0
        surv_vals = [1.0]
        ev_rows = []
        # at time 0, include an event table row too
        at_risk0 = int(np.sum(T >= 0.0))
        ev_rows.append((0.0, at_risk0, 0, 0))

        for t in unique_times:
            at_risk = int(np.sum(T >= t))
            d = observed.get(t, 0)
            c = censored.get(t, 0)

            # apply KM only for events
            if at_risk > 0 and d > 0:
                S *= (1.0 - d / at_risk)

            surv_vals.append(float(S))
            ev_rows.append((float(t), at_risk, int(d), int(c)))

        timeline_index = pd.Index(timeline, name="timeline")
        # surv_vals has len = 1 + len(unique_times); timeline has 1 + len(unique_times) unless unique_times started with 0
        # If unique_times includes 0, timeline == unique_times; but we still inserted initial 1.0 and then looped over unique_times
        # resulting in one extra entry. Fix by de-duplicating time 0.
        if unique_times.size > 0 and unique_times[0] == 0.0:
            # remove the initial 1.0 duplicate and align to timeline
            surv_vals = surv_vals[1:]
        # Now should align
        if len(surv_vals) != len(timeline_index):
            # fallback: align by building series on explicit times
            times = [0.0] + [float(t) for t in unique_times.tolist()]
            vals = [1.0]
            S2 = 1.0
            for t in unique_times:
                at_risk = int(np.sum(T >= t))
                d = observed.get(t, 0)
                if at_risk > 0 and d > 0:
                    S2 *= (1.0 - d / at_risk)
                vals.append(float(S2))
            s = pd.Series(vals, index=pd.Index(times, name="timeline"))
            s = s[~s.index.duplicated(keep="last")]
            s = s.reindex(timeline_index, method="ffill").fillna(1.0)
            surv = s.to_frame(name=self.label)
        else:
            surv = pd.DataFrame({self.label: np.clip(np.asarray(surv_vals, dtype=float), 0.0, 1.0)}, index=timeline_index)

        # Build event_table_ indexed by timeline (including 0 and all durations)
        # For 0 included twice in ev_rows if unique_times started with 0, dedupe keep last
        event_df = pd.DataFrame(ev_rows, columns=["event_at", "at_risk", "observed", "censored"]).set_index("event_at")
        event_df = event_df[~event_df.index.duplicated(keep="last")]
        event_df = event_df.reindex(timeline_index, method="ffill").fillna(0).astype({"at_risk": int, "observed": int, "censored": int})
        event_df.index.name = "timeline"

        self.survival_function_ = surv
        self.event_table_ = event_df
        self.timeline = timeline_index
        return self

    def predict(self, time: float) -> float:
        if self.survival_function_ is None:
            raise ValueError("Must call fit before predict.")
        try:
            t = float(time)
        except Exception as e:
            raise ValueError("time must be numeric.") from e

        if t <= 0:
            return 1.0

        sf = self.survival_function_.iloc[:, 0]
        idx = sf.index.to_numpy(dtype=float)

        # find rightmost index <= t
        pos = np.searchsorted(idx, t, side="right") - 1
        if pos < 0:
            return 1.0
        if pos >= len(sf):
            pos = len(sf) - 1
        val = float(sf.iloc[pos])
        if not np.isfinite(val):
            val = float(np.nan_to_num(val, nan=1.0))
        return float(np.clip(val, 0.0, 1.0))

    def survival_function_at_times(self, times: Iterable[float]) -> pd.Series:
        if self.survival_function_ is None:
            raise ValueError("Must call fit before survival_function_at_times.")
        out = []
        idx = []
        for t in times:
            idx.append(t)
            out.append(self.predict(t))
        return pd.Series(out, index=pd.Index(idx, name="timeline"), name=self.label)
</file>

<file:name=lifelines/fitters/coxph_fitter.py>
from __future__ import annotations

from dataclasses import dataclass
from typing import Any

import numpy as np
import pandas as pd


def _is_number_dtype(s: pd.Series) -> bool:
    return pd.api.types.is_numeric_dtype(s.dtype)


def _safe_exp(x: np.ndarray) -> np.ndarray:
    return np.exp(np.clip(x, -50.0, 50.0))


def _forward_fill_step_at(times_src: np.ndarray, values_src: np.ndarray, times_new: np.ndarray) -> np.ndarray:
    # times_src must be sorted ascending
    pos = np.searchsorted(times_src, times_new, side="right") - 1
    pos = np.clip(pos, 0, len(times_src) - 1)
    return values_src[pos]


@dataclass
class CoxPHFitter:
    penalizer: float = 0.0
    l1_ratio: float = 0.0
    strata: Any = None

    def __post_init__(self) -> None:
        self.params_: pd.Series | None = None
        self.variance_matrix_: pd.DataFrame | None = None
        self.standard_errors_: pd.Series | None = None
        self.summary: pd.DataFrame | None = None

        self.baseline_cumulative_hazard_: pd.DataFrame | None = None
        self.baseline_survival_: pd.DataFrame | None = None

        self.log_likelihood_: float | None = None
        self._duration_col: str | None = None
        self._event_col: str | None = None
        self._covariate_columns: list[str] | None = None

    def fit(
        self,
        df: pd.DataFrame,
        duration_col: str,
        event_col: str,
        show_progress: bool = False,
        **kwargs: Any,
    ) -> "CoxPHFitter":
        if duration_col not in df.columns or event_col not in df.columns:
            raise ValueError("duration_col and event_col must be present in df.")

        self._duration_col = duration_col
        self._event_col = event_col

        work = df.copy()
        # keep only rows with non-missing in duration/event and covariates later
        if not _is_number_dtype(work[duration_col]):
            work[duration_col] = pd.to_numeric(work[duration_col], errors="coerce")
        if not _is_number_dtype(work[event_col]):
            work[event_col] = pd.to_numeric(work[event_col], errors="coerce")

        # choose numeric covariates excluding duration/event
        covariate_cols = [c for c in work.columns if c not in (duration_col, event_col) and _is_number_dtype(work[c])]
        if len(covariate_cols) == 0:
            raise ValueError("No numeric covariates found to fit CoxPH model.")
        self._covariate_columns = covariate_cols

        used_cols = [duration_col, event_col] + covariate_cols
        work = work[used_cols].dropna(axis=0, how="any").copy()
        if work.shape[0] == 0:
            raise ValueError("No rows remaining after dropping missing values.")

        T = work[duration_col].to_numpy(dtype=float)
        E = (work[event_col].to_numpy(dtype=float) > 0).astype(int)
        X = work[covariate_cols].to_numpy(dtype=float)

        n, p = X.shape
        if n <= p:
            # still possible to fit, but warn-like behavior not needed; keep going
            pass

        order = np.argsort(T, kind="mergesort")
        T = T[order]
        E = E[order]
        X = X[order]

        unique_event_times = np.unique(T[E == 1])
        unique_event_times.sort()

        beta = np.zeros(p, dtype=float)
        tol = 1e-7
        max_iter = int(kwargs.get("max_iter", 50))

        def partial_log_lik(beta_vec: np.ndarray) -> float:
            eta = X @ beta_vec
            w = _safe_exp(eta)
            ll = 0.0
            # Breslow ties
            for t in unique_event_times:
                event_mask = (T == t) & (E == 1)
                d = int(np.sum(event_mask))
                if d == 0:
                    continue
                risk_mask = T >= t
                denom = float(np.sum(w[risk_mask]))
                if denom <= 0.0:
                    continue
                ll += float(np.sum(eta[event_mask])) - d * np.log(denom)
            if self.penalizer and self.penalizer > 0:
                ll -= 0.5 * float(self.penalizer) * float(beta_vec @ beta_vec)
            return float(ll)

        ll = partial_log_lik(beta)

        for _ in range(max_iter):
            eta = X @ beta
            w = _safe_exp(eta)

            grad = np.zeros(p, dtype=float)
            hess = np.zeros((p, p), dtype=float)

            for t in unique_event_times:
                event_mask = (T == t) & (E == 1)
                d = int(np.sum(event_mask))
                if d == 0:
                    continue
                risk_mask = T >= t

                w_risk = w[risk_mask]
                X_risk = X[risk_mask, :]
                S0 = float(np.sum(w_risk))
                if S0 <= 0.0:
                    continue
                S1 = (w_risk[:, None] * X_risk).sum(axis=0)  # (p,)
                # S2 = sum(w_i * x_i x_i^T)
                # compute via matrix multiplication
                S2 = (X_risk.T * w_risk) @ X_risk  # (p,p)

                x_events_sum = X[event_mask, :].sum(axis=0)
                grad += x_events_sum - d * (S1 / S0)

                mean = S1 / S0
                var = (S2 / S0) - np.outer(mean, mean)
                hess -= d * var

            if self.penalizer and self.penalizer > 0:
                grad -= self.penalizer * beta
                hess -= self.penalizer * np.eye(p)

            # Solve for Newton step: beta_new = beta - H^{-1} grad
            # Here hess is (approx) second derivative of loglik, negative definite; we use step = solve(-hess, grad)
            H = -hess
            # jitter for numerical stability
            jitter = 0.0
            try:
                step = np.linalg.solve(H, grad)
            except np.linalg.LinAlgError:
                jitter = 1e-9 if self.penalizer == 0 else 0.0
                try:
                    step = np.linalg.solve(H + jitter * np.eye(p), grad)
                except np.linalg.LinAlgError:
                    step = np.linalg.pinv(H + 1e-6 * np.eye(p)) @ grad

            # Backtracking line search to ensure improvement
            step_norm = float(np.max(np.abs(step))) if step.size else 0.0
            if step_norm < tol:
                break

            alpha = 1.0
            improved = False
            for _ls in range(15):
                beta_try = beta + alpha * step
                ll_try = partial_log_lik(beta_try)
                if ll_try >= ll - 1e-12:
                    beta = beta_try
                    ll = ll_try
                    improved = True
                    break
                alpha *= 0.5
            if not improved:
                # accept small step to avoid stalling
                beta = beta + 0.1 * step
                ll = partial_log_lik(beta)

            if float(np.max(np.abs(alpha * step))) < tol:
                break

        self.log_likelihood_ = float(ll)
        self.params_ = pd.Series(beta, index=covariate_cols, name="coef")

        # Variance matrix approx: inverse of observed information (-hess) at optimum
        # recompute hessian at beta
        eta = X @ beta
        w = _safe_exp(eta)
        hess = np.zeros((p, p), dtype=float)
        for t in unique_event_times:
            event_mask = (T == t) & (E == 1)
            d = int(np.sum(event_mask))
            if d == 0:
                continue
            risk_mask = T >= t
            w_risk = w[risk_mask]
            X_risk = X[risk_mask, :]
            S0 = float(np.sum(w_risk))
            if S0 <= 0.0:
                continue
            S1 = (w_risk[:, None] * X_risk).sum(axis=0)
            S2 = (X_risk.T * w_risk) @ X_risk
            mean = S1 / S0
            var = (S2 / S0) - np.outer(mean, mean)
            hess -= d * var
        if self.penalizer and self.penalizer > 0:
            hess -= self.penalizer * np.eye(p)

        info = -hess
        try:
            cov = np.linalg.inv(info)
        except np.linalg.LinAlgError:
            cov = np.linalg.pinv(info + 1e-9 * np.eye(p))

        self.variance_matrix_ = pd.DataFrame(cov, index=covariate_cols, columns=covariate_cols)
        se = np.sqrt(np.clip(np.diag(cov), 0.0, np.inf))
        self.standard_errors_ = pd.Series(se, index=covariate_cols, name="se(coef)")
        self.summary = pd.DataFrame({"coef": self.params_.values, "se(coef)": self.standard_errors_.values}, index=covariate_cols)

        # Baseline cumulative hazard via Breslow
        # H0(t) increments: d(t) / sum_{i in R(t)} exp(eta_i)
        event_times = unique_event_times
        if event_times.size == 0:
            # no events: degenerate baseline
            base_times = np.array([0.0])
            base_H = np.array([0.0])
        else:
            base_times = np.concatenate([[0.0], event_times.astype(float)])
            base_H = [0.0]
            cum = 0.0
            for t in event_times:
                d = int(np.sum((T == t) & (E == 1)))
                risk_mask = T >= t
                denom = float(np.sum(w[risk_mask]))
                inc = (d / denom) if denom > 0 else 0.0
                cum += inc
                base_H.append(float(cum))
            base_H = np.asarray(base_H, dtype=float)

        H_df = pd.DataFrame({"baseline cumulative hazard": base_H}, index=pd.Index(base_times, name="timeline"))
        S0 = np.exp(-H_df["baseline cumulative hazard"].to_numpy(dtype=float))
        S_df = pd.DataFrame({"baseline survival": np.clip(S0, 0.0, 1.0)}, index=H_df.index.copy())

        self.baseline_cumulative_hazard_ = H_df
        self.baseline_survival_ = S_df
        return self

    def predict_survival_function(self, row: pd.DataFrame, times: Any = None) -> pd.DataFrame:
        if self.params_ is None or self.baseline_cumulative_hazard_ is None:
            raise ValueError("Must call fit before predict_survival_function.")
        if not isinstance(row, pd.DataFrame):
            raise ValueError("row must be a pandas DataFrame.")
        if row.shape[0] != 1:
            raise ValueError("row must be a single-row DataFrame.")

        cov_cols = list(self.params_.index)
        missing = [c for c in cov_cols if c not in row.columns]
        if missing:
            raise ValueError(f"Missing covariate columns in row: {missing}")

        x = row[cov_cols].iloc[0].to_numpy(dtype=float)
        eta = float(x @ self.params_.to_numpy(dtype=float))
        hr = float(_safe_exp(np.array([eta], dtype=float))[0])  # exp(eta)

        base_H = self.baseline_cumulative_hazard_["baseline cumulative hazard"]
        base_times = base_H.index.to_numpy(dtype=float)
        base_vals = base_H.to_numpy(dtype=float)

        if times is None:
            out_times = base_times
            H_at = base_vals
        else:
            out_times = np.asarray(times, dtype=float).reshape(-1)
            out_times = np.sort(out_times)
            H_at = _forward_fill_step_at(base_times, base_vals, out_times)

        S = np.exp(-H_at * hr)
        S = np.clip(S, 0.0, 1.0)

        colname = str(row.index[0]) if row.index is not None and len(row.index) == 1 else "0"
        return pd.DataFrame({colname: S}, index=pd.Index(out_times, name="timeline"))
</file>

<file:name=lifelines/datasets/__init__.py>
from __future__ import annotations

from .waltons import load_waltons

__all__ = ["load_waltons"]
</file>

<file:name=lifelines/datasets/waltons.py>
from __future__ import annotations

import pandas as pd


def load_waltons() -> pd.DataFrame:
    """
    A small built-in Waltons-like dataset.

    Returns a DataFrame with columns:
      - T: duration
      - E: event indicator (1=event observed, 0=censored)
      - group: categorical group label
    """
    data = [
        {"T": 6, "E": 1, "group": "control"},
        {"T": 7, "E": 1, "group": "control"},
        {"T": 10, "E": 0, "group": "control"},
        {"T": 12, "E": 1, "group": "control"},
        {"T": 15, "E": 0, "group": "control"},
        {"T": 18, "E": 1, "group": "control"},
        {"T": 3, "E": 1, "group": "miR"},
        {"T": 4, "E": 1, "group": "miR"},
        {"T": 5, "E": 1, "group": "miR"},
        {"T": 8, "E": 0, "group": "miR"},
        {"T": 9, "E": 1, "group": "miR"},
        {"T": 11, "E": 0, "group": "miR"},
    ]
    df = pd.DataFrame(data)
    df["T"] = pd.to_numeric(df["T"], errors="raise")
    df["E"] = pd.to_numeric(df["E"], errors="raise").astype(int)
    df["group"] = df["group"].astype("object")
    return df
</file>

<file:name=lifelines/utils/__init__.py>
__all__ = []
</file>

<file:name=_agent_tests/test_agent_basic.py>
import numpy as np
import pandas as pd

import lifelines


def test_imports_and_datasets():
    assert hasattr(lifelines, "KaplanMeierFitter")
    assert hasattr(lifelines, "CoxPHFitter")
    assert hasattr(lifelines, "datasets")
    df = lifelines.datasets.load_waltons()
    assert isinstance(df, pd.DataFrame)
    assert set(["T", "E", "group"]).issubset(df.columns)
    assert df.shape[0] > 0
    assert pd.api.types.is_numeric_dtype(df["T"])
    assert pd.api.types.is_integer_dtype(df["E"])
    assert df["group"].dtype == object


def test_kaplan_meier_fit_predict_handcheck():
    kmf = lifelines.KaplanMeierFitter()
    durations = [1, 2, 2, 3]
    events = [1, 1, 0, 1]
    kmf.fit(durations, events)

    sf = kmf.survival_function_
    assert isinstance(sf, pd.DataFrame)
    assert sf.index.is_monotonic_increasing
    vals = sf.iloc[:, 0].to_numpy()
    assert np.all((vals >= 0) & (vals <= 1))

    # Hand-checked:
    # at t=1: S= (1-1/4)=0.75
    # at t=2: risk just before 2 is 3, d=1 => 0.75*(1-1/3)=0.5
    # at t=3: risk just before 3 is 1, d=1 => 0.0
    assert abs(kmf.predict(0) - 1.0) < 1e-12
    assert abs(kmf.predict(1) - 0.75) < 1e-12
    assert abs(kmf.predict(1.5) - 0.75) < 1e-12
    assert abs(kmf.predict(2) - 0.5) < 1e-12
    assert abs(kmf.predict(10) - 0.0) < 1e-12


def test_coxph_fit_and_predict_survival_function():
    # Construct a small dataset where higher x tends to have earlier events.
    df = pd.DataFrame(
        {
            "T": [1, 2, 3, 4, 5, 6],
            "E": [1, 1, 1, 1, 0, 0],
            "x": [2.0, 2.0, 1.0, 1.0, 0.0, 0.0],
            "non_numeric": ["a", "b", "c", "d", "e", "f"],
        }
    )
    cph = lifelines.CoxPHFitter(penalizer=0.1)
    cph.fit(df, duration_col="T", event_col="E")
    assert isinstance(cph.summary, pd.DataFrame)
    assert "coef" in cph.summary.columns
    assert "se(coef)" in cph.summary.columns
    assert "x" in cph.summary.index

    # Positive coef means higher x -> higher hazard -> shorter survival, expected here.
    assert np.isfinite(cph.summary.loc["x", "coef"])

    row = pd.DataFrame({"x": [2.0]})
    sf = cph.predict_survival_function(row)
    assert isinstance(sf, pd.DataFrame)
    svals = sf.iloc[:, 0].to_numpy(dtype=float)
    assert np.all((svals >= -1e-12) & (svals <= 1 + 1e-12))
    # starts at 1 at time 0
    assert abs(float(sf.iloc[0, 0]) - 1.0) < 1e-8
    # should be non-increasing
    assert np.all(np.diff(svals) <= 1e-10)


def test_coxph_ignores_non_numeric_and_drops_missing():
    df = pd.DataFrame(
        {
            "T": [1, 2, 3, 4],
            "E": [1, 1, 0, 1],
            "x": [0.0, 1.0, np.nan, 2.0],
            "group": ["a", "a", "b", "b"],  # should be ignored
        }
    )
    cph = lifelines.CoxPHFitter(penalizer=0.1)
    cph.fit(df, duration_col="T", event_col="E")
    assert "x" in cph.params_.index
    assert "group" not in cph.params_.index
</file>
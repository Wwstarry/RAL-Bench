<plan>
1) Repository layout and import graph
   - Package layout
     - petl/__init__.py
       - Expose public API functions: fromcsv, tocsv, fromdicts, convert, select, selectge, selectgt, sort, addfield, join
       - Import and re-export from internal modules to match expected API surface.
     - petl/io/csv.py
       - Implement CSV source (lazy) and sink (writer).
     - petl/transform/conversions.py
       - Implement convert().
     - petl/transform/selects.py
       - Implement select(), selectge(), selectgt().
     - petl/transform/sort.py
       - Implement sort().
     - petl/transform/joins.py
       - Implement join().
   - Import graph (avoid cycles)
     - __init__.py imports functions from leaf modules.
     - Transform modules should not import from __init__.py; they should operate on “table protocol” (any iterable yielding rows).
     - Optionally create a small internal “core” inside __init__.py only (e.g., helper wrapper class), but keep it dependency-free to avoid cycles.
   - Core concept: “Table wrappers”
     - Each ETL operator returns an object implementing __iter__ that yields header then data rows.
     - Wrappers store only references to upstream table and params; do not cache rows.

2) Public APIs to implement (modules/classes/functions)
   - petl/__init__.py
     - fromcsv(path, encoding='utf-8', dialect='excel', **csv_kwargs)
       - delegates to petl.io.csv.fromcsv
     - tocsv(table, path, encoding='utf-8', dialect='excel', **csv_kwargs)
       - delegates to petl.io.csv.tocsv
     - fromdicts(records, header=None)
       - implement here or in a small internal helper module-less section
     - convert(table, field, func, failonerror=False, default=None)
       - delegates to petl.transform.conversions.convert
     - select(table, predicate)
       - delegates to petl.transform.selects.select
     - selectge(table, field, threshold)
       - delegates to petl.transform.selects.selectge
     - selectgt(table, field, threshold)
       - delegates to petl.transform.selects.selectgt
     - sort(table, field, reverse=False)
       - delegates to petl.transform.sort.sort
     - addfield(table, fieldname, func, index=None)
       - implement in __init__.py or a tiny internal helper section (tests require it but no module mandated)
     - join(left, right, key='id')
       - delegates to petl.transform.joins.join
     - Small internal utilities (not exported)
       - _as_field_index(header, field): accepts int or str; returns int index; raises KeyError/ValueError appropriately.
       - _iter_header_and_rows(table): yields (header, rows_iter) or just standardizes iteration.
   - petl/io/csv.py
     - fromcsv(path, encoding='utf-8', dialect='excel', **csv_kwargs) -> Table
       - Return a lazy table wrapper reading the file upon iteration.
     - tocsv(table, path, encoding='utf-8', dialect='excel', write_header=True, **csv_kwargs) -> None
       - Iterate table and write rows.
     - Internal class: CsvView (table wrapper)
       - __iter__ opens file, creates csv.reader, yields rows.
       - Ensure file closed via context manager.
   - petl/transform/conversions.py
     - convert(table, field, func, failonerror=False, default=None) -> Table
       - Internal class: ConvertView
       - On iteration:
         - Read header from upstream.
         - Determine field index.
         - Yield header unchanged.
         - For each data row, apply func to value at field index.
         - Error handling:
           - If failonerror True: re-raise exceptions from func.
           - Else: on exception, use default if provided else keep original value (choose one consistent behavior; prefer default if not None).
   - petl/transform/selects.py
     - select(table, predicate) -> Table
       - predicate may accept:
         - (row) OR (rowdict) OR (value(s)) depending on tests; implement simplest robust approach:
           - Call predicate(row) first; if TypeError due to wrong arity, call predicate(dict(zip(header,row))).
       - Internal class: SelectView
     - selectge(table, field, threshold) -> Table
       - Filter rows where row[field] >= threshold (None-safe: if value is None -> False).
     - selectgt(table, field, threshold) -> Table
       - Filter rows where row[field] > threshold (None-safe).
   - petl/transform/sort.py
     - sort(table, field, reverse=False) -> Table
       - Internal class: SortView
       - Sort is a blocking operation requiring materialization of all data rows for sorting.
       - Implementation:
         - Iterate upstream once, capture header and list(rows).
         - Determine key index.
         - Sort list with key=lambda r: r[i] (consider None ordering; see behaviors below).
         - Yield header then sorted rows.
   - petl/transform/joins.py
     - join(left, right, key='id') -> Table
       - Internal class: JoinView
       - For performance:
         - Build an index from right table keyed by join key value (hash map: key -> list of rows) by iterating right once.
         - Then iterate left and produce combined rows for matches.
       - Header behavior:
         - Output header includes:
           - all fields from left header
           - plus fields from right header excluding the join key if it already exists in left (avoid duplicate key column)
         - Determine indices and mapping.
       - Row behavior:
         - For each left row, lookup right rows with same key value.
         - If multiple matches on right, output multiple combined rows (cartesian for that left row).
         - If no match, output nothing (inner join) unless tests expect otherwise; implement inner join only.

3) Key behaviors & edge cases
   - Table protocol and laziness
     - Every operator except sort/join-indexing must stream:
       - convert/select/selectge/selectgt/addfield should yield rows as upstream yields them.
     - Avoid storing upstream rows in wrappers (no caching), except:
       - sort must store all rows (inherently).
       - join must store an index for the right side; left should be streamed.
     - Table can be:
       - Any iterable yielding rows (list-of-tuples, generator, wrapper objects).
       - Must treat first yielded row as header.
   - Header handling and field addressing
     - Field selectors (field, key) can be:
       - int index
       - str column name
     - If str not found: raise KeyError.
     - If int out of range: raise IndexError.
   - Row types
     - Rows may be tuples or lists; preserve as tuples for output for consistency (but accept any sequence).
     - When modifying rows (convert/addfield/join), construct tuples.
   - CSV I/O specifics
     - fromcsv:
       - Open file with newline='' to work properly with csv module.
       - Use csv.reader; yield each row as a tuple of strings (csv module returns list[str]).
       - Do not auto-convert types.
     - tocsv:
       - Accept table (iterable).
       - Write rows via csv.writer; write exactly what is yielded (including header).
       - Ensure newline='' and encoding.
   - select predicate calling
     - Prefer calling predicate(row) where row is a tuple.
     - Fallback: predicate(rowdict) where rowdict is dict(header->value).
     - This covers typical usage patterns without adding heavy overhead in the common case.
   - selectge/selectgt comparisons
     - If value is None or empty string:
       - Treat as not passing predicate (return False) to avoid TypeError.
     - If threshold is None: treat as always False (or raise); choose always False for safety.
   - sort ordering
     - Default key is raw field value.
     - Mixed types can raise TypeError in Python3; tests likely keep comparable types.
     - None handling:
       - Provide a key wrapper: (val is None, val) so that None sorts last consistently.
   - addfield semantics
     - Add a new field/column computed per row.
     - Header:
       - Insert at end by default; if index is provided, insert at that position.
     - func invocation:
       - Call func(row) first; if TypeError, call func(dict(zip(header,row))).
     - Row output:
       - Insert computed value at correct position, preserve other fields.
   - join semantics
     - Key parameter can be str or int; if str, locate in both headers.
     - Duplicate fields:
       - Exclude right join key field if same name as left join key field (common case).
       - If headers share other field names, keep both (tests likely don’t cover collisions); simplest is append right fields as-is (excluding key) even if name duplicates, yielding duplicate header names. That is acceptable for minimal compatibility unless tests assert uniqueness; if concerned, implement disambiguation by suffixing “_right” for collisions, but only if needed by tests (avoid diverging from expected).
     - Streaming:
       - Build right index first on iteration start (cost: one pass over right).
       - Then stream left and yield joined rows.
     - Memory:
       - Right index stores right rows; acceptable per tests as join inherently needs lookup. Avoid indexing both sides.
   - Errors and robustness
     - Ensure wrappers are re-iterable:
       - For fromcsv: each __iter__ re-opens file and yields rows from start.
       - For transforms: __iter__ calls iter(upstream) fresh; depends on upstream being re-iterable. This matches typical PETL usage and tests (they usually re-iterate tables).
       - For fromdicts: wrap records in a re-iterable container or store the records reference and re-iterate it (if input is iterator, it won’t be re-iterable; but tests likely pass list. Option: materialize dict records into list once in fromdicts to guarantee re-iterable; keep minimal memory by only doing this in fromdicts, which is in-memory anyway).
     - Performance considerations
       - Avoid per-row dict(zip(...)) unless needed (predicate/func arity fallback only).
       - Field index lookup once per iteration, not per row.

4) Minimal internal test plan (what to test and why)
   - Smoke: table protocol
     - Create a simple table as list of tuples; run through convert/select/sort/addfield and ensure first row is header and downstream rows correct.
   - CSV roundtrip
     - Write small table to temp CSV via tocsv; read back via fromcsv; compare rows.
     - Ensure laziness: create fromcsv table, do not iterate, ensure file not opened until iteration (hard to assert directly; can use a custom open spy in unit tests or rely on behavior).
   - fromdicts
     - With explicit header:
       - records missing some keys -> output None for missing.
       - extra keys ignored unless included in header.
     - Without header:
       - infer header from union of keys in first record only (simple) or from all records (more expensive). Prefer: if header is None, infer from first record’s keys preserving insertion order; tests likely cover predictable header.
   - convert
     - Convert by field name and by index.
     - Conversion error path: func raises, verify default/keep-original behavior when failonerror False, and raising when True.
   - select/selectge/selectgt
     - select with predicate(row) and predicate(dict) forms.
     - selectge/selectgt compare numeric strings after convert to int pipeline: fromcsv -> convert -> selectgt.
   - sort
     - Sort by field name and by index; ensure stable ordering and header preserved.
   - addfield
     - Add computed field at end and inserted at index.
   - join
     - Basic inner join on ‘id’ with one-to-one match.
     - One-to-many match on right yields multiple rows.
     - Ensure header composition excludes duplicate key from right.
   - Resource/performance sanity (micro)
     - Create a large-ish generator table and apply streaming transforms (convert/select/addfield) and iterate first N rows; confirm it doesn’t pre-consume all input (can be tested with a counting iterator).

5) Risks (dependencies, tricky behaviors) and mitigations
   - Risk: re-iterability of tables
     - Some wrappers depend on upstream being re-iterable; tests may iterate same pipeline multiple times.
     - Mitigation:
       - Make fromcsv and fromdicts re-iterable by design.
       - Document that if user passes a one-shot iterator into transforms it’s one-shot; acceptable for tests.
   - Risk: predicate/func calling conventions
     - Tests might use lambda rec: rec['field'] style (dict) or lambda row: row[0] style (tuple).
     - Mitigation: implement dual-call strategy with TypeError fallback; keep overhead low.
   - Risk: header inference from fromdicts
     - If tests expect stable header ordering, naive union-of-all-keys could reorder.
     - Mitigation: if header not provided, use keys from first record in iteration order (Python 3.7+ preserves insertion order). This is deterministic.
   - Risk: join field collisions
     - Duplicate column names between left and right beyond key could break tests if they expect disambiguation.
     - Mitigation: start with simplest (allow duplicates) but implement optional disambiguation only for exact collisions if tests fail:
       - If right field name already in left header, rename to f"{name}_right".
   - Risk: performance_test constraints
     - Excessive per-row overhead (building dicts) could slow down.
     - Mitigation:
       - Only build rowdict on fallback path.
       - Cache header->index mapping once per iterator.
       - Use local variables in loops (micro-optimizations) only if needed.
   - Risk: CSV newline/encoding portability
     - Incorrect newline handling can create blank lines on Windows.
     - Mitigation: always open with newline='' for csv reader/writer, pass encoding explicitly.
   - Risk: None/mixed-type comparisons in sort/selectge/selectgt
     - Could raise TypeError in Python3.
     - Mitigation:
       - For sort: sort key returns (is_none, value) to avoid comparing None with others.
       - For selectge/selectgt: catch TypeError and treat row as failing predicate.
</plan>
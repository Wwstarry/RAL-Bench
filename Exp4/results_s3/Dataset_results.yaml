project_name: Dataset
task_file: D:\桌面\RealAppCodeBench_generic_eval\tasks\Dataset\dataset.yaml
generated_repo: D:\桌面\RealAppCodeBench_generic_eval\generation_m4\Dataset
timestamp: '2026-01-14 18:57:06'
functional_score: 0.6364
non_functional_score: 0.6217
non_functional_subscores:
  maintainability: 0.6159
  security: 1.0
  robustness: 1.0
  performance: 0.0
  resource: 0.0
non_functional_weights:
  maintainability: 0.36
  security: 0.24
  robustness: 0.16
  performance: 0.12
  resource: 0.12
results:
  functional:
    returncode: 1
    stdout: "F....F..F.F                                                         \
      \     [100%]\n================================== FAILURES ===================================\n\
      ______________________ test_insert_and_query_basic_rows _______________________\n\
      \n    def test_insert_and_query_basic_rows() -> None:\n        db = create_in_memory_db()\n\
      \        table = db[\"users\"]\n    \n        table.insert({\"name\": \"Alice\"\
      , \"age\": 30, \"country\": \"DE\"})\n        table.insert({\"name\": \"Bob\"\
      , \"age\": 41, \"country\": \"US\", \"active\": True})\n        table.insert({\"\
      name\": \"Charlie\", \"age\": 41, \"country\": \"US\", \"active\": False})\n\
      \    \n        assert \"id\" in _table_columns(table)\n        assert \"name\"\
      \ in _table_columns(table)\n        assert \"country\" in _table_columns(table)\n\
      \        assert len(table) == 3\n    \n        alice = table.find_one(name=\"\
      Alice\")\n        assert alice is not None\n        assert alice[\"country\"\
      ] == \"DE\"\n    \n>       older = list(table.find(age={\">=\": 40}))\n\ntests\\\
      Dataset\\functional_test.py:155: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <dataset.table.Table object\
      \ at 0x000001E82BCF4B50>\nfilters = {'age': {'>=': 40}}, where = ' WHERE \"\
      age\" = :p0'\nparams = {'p0': {'>=': 40}}, valid = True\nsql = 'SELECT * FROM\
      \ \"users\" WHERE \"age\" = :p0'\n\n    def find(self, **filters: Any) -> Iterator[Mapping[str,\
      \ Any]]:\n        if not self._table_exists():\n            return iter(())\n\
      \        where, params, valid = self._where_clause(filters)\n        if not\
      \ valid:\n            return iter(())\n        sql = f\"SELECT * FROM {self._qname}{where}\"\
      \n        cur = self.db.connection.cursor()\n        try:\n>           cur.execute(sql,\
      \ params)\nE           sqlite3.InterfaceError: Error binding parameter :p0 -\
      \ probably unsupported type.\n\ngeneration_m4\\Dataset\\dataset\\table.py:341:\
      \ InterfaceError\n_______________________ test_find_order_by_limit_offset _______________________\n\
      \n    def test_find_order_by_limit_offset() -> None:\n        db = create_in_memory_db()\n\
      \        table = db[\"nums\"]\n        for i in range(10):\n            table.insert({\"\
      n\": i})\n    \n        rows = list(table.find(order_by=\"n\", _limit=3, _offset=4))\n\
      >       assert [r[\"n\"] for r in rows] == [4, 5, 6]\nE       assert [] == [4,\
      \ 5, 6]\nE         \nE         Right contains 3 more items, first extra item:\
      \ 4\nE         Use -v to get more diff\n\ntests\\Dataset\\functional_test.py:249:\
      \ AssertionError\n___________________ test_drop_table_removes_from_db_tables\
      \ ____________________\n\n    def test_drop_table_removes_from_db_tables() ->\
      \ None:\n        db = create_in_memory_db()\n        table = db[\"to_drop\"\
      ]\n        table.insert({\"x\": 1})\n    \n>       assert \"to_drop\" in _db_tables(db)\n\
      E       AssertionError: assert 'to_drop' in []\nE        +  where [] = _db_tables(<dataset.database.Database\
      \ object at 0x000001E82BCAD2E0>)\n\ntests\\Dataset\\functional_test.py:301:\
      \ AssertionError\n_____________________ test_distinct_returns_unique_values\
      \ _____________________\n\n    def test_distinct_returns_unique_values() ->\
      \ None:\n        db = create_in_memory_db()\n        table = db[\"colors\"]\n\
      \        table.insert_many([{\"c\": \"red\"}, {\"c\": \"red\"}, {\"c\": \"blue\"\
      }])\n    \n        distinct = list(table.distinct(\"c\"))\n>       values =\
      \ {r[\"c\"] for r in distinct}\n\ntests\\Dataset\\functional_test.py:333: \n\
      _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _\n\n.0 = <list_iterator object at 0x000001E82BD3E1C0>\n\n>   values = {r[\"\
      c\"] for r in distinct}\nE   TypeError: string indices must be integers\n\n\
      tests\\Dataset\\functional_test.py:333: TypeError\n===========================\
      \ short test summary info ===========================\nFAILED tests/Dataset/functional_test.py::test_insert_and_query_basic_rows\
      \ - s...\nFAILED tests/Dataset/functional_test.py::test_find_order_by_limit_offset\
      \ - as...\nFAILED tests/Dataset/functional_test.py::test_drop_table_removes_from_db_tables\n\
      FAILED tests/Dataset/functional_test.py::test_distinct_returns_unique_values\n\
      4 failed, 7 passed in 3.64s\n"
    elapsed_time_s: 4.987128
    avg_memory_mb: 34.39
    avg_cpu_percent: 95.2
    passed: 7
    failed: 4
    skipped: 0
    total: 11
    score_inputs_passed: 7
    score_inputs_failed: 4
    score_inputs_total: 11
    score_inputs_returncode: 1
    score_inputs_failed_suite: true
    score_inputs_baseline_keys:
    - functional_suite_time_s
    - functional_tests_total
  performance:
    returncode: 2
    stdout: "\n=================================== ERRORS ====================================\n\
      _____________ ERROR collecting tests/Dataset/performance_test.py ______________\n\
      tests\\Dataset\\performance_test.py:18: in <module>\n    raise RuntimeError(f\"\
      Unsupported DATASET_TARGET value: {TARGET_ENV}\")\nE   RuntimeError: Unsupported\
      \ DATASET_TARGET value: generated\n=========================== short test summary\
      \ info ===========================\nERROR tests/Dataset/performance_test.py\
      \ - RuntimeError: Unsupported DATASET_T...\n!!!!!!!!!!!!!!!!!!! Interrupted:\
      \ 1 error during collection !!!!!!!!!!!!!!!!!!!!\n1 error in 0.50s\n"
    elapsed_time_s: 1.850048
    avg_memory_mb: 35.67
    avg_cpu_percent: 95.5
    passed: 0
    failed: 0
    skipped: 0
    total: 1
    score_inputs_passed: 0
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 2
    score_inputs_failed_suite: true
    score_inputs_baseline_keys:
    - performance_suite_time_s
    - performance_tests_total
    score_inputs_baseline_time_s: 2.781374
    score_inputs_actual_time_s: 1.850048
  resource:
    returncode: 2
    stdout: "\n=================================== ERRORS ====================================\n\
      _______________ ERROR collecting tests/Dataset/resource_test.py _______________\n\
      tests\\Dataset\\resource_test.py:18: in <module>\n    raise RuntimeError(f\"\
      Unsupported DATASET_TARGET value: {TARGET_ENV}\")\nE   RuntimeError: Unsupported\
      \ DATASET_TARGET value: generated\n=========================== short test summary\
      \ info ===========================\nERROR tests/Dataset/resource_test.py - RuntimeError:\
      \ Unsupported DATASET_TARG...\n!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during\
      \ collection !!!!!!!!!!!!!!!!!!!!\n1 error in 0.53s\n"
    elapsed_time_s: 1.945802
    avg_memory_mb: 35.59
    avg_cpu_percent: 99.2
    passed: 0
    failed: 0
    skipped: 0
    total: 1
    score_inputs_passed: 0
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 2
    score_inputs_failed_suite: true
    score_inputs_baseline_keys:
    - avg_cpu_percent
    - avg_memory_mb
    - resource_suite_time_s
    - resource_tests_total
    score_inputs_baseline_mem_mb: 55.98
    score_inputs_baseline_cpu_pct: 100.6
    score_inputs_actual_mem_mb: 35.59
    score_inputs_actual_cpu_pct: 99.2
  robustness:
    returncode: 0
    stdout: "......                                                              \
      \     [100%]\n============================== warnings summary ===============================\n\
      tests\\Dataset\\robustness_test.py:92\n  D:\\桌面\\RealAppCodeBench_generic_eval\\\
      tests\\Dataset\\robustness_test.py:92: PytestUnknownMarkWarning: Unknown pytest.mark.timeout\
      \ - is this a typo?  You can register custom marks to avoid this warning - for\
      \ details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.timeout(10)\n\
      \ntests\\Dataset\\robustness_test.py:102\n  D:\\桌面\\RealAppCodeBench_generic_eval\\\
      tests\\Dataset\\robustness_test.py:102: PytestUnknownMarkWarning: Unknown pytest.mark.timeout\
      \ - is this a typo?  You can register custom marks to avoid this warning - for\
      \ details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.timeout(10)\n\
      \ntests\\Dataset\\robustness_test.py:120\n  D:\\桌面\\RealAppCodeBench_generic_eval\\\
      tests\\Dataset\\robustness_test.py:120: PytestUnknownMarkWarning: Unknown pytest.mark.timeout\
      \ - is this a typo?  You can register custom marks to avoid this warning - for\
      \ details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.timeout(10)\n\
      \ntests\\Dataset\\robustness_test.py:135\n  D:\\桌面\\RealAppCodeBench_generic_eval\\\
      tests\\Dataset\\robustness_test.py:135: PytestUnknownMarkWarning: Unknown pytest.mark.timeout\
      \ - is this a typo?  You can register custom marks to avoid this warning - for\
      \ details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.timeout(10)\n\
      \ntests\\Dataset\\robustness_test.py:149\n  D:\\桌面\\RealAppCodeBench_generic_eval\\\
      tests\\Dataset\\robustness_test.py:149: PytestUnknownMarkWarning: Unknown pytest.mark.timeout\
      \ - is this a typo?  You can register custom marks to avoid this warning - for\
      \ details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.timeout(10)\n\
      \ntests\\Dataset\\robustness_test.py:163\n  D:\\桌面\\RealAppCodeBench_generic_eval\\\
      tests\\Dataset\\robustness_test.py:163: PytestUnknownMarkWarning: Unknown pytest.mark.timeout\
      \ - is this a typo?  You can register custom marks to avoid this warning - for\
      \ details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.timeout(10)\n\
      \n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n\
      6 passed, 6 warnings in 0.18s\n"
    elapsed_time_s: 1.582384
    avg_memory_mb: 31.96
    avg_cpu_percent: 97.9
    passed: 6
    failed: 0
    skipped: 0
    total: 6
    score_inputs_passed: 6
    score_inputs_failed: 0
    score_inputs_total: 6
    score_inputs_returncode: 0
    score_inputs_failed_suite: false
    score_inputs_baseline_keys:
    - robustness_suite_time_s
    - robustness_tests_total
  security:
    returncode: 0
    stdout: 'SECURITY_METRICS high_risk_count=0.0 files_scanned=3.0 total_loc=517.0

      .

      1 passed in 0.13s

      '
    elapsed_time_s: 1.499675
    avg_memory_mb: 31.61
    avg_cpu_percent: 95.5
    passed: 1
    failed: 0
    skipped: 0
    total: 1
    metrics:
      high_risk_count: 0.0
      files_scanned: 3.0
      total_loc: 517.0
    score_inputs_passed: 1
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 0
    score_inputs_failed_suite: false
    score_inputs_baseline_keys:
    - metrics
    - security_suite_time_s
    - security_tests_total
    score_inputs_baseline_high_risk_count: 0.0
    score_inputs_generated_high_risk_count: 0.0
  maintainability:
    returncode: 0
    stdout: 'MAINT_METRICS mi_min=12.3776 files_scanned=3.0 total_loc=517.0 max_cc=16.0

      .

      1 passed in 0.20s

      '
    elapsed_time_s: 1.543297
    avg_memory_mb: 32.1
    avg_cpu_percent: 102.1
    passed: 1
    failed: 0
    skipped: 0
    total: 1
    metrics:
      mi_min: 12.3776
      files_scanned: 3.0
      total_loc: 517.0
      max_cc: 16.0
    score_inputs_passed: 1
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 0
    score_inputs_failed_suite: false
    score_inputs_baseline_keys:
    - maintainability_suite_time_s
    - maintainability_tests_total
    - metrics
    score_inputs_baseline_mi_min: 7.7184
    score_inputs_generated_mi_min: 12.3776
    score_inputs_ratio_g_over_b: 1.6036484245439468
baseline_metrics:
  performance:
    performance_suite_time_s: 2.781374
    performance_tests_total: 2
  resource:
    resource_suite_time_s: 2.854474
    resource_tests_total: 2
    avg_memory_mb: 55.98
    avg_cpu_percent: 100.6
  functional:
    functional_suite_time_s: 4.552087
    functional_tests_total: 11
  robustness:
    robustness_suite_time_s: 2.083828
    robustness_tests_total: 6
  security:
    security_suite_time_s: 1.286846
    security_tests_total: 1
    metrics:
      high_risk_count: 0.0
      files_scanned: 6.0
      total_loc: 1131.0
  maintainability:
    maintainability_suite_time_s: 1.311875
    maintainability_tests_total: 1
    metrics:
      mi_min: 7.7184
      files_scanned: 6.0
      total_loc: 1131.0
      max_cc: 16.0
pytest_logs_dir: D:\桌面\RealAppCodeBench_generic_eval\results\Dataset\pytest_logs

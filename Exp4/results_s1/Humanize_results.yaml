project_name: Humanize
task_file: D:\桌面\RealAppCodeBench_generic_eval\tasks\Humanize\humanize.yaml
generated_repo: D:\桌面\RealAppCodeBench_generic_eval\generation_m1\Humanize
timestamp: '2026-01-14 22:03:42'
functional_score: 0.5333
non_functional_score: 0.703
non_functional_subscores:
  maintainability: 0.8418
  security: 1.0
  robustness: 1.0
  performance: 0.0
  resource: 0.0
non_functional_weights:
  maintainability: 0.36
  security: 0.24
  robustness: 0.16
  performance: 0.12
  resource: 0.12
results:
  functional:
    returncode: 1
    stdout: "..F...F...sssss                                                     \
      \     [100%]\n================================== FAILURES ===================================\n\
      ______________________________ test_naturalsize _______________________________\n\
      \n    def test_naturalsize() -> None:\n>       assert humanize.naturalsize(1024)\
      \ == \"1.0 kB\"\nE       AssertionError: assert '1 kB' == '1.0 kB'\nE      \
      \   \nE         - 1.0 kB\nE         ?  --\nE         + 1 kB\n\ntests\\Humanize\\\
      functional_test.py:107: AssertionError\n_____________________ test_intcomma_float_keeps_decimals\
      \ ______________________\n\n    def test_intcomma_float_keeps_decimals() ->\
      \ None:\n        s = humanize.intcomma(1234.56)\n        assert isinstance(s,\
      \ str)\n>       assert s == \"1,234.56\"\nE       AssertionError: assert '1,234.560000'\
      \ == '1,234.56'\nE         \nE         - 1,234.56\nE         + 1,234.560000\n\
      E         ?         ++++\n\ntests\\Humanize\\functional_test.py:140: AssertionError\n\
      =========================== short test summary info ===========================\n\
      FAILED tests/Humanize/functional_test.py::test_naturalsize - AssertionError:\
      \ ...\nFAILED tests/Humanize/functional_test.py::test_intcomma_float_keeps_decimals\n\
      2 failed, 8 passed, 5 skipped in 0.46s\n"
    elapsed_time_s: 1.926993
    avg_memory_mb: 31.66
    avg_cpu_percent: 99.2
    passed: 8
    failed: 2
    skipped: 5
    total: 15
    score_inputs_passed: 8
    score_inputs_failed: 2
    score_inputs_total: 15
    score_inputs_returncode: 1
    score_inputs_failed_suite: true
    score_inputs_baseline_keys:
    - functional_suite_time_s
    - functional_tests_total
  performance:
    returncode: 2
    stdout: "\n=================================== ERRORS ====================================\n\
      _____________ ERROR collecting tests/Humanize/performance_test.py _____________\n\
      tests\\Humanize\\performance_test.py:20: in <module>\n    raise RuntimeError(f\"\
      Target repository does not exist: {REPO_ROOT}\")\nE   RuntimeError: Target repository\
      \ does not exist: D:\\桌面\\RealAppCodeBench_generic_eval\\generation\\Humanize\n\
      =========================== short test summary info ===========================\n\
      ERROR tests/Humanize/performance_test.py - RuntimeError: Target repository do...\n\
      !!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!\n\
      1 error in 0.57s\n"
    elapsed_time_s: 2.008344
    avg_memory_mb: 35.31
    avg_cpu_percent: 98.3
    passed: 0
    failed: 0
    skipped: 0
    total: 1
    score_inputs_passed: 0
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 2
    score_inputs_failed_suite: true
    score_inputs_baseline_keys:
    - performance_suite_time_s
    - performance_tests_total
    score_inputs_baseline_time_s: 1.604119
    score_inputs_actual_time_s: 2.008344
  resource:
    returncode: 2
    stdout: "\n=================================== ERRORS ====================================\n\
      ______________ ERROR collecting tests/Humanize/resource_test.py _______________\n\
      tests\\Humanize\\resource_test.py:20: in <module>\n    raise RuntimeError(f\"\
      Target repository does not exist: {REPO_ROOT}\")\nE   RuntimeError: Target repository\
      \ does not exist: D:\\桌面\\RealAppCodeBench_generic_eval\\generation\\Humanize\n\
      =========================== short test summary info ===========================\n\
      ERROR tests/Humanize/resource_test.py - RuntimeError: Target repository does\
      \ ...\n!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!\n\
      1 error in 0.61s\n"
    elapsed_time_s: 2.102341
    avg_memory_mb: 34.8
    avg_cpu_percent: 96.1
    passed: 0
    failed: 0
    skipped: 0
    total: 1
    score_inputs_passed: 0
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 2
    score_inputs_failed_suite: true
    score_inputs_baseline_keys:
    - avg_cpu_percent
    - avg_memory_mb
    - resource_suite_time_s
    - resource_tests_total
    score_inputs_baseline_mem_mb: 30.73
    score_inputs_baseline_cpu_pct: 98.5
    score_inputs_actual_mem_mb: 34.8
    score_inputs_actual_cpu_pct: 96.1
  robustness:
    returncode: 0
    stdout: '...                                                                      [100%]

      3 passed in 0.15s

      '
    elapsed_time_s: 1.548058
    avg_memory_mb: 30.78
    avg_cpu_percent: 98.9
    passed: 3
    failed: 0
    skipped: 0
    total: 3
    score_inputs_passed: 3
    score_inputs_failed: 0
    score_inputs_total: 3
    score_inputs_returncode: 0
    score_inputs_failed_suite: false
    score_inputs_baseline_keys:
    - robustness_suite_time_s
    - robustness_tests_total
  security:
    returncode: 0
    stdout: 'SECURITY_METRICS high_risk_count=0.0 files_scanned=6.0 total_loc=329.0

      .

      1 passed in 0.16s

      '
    elapsed_time_s: 1.680897
    avg_memory_mb: 31.45
    avg_cpu_percent: 96.0
    passed: 1
    failed: 0
    skipped: 0
    total: 1
    metrics:
      high_risk_count: 0.0
      files_scanned: 6.0
      total_loc: 329.0
    score_inputs_passed: 1
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 0
    score_inputs_failed_suite: false
    score_inputs_baseline_keys:
    - metrics
    - security_suite_time_s
    - security_tests_total
    score_inputs_baseline_high_risk_count: 0.0
    score_inputs_generated_high_risk_count: 0.0
  maintainability:
    returncode: 0
    stdout: 'MAINT_METRICS mi_min=27.6744 files_scanned=6.0 total_loc=329.0 max_cc=15.0

      .

      1 passed in 0.22s

      '
    elapsed_time_s: 1.76606
    avg_memory_mb: 31.63
    avg_cpu_percent: 97.2
    passed: 1
    failed: 0
    skipped: 0
    total: 1
    metrics:
      mi_min: 27.6744
      files_scanned: 6.0
      total_loc: 329.0
      max_cc: 15.0
    score_inputs_passed: 1
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 0
    score_inputs_failed_suite: false
    score_inputs_baseline_keys:
    - maintainability_suite_time_s
    - maintainability_tests_total
    - metrics
    score_inputs_baseline_mi_min: 5.2014
    score_inputs_generated_mi_min: 27.6744
    score_inputs_ratio_g_over_b: 5.320567539508594
baseline_metrics:
  performance:
    performance_suite_time_s: 1.604119
    performance_tests_total: 1
  resource:
    resource_suite_time_s: 1.196727
    resource_tests_total: 1
    avg_memory_mb: 30.73
    avg_cpu_percent: 98.5
  functional:
    functional_suite_time_s: 1.228457
    functional_tests_total: 15
  maintainability:
    maintainability_suite_time_s: 1.428683
    maintainability_tests_total: 1
    metrics:
      mi_min: 5.2014
      files_scanned: 12.0
      total_loc: 2595.0
      max_cc: 30.0
  robustness:
    robustness_suite_time_s: 1.219278
    robustness_tests_total: 3
  security:
    security_suite_time_s: 1.270661
    security_tests_total: 1
    metrics:
      high_risk_count: 0.0
      files_scanned: 7.0
      total_loc: 1285.0
pytest_logs_dir: D:\桌面\RealAppCodeBench_generic_eval\results\Humanize\pytest_logs

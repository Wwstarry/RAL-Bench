project_name: Typer
task_file: D:\桌面\RealAppCodeBench_generic_eval\tasks\Typer\typer.yaml
generated_repo: D:\桌面\RealAppCodeBench_generic_eval\generation_m1\Typer
timestamp: '2026-01-14 23:55:00'
functional_score: 0.3333
non_functional_score: 0.7355
non_functional_subscores:
  maintainability: 0.932
  security: 1.0
  robustness: 1.0
  performance: 0.0
  resource: 0.0
non_functional_weights:
  maintainability: 0.36
  security: 0.24
  robustness: 0.16
  performance: 0.12
  resource: 0.12
results:
  functional:
    returncode: 1
    stdout: "FFF..F..FFFF                                                        \
      \     [100%]\n================================== FAILURES ===================================\n\
      __________________________ test_simple_hello_command __________________________\n\
      \n    def test_simple_hello_command() -> None:\n>       app = _create_greeter_app()\n\
      \ntests\\Typer\\functional_test.py:197: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    def _create_greeter_app()\
      \ -> typer.Typer:\n        \"\"\"\n        Single-command style app (callback-only):\n\
      \          app NAME [--excited]\n        \"\"\"\n        app = typer.Typer()\n\
      \    \n>       @app.callback(invoke_without_command=True)\nE       TypeError:\
      \ callback() got an unexpected keyword argument 'invoke_without_command'\n\n\
      tests\\Typer\\functional_test.py:70: TypeError\n______________________ test_simple_hello_command_excited\
      \ ______________________\n\n    def test_simple_hello_command_excited() -> None:\n\
      >       app = _create_greeter_app()\n\ntests\\Typer\\functional_test.py:204:\
      \ \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _\n\n    def _create_greeter_app() -> typer.Typer:\n        \"\"\"\n\
      \        Single-command style app (callback-only):\n          app NAME [--excited]\n\
      \        \"\"\"\n        app = typer.Typer()\n    \n>       @app.callback(invoke_without_command=True)\n\
      E       TypeError: callback() got an unexpected keyword argument 'invoke_without_command'\n\
      \ntests\\Typer\\functional_test.py:70: TypeError\n_______________ test_greeter_help_mentions_option_and_argument\
      \ ________________\n\n    def test_greeter_help_mentions_option_and_argument()\
      \ -> None:\n>       app = _create_greeter_app()\n\ntests\\Typer\\functional_test.py:212:\
      \ \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _\n\n    def _create_greeter_app() -> typer.Typer:\n        \"\"\"\n\
      \        Single-command style app (callback-only):\n          app NAME [--excited]\n\
      \        \"\"\"\n        app = typer.Typer()\n    \n>       @app.callback(invoke_without_command=True)\n\
      E       TypeError: callback() got an unexpected keyword argument 'invoke_without_command'\n\
      \ntests\\Typer\\functional_test.py:70: TypeError\n_____________________ test_todo_remove_then_list_updates\
      \ ______________________\n\n    def test_todo_remove_then_list_updates() ->\
      \ None:\n        app = _create_todo_app()\n    \n        runner.invoke(app,\
      \ [\"add\", \"Task 1\"])\n        runner.invoke(app, [\"add\", \"Task 2\"])\n\
      \    \n        r_remove = runner.invoke(app, [\"remove\", \"1\"])\n>       assert\
      \ r_remove.exit_code == 0\nE       assert 1 == 0\nE        +  where 1 = Result(exit_code=1,\
      \ stdout='', stderr=\"unsupported operand type(s) for -: 'str' and 'int'\\n\"\
      , exception=None).exit_code\n\ntests\\Typer\\functional_test.py:252: AssertionError\n\
      ________________________ test_prompt_option_happy_path ________________________\n\
      \n    def test_prompt_option_happy_path() -> None:\n        app = _create_prompt_app()\n\
      \        # Now stable: \"greet\" always exists as a subcommand (multi-command\
      \ app).\n        result = runner.invoke(app, [\"greet\"], input=\"Alice\\n\"\
      )\n        assert result.exit_code == 0\n>       assert \"Hi Alice\" in result.stdout\n\
      E       AssertionError: assert 'Hi Alice' in 'Hi None\\n'\nE        +  where\
      \ 'Hi None\\n' = Result(exit_code=0, stdout='Hi None\\n', stderr='', exception=None).stdout\n\
      \ntests\\Typer\\functional_test.py:284: AssertionError\n________________________\
      \ test_envvar_option_happy_path ________________________\n\nmonkeypatch = <_pytest.monkeypatch.MonkeyPatch\
      \ object at 0x000001CB660593D0>\n\n    def test_envvar_option_happy_path(monkeypatch:\
      \ pytest.MonkeyPatch) -> None:\n        app = _create_env_app()\n        monkeypatch.setenv(\"\
      APP_TOKEN\", \"abc123\")\n    \n        result = runner.invoke(app, [\"show\"\
      ])\n>       assert result.exit_code == 0\nE       AssertionError: assert 2 ==\
      \ 0\nE        +  where 2 = Result(exit_code=2, stdout='', stderr='Error: Missing\
      \ option --token\\n', exception=None).exit_code\n\ntests\\Typer\\functional_test.py:292:\
      \ AssertionError\n_____________ test_callback_global_option_affects_command_output\
      \ ______________\n\n    def test_callback_global_option_affects_command_output()\
      \ -> None:\n        app = _create_callback_app()\n    \n        r1 = runner.invoke(app,\
      \ [\"run\"])\n        assert r1.exit_code == 0\n        assert \"running\" in\
      \ r1.stdout\n>       assert \"verbose\" not in r1.stdout\nE       AssertionError:\
      \ assert 'verbose' not in 'running (verbose)\\n'\nE         \nE         'verbose'\
      \ is contained here:\nE           running (verbose)\nE         ?          +++++++\n\
      \ntests\\Typer\\functional_test.py:302: AssertionError\n____________________\
      \ test_typed_arguments_and_float_option ____________________\n\n    def test_typed_arguments_and_float_option()\
      \ -> None:\n        app = _create_types_app()\n        # Now stable: \"calc\"\
      \ always exists as a subcommand (multi-command app).\n        r = runner.invoke(app,\
      \ [\"calc\", \"2\", \"3\", \"--scale\", \"2.0\"])\n>       assert r.exit_code\
      \ == 0\nE       AssertionError: assert 2 == 0\nE        +  where 2 = Result(exit_code=2,\
      \ stdout='', stderr='Error: Got unexpected extra argument\\n', exception=None).exit_code\n\
      \ntests\\Typer\\functional_test.py:313: AssertionError\n===========================\
      \ short test summary info ===========================\nFAILED tests/Typer/functional_test.py::test_simple_hello_command\
      \ - TypeError:...\nFAILED tests/Typer/functional_test.py::test_simple_hello_command_excited\
      \ - Ty...\nFAILED tests/Typer/functional_test.py::test_greeter_help_mentions_option_and_argument\n\
      FAILED tests/Typer/functional_test.py::test_todo_remove_then_list_updates -\
      \ a...\nFAILED tests/Typer/functional_test.py::test_prompt_option_happy_path\
      \ - Assert...\nFAILED tests/Typer/functional_test.py::test_envvar_option_happy_path\
      \ - Assert...\nFAILED tests/Typer/functional_test.py::test_callback_global_option_affects_command_output\n\
      FAILED tests/Typer/functional_test.py::test_typed_arguments_and_float_option\n\
      8 failed, 4 passed in 0.56s\n"
    elapsed_time_s: 2.115731
    avg_memory_mb: 32.27
    avg_cpu_percent: 99.2
    passed: 4
    failed: 8
    skipped: 0
    total: 12
    score_inputs_passed: 4
    score_inputs_failed: 8
    score_inputs_total: 12
    score_inputs_returncode: 1
    score_inputs_failed_suite: true
    score_inputs_baseline_keys:
    - functional_suite_time_s
    - functional_tests_total
  performance:
    returncode: 2
    stdout: "\n=================================== ERRORS ====================================\n\
      ______________ ERROR collecting tests/Typer/performance_test.py _______________\n\
      tests\\Typer\\performance_test.py:18: in <module>\n    raise RuntimeError(f\"\
      Target repository does not exist: {REPO_ROOT}\")\nE   RuntimeError: Target repository\
      \ does not exist: D:\\桌面\\RealAppCodeBench_generic_eval\\generation\\Typer\n\
      =========================== short test summary info ===========================\n\
      ERROR tests/Typer/performance_test.py - RuntimeError: Target repository does\
      \ ...\n!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!\n\
      1 error in 0.46s\n"
    elapsed_time_s: 1.666576
    avg_memory_mb: 35.79
    avg_cpu_percent: 101.0
    passed: 0
    failed: 0
    skipped: 0
    total: 1
    score_inputs_passed: 0
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 2
    score_inputs_failed_suite: true
    score_inputs_baseline_keys:
    - performance_suite_time_s
    - performance_tests_total
    score_inputs_baseline_time_s: 1.64517
    score_inputs_actual_time_s: 1.666576
  resource:
    returncode: 2
    stdout: "\n=================================== ERRORS ====================================\n\
      ________________ ERROR collecting tests/Typer/resource_test.py ________________\n\
      tests\\Typer\\resource_test.py:17: in <module>\n    raise RuntimeError(f\"Target\
      \ repository does not exist: {REPO_ROOT}\")\nE   RuntimeError: Target repository\
      \ does not exist: D:\\桌面\\RealAppCodeBench_generic_eval\\generation\\Typer\n\
      =========================== short test summary info ===========================\n\
      ERROR tests/Typer/resource_test.py - RuntimeError: Target repository does not...\n\
      !!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!\n\
      1 error in 0.45s\n"
    elapsed_time_s: 1.622848
    avg_memory_mb: 35.81
    avg_cpu_percent: 98.0
    passed: 0
    failed: 0
    skipped: 0
    total: 1
    score_inputs_passed: 0
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 2
    score_inputs_failed_suite: true
    score_inputs_baseline_keys:
    - avg_cpu_percent
    - avg_memory_mb
    - resource_suite_time_s
    - resource_tests_total
    score_inputs_baseline_mem_mb: 39.07
    score_inputs_baseline_cpu_pct: 99.2
    score_inputs_actual_mem_mb: 35.81
    score_inputs_actual_cpu_pct: 98.0
  robustness:
    returncode: 0
    stdout: '....                                                                     [100%]

      4 passed in 0.13s

      '
    elapsed_time_s: 1.450113
    avg_memory_mb: 31.84
    avg_cpu_percent: 98.8
    passed: 4
    failed: 0
    skipped: 0
    total: 4
    score_inputs_passed: 4
    score_inputs_failed: 0
    score_inputs_total: 4
    score_inputs_returncode: 0
    score_inputs_failed_suite: false
    score_inputs_baseline_keys:
    - robustness_suite_time_s
    - robustness_tests_total
  security:
    returncode: 0
    stdout: 'SECURITY_METRICS high_risk_count=0.0 files_scanned=6.0 total_loc=576.0

      .

      1 passed in 0.12s

      '
    elapsed_time_s: 1.290595
    avg_memory_mb: 32.24
    avg_cpu_percent: 101.3
    passed: 1
    failed: 0
    skipped: 0
    total: 1
    metrics:
      high_risk_count: 0.0
      files_scanned: 6.0
      total_loc: 576.0
    score_inputs_passed: 1
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 0
    score_inputs_failed_suite: false
    score_inputs_baseline_keys:
    - metrics
    - security_suite_time_s
    - security_tests_total
    score_inputs_baseline_high_risk_count: 0.0
    score_inputs_generated_high_risk_count: 0.0
  maintainability:
    returncode: 0
    stdout: 'MAINT_METRICS mi_min=12.8955 files_scanned=6.0 total_loc=576.0 max_cc=47.0

      .

      1 passed in 0.17s

      '
    elapsed_time_s: 1.290942
    avg_memory_mb: 32.08
    avg_cpu_percent: 100.0
    passed: 1
    failed: 0
    skipped: 0
    total: 1
    metrics:
      mi_min: 12.8955
      files_scanned: 6.0
      total_loc: 576.0
      max_cc: 47.0
    score_inputs_passed: 1
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 0
    score_inputs_failed_suite: false
    score_inputs_baseline_keys:
    - maintainability_suite_time_s
    - maintainability_tests_total
    - metrics
    score_inputs_baseline_mi_min: 0.9413
    score_inputs_generated_mi_min: 12.8955
    score_inputs_ratio_g_over_b: 13.699670668224796
baseline_metrics:
  performance:
    performance_suite_time_s: 1.64517
    performance_tests_total: 1
  resource:
    resource_suite_time_s: 2.053706
    resource_tests_total: 2
    avg_memory_mb: 39.07
    avg_cpu_percent: 99.2
  functional:
    functional_suite_time_s: 2.016633
    functional_tests_total: 12
  robustness:
    robustness_suite_time_s: 2.19185
    robustness_tests_total: 4
  security:
    security_suite_time_s: 2.161153
    security_tests_total: 1
    metrics:
      high_risk_count: 0.0
      files_scanned: 16.0
      total_loc: 4106.0
  maintainability:
    maintainability_suite_time_s: 2.28696
    maintainability_tests_total: 1
    metrics:
      mi_min: 0.9413
      files_scanned: 16.0
      total_loc: 4106.0
      max_cc: 30.0
pytest_logs_dir: D:\桌面\RealAppCodeBench_generic_eval\results\Typer\pytest_logs

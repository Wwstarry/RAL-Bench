project_name: Typer
task_file: D:\桌面\RealAppCodeBench_generic_eval\tasks\Typer\typer.yaml
generated_repo: D:\桌面\RealAppCodeBench_generic_eval\generation\Typer
timestamp: '2026-01-01 00:52:47'
functional_score: 0.1667
non_functional_score: 0.8583
non_functional_subscores:
  maintainability: 0.9397
  security: 1.0
  robustness: 1.0
  performance: 1.0
  resource: 0.0
non_functional_weights:
  maintainability: 0.36
  security: 0.24
  robustness: 0.16
  performance: 0.12
  resource: 0.12
results:
  functional:
    returncode: 1
    stdout: "FFFFFF..FFFF                                                        \
      \     [100%]\n================================== FAILURES ===================================\n\
      __________________________ test_simple_hello_command __________________________\n\
      \n    def test_simple_hello_command() -> None:\n        app = _create_greeter_app()\n\
      \        result = runner.invoke(app, [\"World\"])\n>       assert result.exit_code\
      \ == 0\nE       AssertionError: assert 2 == 0\nE        +  where 2 = Result(stdout='',\
      \ stderr='Error: No such command: World\\n', exit_code=2, exception=None).exit_code\n\
      \ntests\\Typer\\functional_test.py:199: AssertionError\n______________________\
      \ test_simple_hello_command_excited ______________________\n\n    def test_simple_hello_command_excited()\
      \ -> None:\n        app = _create_greeter_app()\n        # Safer ordering across\
      \ Click versions: options before args.\n        result = runner.invoke(app,\
      \ [\"--excited\", \"World\"])\n>       assert result.exit_code == 0\nE     \
      \  AssertionError: assert 2 == 0\nE        +  where 2 = Result(stdout='', stderr='Error:\
      \ No such option: --excited\\n', exit_code=2, exception=None).exit_code\n\n\
      tests\\Typer\\functional_test.py:207: AssertionError\n_______________ test_greeter_help_mentions_option_and_argument\
      \ ________________\n\n    def test_greeter_help_mentions_option_and_argument()\
      \ -> None:\n        app = _create_greeter_app()\n        result = runner.invoke(app,\
      \ [\"--help\"])\n        assert result.exit_code == 0\n        out = result.stdout\n\
      >       assert \"--excited\" in out\nE       AssertionError: assert '--excited'\
      \ in 'Usage: app [OPTIONS] COMMAND [ARGS]...\\n\\nOptions:\\n  --help      \
      \   Show this message and exit.\\n\\nCommands:\\n'\n\ntests\\Typer\\functional_test.py:216:\
      \ AssertionError\n_____________________ test_todo_list_empty_shows_no_tasks\
      \ _____________________\n\n    def test_todo_list_empty_shows_no_tasks() ->\
      \ None:\n        app = _create_todo_app()\n        r = runner.invoke(app, [\"\
      list\"])\n        assert r.exit_code == 0\n>       assert \"No tasks.\" in r.stdout\n\
      E       AssertionError: assert 'No tasks.' in ''\nE        +  where '' = Result(stdout='',\
      \ stderr='', exit_code=0, exception=None).stdout\n\ntests\\Typer\\functional_test.py:224:\
      \ AssertionError\n---------------------------- Captured stdout call -----------------------------\n\
      No tasks.\n___________________________ test_todo_add_and_list ____________________________\n\
      \n    def test_todo_add_and_list() -> None:\n        app = _create_todo_app()\n\
      \    \n        r1 = runner.invoke(app, [\"add\", \"Write tests\"])\n       \
      \ r2 = runner.invoke(app, [\"add\", \"Review PRs\"])\n    \n        assert r1.exit_code\
      \ == 0\n>       assert \"Added: Write tests\" in r1.stdout\nE       AssertionError:\
      \ assert 'Added: Write tests' in ''\nE        +  where '' = Result(stdout='',\
      \ stderr='', exit_code=0, exception=None).stdout\n\ntests\\Typer\\functional_test.py:234:\
      \ AssertionError\n---------------------------- Captured stdout call -----------------------------\n\
      Added: Write tests\nAdded: Review PRs\n_____________________ test_todo_remove_then_list_updates\
      \ ______________________\n\n    def test_todo_remove_then_list_updates() ->\
      \ None:\n        app = _create_todo_app()\n    \n        runner.invoke(app,\
      \ [\"add\", \"Task 1\"])\n        runner.invoke(app, [\"add\", \"Task 2\"])\n\
      \    \n        r_remove = runner.invoke(app, [\"remove\", \"1\"])\n>       assert\
      \ r_remove.exit_code == 0\nE       assert 1 == 0\nE        +  where 1 = Result(stdout='',\
      \ stderr=\"Error: unsupported operand type(s) for -: 'str' and 'int'\\n\", exit_code=1,\
      \ exception=TypeError(\"unsupported operand type(s) for -: 'str' and 'int'\"\
      )).exit_code\n\ntests\\Typer\\functional_test.py:252: AssertionError\n----------------------------\
      \ Captured stdout call -----------------------------\nAdded: Task 1\nAdded:\
      \ Task 2\n________________________ test_prompt_option_happy_path ________________________\n\
      \n    def test_prompt_option_happy_path() -> None:\n>       app = _create_prompt_app()\n\
      \ntests\\Typer\\functional_test.py:280: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    def _create_prompt_app()\
      \ -> typer.Typer:\n        \"\"\"\n        Multi-command app to avoid Typer's\
      \ single-command \"collapse\" behavior in\n        some versions. This guarantees\
      \ that \"greet\" exists as a subcommand.\n        \"\"\"\n        app = typer.Typer()\n\
      \    \n        @app.command()\n        def greet(\n>           name: str = typer.Option(\n\
      \                None,\n                \"--name\",\n                prompt=True,\n\
      \                help=\"Name to greet (prompted when missing).\",\n        \
      \    )\n        ) -> None:\nE       TypeError: Option() got an unexpected keyword\
      \ argument 'prompt'\n\ntests\\Typer\\functional_test.py:121: TypeError\n________________________\
      \ test_envvar_option_happy_path ________________________\n\nmonkeypatch = <_pytest.monkeypatch.MonkeyPatch\
      \ object at 0x0000014E9821E190>\n\n    def test_envvar_option_happy_path(monkeypatch:\
      \ pytest.MonkeyPatch) -> None:\n>       app = _create_env_app()\n\ntests\\Typer\\\
      functional_test.py:288: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    def _create_env_app() -> typer.Typer:\n\
      \        \"\"\"\n        Multi-command app to guarantee that \"show\" exists\
      \ as a subcommand.\n        \"\"\"\n        app = typer.Typer()\n    \n    \
      \    @app.command()\n>       def show(token: str = typer.Option(..., \"--token\"\
      , envvar=\"APP_TOKEN\")) -> None:\nE       TypeError: Option() got an unexpected\
      \ keyword argument 'envvar'\n\ntests\\Typer\\functional_test.py:144: TypeError\n\
      _____________ test_callback_global_option_affects_command_output ______________\n\
      \n    def test_callback_global_option_affects_command_output() -> None:\n  \
      \      app = _create_callback_app()\n    \n        r1 = runner.invoke(app, [\"\
      run\"])\n        assert r1.exit_code == 0\n>       assert \"running\" in r1.stdout\n\
      E       AssertionError: assert 'running' in ''\nE        +  where '' = Result(stdout='',\
      \ stderr='', exit_code=0, exception=None).stdout\n\ntests\\Typer\\functional_test.py:301:\
      \ AssertionError\n---------------------------- Captured stdout call -----------------------------\n\
      running\n____________________ test_typed_arguments_and_float_option ____________________\n\
      \n    def test_typed_arguments_and_float_option() -> None:\n        app = _create_types_app()\n\
      \        # Now stable: \"calc\" always exists as a subcommand (multi-command\
      \ app).\n        r = runner.invoke(app, [\"calc\", \"2\", \"3\", \"--scale\"\
      , \"2.0\"])\n>       assert r.exit_code == 0\nE       assert 1 == 0\nE     \
      \   +  where 1 = Result(stdout='', stderr=\"Error: can't multiply sequence by\
      \ non-int of type 'str'\\n\", exit_code=1, exception=TypeError(\"can't multiply\
      \ sequence by non-int of type 'str'\")).exit_code\n\ntests\\Typer\\functional_test.py:313:\
      \ AssertionError\n=========================== short test summary info ===========================\n\
      FAILED tests/Typer/functional_test.py::test_simple_hello_command - AssertionE...\n\
      FAILED tests/Typer/functional_test.py::test_simple_hello_command_excited - As...\n\
      FAILED tests/Typer/functional_test.py::test_greeter_help_mentions_option_and_argument\n\
      FAILED tests/Typer/functional_test.py::test_todo_list_empty_shows_no_tasks -\
      \ ...\nFAILED tests/Typer/functional_test.py::test_todo_add_and_list - AssertionErro...\n\
      FAILED tests/Typer/functional_test.py::test_todo_remove_then_list_updates -\
      \ a...\nFAILED tests/Typer/functional_test.py::test_prompt_option_happy_path\
      \ - TypeEr...\nFAILED tests/Typer/functional_test.py::test_envvar_option_happy_path\
      \ - TypeEr...\nFAILED tests/Typer/functional_test.py::test_callback_global_option_affects_command_output\n\
      FAILED tests/Typer/functional_test.py::test_typed_arguments_and_float_option\n\
      10 failed, 2 passed in 0.68s\n"
    elapsed_time_s: 1.97494
    avg_memory_mb: 32.69
    avg_cpu_percent: 98.3
    passed: 2
    failed: 10
    skipped: 0
    total: 12
    score_inputs_passed: 2
    score_inputs_failed: 10
    score_inputs_total: 12
    score_inputs_returncode: 1
    score_inputs_failed_suite: true
    score_inputs_baseline_keys:
    - functional_suite_time_s
    - functional_tests_total
  performance:
    returncode: 0
    stdout: '.                                                                        [100%]

      1 passed in 0.10s

      '
    elapsed_time_s: 1.326689
    avg_memory_mb: 31.2
    avg_cpu_percent: 100.0
    passed: 1
    failed: 0
    skipped: 0
    total: 1
    score_inputs_passed: 1
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 0
    score_inputs_failed_suite: false
    score_inputs_baseline_keys:
    - performance_suite_time_s
    - performance_tests_total
    score_inputs_baseline_time_s: 1.64517
    score_inputs_actual_time_s: 1.326689
  resource:
    returncode: 1
    stdout: "F.                                                                  \
      \     [100%]\n================================== FAILURES ===================================\n\
      ______________________ test_project_workflow_integration ______________________\n\
      \n    def test_project_workflow_integration() -> None:\n        \"\"\"End-to-end\
      \ test of initializing a project and managing tasks.\"\"\"\n        app = _create_project_app()\n\
      \    \n        r1 = runner.invoke(app, [\"init\", \"demo\"])\n        assert\
      \ r1.exit_code == 0\n>       assert \"Initialized project demo\" in r1.stdout\n\
      E       AssertionError: assert 'Initialized project demo' in ''\nE        +\
      \  where '' = Result(stdout='', stderr='', exit_code=0, exception=None).stdout\n\
      \ntests\\Typer\\resource_test.py:70: AssertionError\n----------------------------\
      \ Captured stdout call -----------------------------\nInitialized project demo\n\
      =========================== short test summary info ===========================\n\
      FAILED tests/Typer/resource_test.py::test_project_workflow_integration - Asse...\n\
      1 failed, 1 passed in 0.42s\n"
    elapsed_time_s: 1.822738
    avg_memory_mb: 31.44
    avg_cpu_percent: 97.3
    passed: 1
    failed: 1
    skipped: 0
    total: 2
    score_inputs_passed: 1
    score_inputs_failed: 1
    score_inputs_total: 2
    score_inputs_returncode: 1
    score_inputs_failed_suite: true
    score_inputs_baseline_keys:
    - avg_cpu_percent
    - avg_memory_mb
    - resource_suite_time_s
    - resource_tests_total
    score_inputs_baseline_mem_mb: 39.07
    score_inputs_baseline_cpu_pct: 99.2
    score_inputs_actual_mem_mb: 31.44
    score_inputs_actual_cpu_pct: 97.3
  robustness:
    returncode: 0
    stdout: '....                                                                     [100%]

      4 passed in 0.17s

      '
    elapsed_time_s: 1.564819
    avg_memory_mb: 31.09
    avg_cpu_percent: 103.2
    passed: 4
    failed: 0
    skipped: 0
    total: 4
    score_inputs_passed: 4
    score_inputs_failed: 0
    score_inputs_total: 4
    score_inputs_returncode: 0
    score_inputs_failed_suite: false
    score_inputs_baseline_keys:
    - robustness_suite_time_s
    - robustness_tests_total
  security:
    returncode: 0
    stdout: 'SECURITY_METRICS high_risk_count=0.0 files_scanned=5.0 total_loc=370.0

      .

      1 passed in 0.17s

      '
    elapsed_time_s: 1.472721
    avg_memory_mb: 31.74
    avg_cpu_percent: 100.0
    passed: 1
    failed: 0
    skipped: 0
    total: 1
    metrics:
      high_risk_count: 0.0
      files_scanned: 5.0
      total_loc: 370.0
    score_inputs_passed: 1
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 0
    score_inputs_failed_suite: false
    score_inputs_baseline_keys:
    - metrics
    - security_suite_time_s
    - security_tests_total
    score_inputs_baseline_high_risk_count: 0.0
    score_inputs_generated_high_risk_count: 0.0
  maintainability:
    returncode: 0
    stdout: 'MAINT_METRICS mi_min=14.6778 files_scanned=4.0 total_loc=370.0 max_cc=44.0

      .

      1 passed in 0.15s

      '
    elapsed_time_s: 1.455714
    avg_memory_mb: 31.88
    avg_cpu_percent: 98.8
    passed: 1
    failed: 0
    skipped: 0
    total: 1
    metrics:
      mi_min: 14.6778
      files_scanned: 4.0
      total_loc: 370.0
      max_cc: 44.0
    score_inputs_passed: 1
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 0
    score_inputs_failed_suite: false
    score_inputs_baseline_keys:
    - maintainability_suite_time_s
    - maintainability_tests_total
    - metrics
    score_inputs_baseline_mi_min: 0.9413
    score_inputs_generated_mi_min: 14.6778
    score_inputs_ratio_g_over_b: 15.59311590353766
baseline_metrics:
  performance:
    performance_suite_time_s: 1.64517
    performance_tests_total: 1
  resource:
    resource_suite_time_s: 2.053706
    resource_tests_total: 2
    avg_memory_mb: 39.07
    avg_cpu_percent: 99.2
  functional:
    functional_suite_time_s: 2.016633
    functional_tests_total: 12
  robustness:
    robustness_suite_time_s: 2.19185
    robustness_tests_total: 4
  security:
    security_suite_time_s: 2.161153
    security_tests_total: 1
    metrics:
      high_risk_count: 0.0
      files_scanned: 16.0
      total_loc: 4106.0
  maintainability:
    maintainability_suite_time_s: 2.28696
    maintainability_tests_total: 1
    metrics:
      mi_min: 0.9413
      files_scanned: 16.0
      total_loc: 4106.0
      max_cc: 30.0
pytest_logs_dir: D:\桌面\RealAppCodeBench_generic_eval\results\Typer\pytest_logs

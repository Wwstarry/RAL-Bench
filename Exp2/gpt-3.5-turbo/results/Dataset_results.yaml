project_name: Dataset
task_file: D:\桌面\RealAppCodeBench_generic_eval\tasks\Dataset\dataset.yaml
generated_repo: D:\桌面\RealAppCodeBench_generic_eval\generation\Dataset
timestamp: '2025-12-31 20:59:03'
functional_score: 0.3636
non_functional_score: 0.66
non_functional_subscores:
  maintainability: 0.7223
  security: 1.0
  robustness: 1.0
  performance: 0.0
  resource: 0.0
non_functional_weights:
  maintainability: 0.36
  security: 0.24
  robustness: 0.16
  performance: 0.12
  resource: 0.12
results:
  functional:
    returncode: 1
    stdout: "FFF..FF.F.F                                                         \
      \     [100%]\n================================== FAILURES ===================================\n\
      ______________________ test_insert_and_query_basic_rows _______________________\n\
      \n    def test_insert_and_query_basic_rows() -> None:\n        db = create_in_memory_db()\n\
      \        table = db[\"users\"]\n    \n        table.insert({\"name\": \"Alice\"\
      , \"age\": 30, \"country\": \"DE\"})\n        table.insert({\"name\": \"Bob\"\
      , \"age\": 41, \"country\": \"US\", \"active\": True})\n        table.insert({\"\
      name\": \"Charlie\", \"age\": 41, \"country\": \"US\", \"active\": False})\n\
      \    \n>       assert \"id\" in _table_columns(table)\nE       AssertionError:\
      \ assert 'id' in ['name', 'age', 'country', 'active']\nE        +  where ['name',\
      \ 'age', 'country', 'active'] = _table_columns(<dataset.table.Table object at\
      \ 0x0000025575C01E20>)\n\ntests\\Dataset\\functional_test.py:146: AssertionError\n\
      _______________________ test_update_upsert_and_indexes ________________________\n\
      \n    def test_update_upsert_and_indexes() -> None:\n        db = create_in_memory_db()\n\
      \        table = db[\"accounts\"]\n    \n        rows = [\n            {\"account_id\"\
      : 1, \"owner\": \"Alice\", \"balance\": 100.0, \"currency\": \"EUR\"},\n   \
      \         {\"account_id\": 2, \"owner\": \"Bob\", \"balance\": 250.0, \"currency\"\
      : \"USD\"},\n        ]\n        table.insert_many(rows)\n    \n        if hasattr(table,\
      \ \"create_index\") and hasattr(table, \"has_index\"):\n            table.create_index([\"\
      owner\", \"currency\"])\n            assert table.has_index([\"owner\", \"currency\"\
      ])\n    \n        table.update({\"account_id\": 1, \"balance\": 150.0}, [\"\
      account_id\"])\n        updated = table.find_one(account_id=1)\n        assert\
      \ updated is not None\n>       assert pytest.approx(updated[\"balance\"]) ==\
      \ 150.0\nE       assert 150.0 == 150.0\nE         \nE         comparison failed\n\
      E         Obtained: 150.0\nE         Expected: 150.0\n\ntests\\Dataset\\functional_test.py:184:\
      \ AssertionError\n____________________ test_transactions_commit_and_rollback\
      \ ____________________\n\ntmp_path = WindowsPath('C:/Users/86152/AppData/Local/Temp/pytest-of-86152/pytest-333/test_transactions_commit_and_r0')\n\
      \n    def test_transactions_commit_and_rollback(tmp_path: Path) -> None:\n \
      \       db_path = tmp_path / \"tx_sample.db\"\n        db_url = \"sqlite:///%s\"\
      \ % str(db_path)\n        db = dataset.connect(db_url)\n        table = db[\"\
      events\"]\n    \n        db.begin()\n        table.insert({\"name\": \"committed\"\
      , \"category\": \"ok\"})\n>       db.commit()\n\ntests\\Dataset\\functional_test.py:204:\
      \ \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _\n\nself = <dataset.database.Database object at 0x0000025575C7FB50>\n\
      \n    def commit(self):\n        if not self._transaction_active:\n        \
      \    return\n>       self._conn.execute(\"COMMIT\")\nE       sqlite3.OperationalError:\
      \ cannot commit - no transaction is active\n\ngeneration\\Dataset\\dataset\\\
      database.py:36: OperationalError\n_______________________ test_find_order_by_limit_offset\
      \ _______________________\n\n    def test_find_order_by_limit_offset() -> None:\n\
      \        db = create_in_memory_db()\n        table = db[\"nums\"]\n        for\
      \ i in range(10):\n            table.insert({\"n\": i})\n    \n        rows\
      \ = list(table.find(order_by=\"n\", _limit=3, _offset=4))\n>       assert [r[\"\
      n\"] for r in rows] == [4, 5, 6]\nE       assert [] == [4, 5, 6]\nE        \
      \ \nE         Right contains 3 more items, first extra item: 4\nE         Use\
      \ -v to get more diff\n\ntests\\Dataset\\functional_test.py:249: AssertionError\n\
      ___________________ test_table_all_iteration_and_row_shape ____________________\n\
      \n    def test_table_all_iteration_and_row_shape() -> None:\n        db = create_in_memory_db()\n\
      \        table = db[\"people\"]\n        table.insert({\"name\": \"Alice\",\
      \ \"age\": 30})\n        table.insert({\"name\": \"Bob\", \"age\": 31})\n  \
      \  \n        rows = list(table.all())\n        assert len(rows) == 2\n>    \
      \   assert all((\"id\" in r and \"name\" in r) for r in rows)\nE       assert\
      \ False\nE        +  where False = all(<generator object test_table_all_iteration_and_row_shape.<locals>.<genexpr>\
      \ at 0x0000025575BC8350>)\n\ntests\\Dataset\\functional_test.py:260: AssertionError\n\
      ___________________ test_drop_table_removes_from_db_tables ____________________\n\
      \n    def test_drop_table_removes_from_db_tables() -> None:\n        db = create_in_memory_db()\n\
      \        table = db[\"to_drop\"]\n        table.insert({\"x\": 1})\n    \n>\
      \       assert \"to_drop\" in _db_tables(db)\nE       AssertionError: assert\
      \ 'to_drop' in []\nE        +  where [] = _db_tables(<dataset.database.Database\
      \ object at 0x0000025575BF9A90>)\n\ntests\\Dataset\\functional_test.py:301:\
      \ AssertionError\n_____________________ test_distinct_returns_unique_values\
      \ _____________________\n\n    def test_distinct_returns_unique_values() ->\
      \ None:\n        db = create_in_memory_db()\n        table = db[\"colors\"]\n\
      \        table.insert_many([{\"c\": \"red\"}, {\"c\": \"red\"}, {\"c\": \"blue\"\
      }])\n    \n        distinct = list(table.distinct(\"c\"))\n>       values =\
      \ {r[\"c\"] for r in distinct}\n\ntests\\Dataset\\functional_test.py:333: \n\
      _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _\n\n.0 = <list_iterator object at 0x0000025575C7F970>\n\n>   values = {r[\"\
      c\"] for r in distinct}\nE   TypeError: string indices must be integers\n\n\
      tests\\Dataset\\functional_test.py:333: TypeError\n===========================\
      \ short test summary info ===========================\nFAILED tests/Dataset/functional_test.py::test_insert_and_query_basic_rows\
      \ - A...\nFAILED tests/Dataset/functional_test.py::test_update_upsert_and_indexes\
      \ - ass...\nFAILED tests/Dataset/functional_test.py::test_transactions_commit_and_rollback\n\
      FAILED tests/Dataset/functional_test.py::test_find_order_by_limit_offset - as...\n\
      FAILED tests/Dataset/functional_test.py::test_table_all_iteration_and_row_shape\n\
      FAILED tests/Dataset/functional_test.py::test_drop_table_removes_from_db_tables\n\
      FAILED tests/Dataset/functional_test.py::test_distinct_returns_unique_values\n\
      7 failed, 4 passed in 3.10s\n"
    elapsed_time_s: 4.251592
    avg_memory_mb: 33.75
    avg_cpu_percent: 99.2
    passed: 4
    failed: 7
    skipped: 0
    total: 11
    score_inputs_passed: 4
    score_inputs_failed: 7
    score_inputs_total: 11
    score_inputs_returncode: 1
    score_inputs_failed_suite: true
    score_inputs_baseline_keys:
    - functional_suite_time_s
    - functional_tests_total
  performance:
    returncode: 2
    stdout: "\n=================================== ERRORS ====================================\n\
      _____________ ERROR collecting tests/Dataset/performance_test.py ______________\n\
      tests\\Dataset\\performance_test.py:18: in <module>\n    raise RuntimeError(f\"\
      Unsupported DATASET_TARGET value: {TARGET_ENV}\")\nE   RuntimeError: Unsupported\
      \ DATASET_TARGET value: generated\n=========================== short test summary\
      \ info ===========================\nERROR tests/Dataset/performance_test.py\
      \ - RuntimeError: Unsupported DATASET_T...\n!!!!!!!!!!!!!!!!!!! Interrupted:\
      \ 1 error during collection !!!!!!!!!!!!!!!!!!!!\n1 error in 0.43s\n"
    elapsed_time_s: 1.634604
    avg_memory_mb: 35.84
    avg_cpu_percent: 98.0
    passed: 0
    failed: 0
    skipped: 0
    total: 1
    score_inputs_passed: 0
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 2
    score_inputs_failed_suite: true
    score_inputs_baseline_keys:
    - performance_suite_time_s
    - performance_tests_total
    score_inputs_baseline_time_s: 2.781374
    score_inputs_actual_time_s: 1.634604
  resource:
    returncode: 2
    stdout: "\n=================================== ERRORS ====================================\n\
      _______________ ERROR collecting tests/Dataset/resource_test.py _______________\n\
      tests\\Dataset\\resource_test.py:18: in <module>\n    raise RuntimeError(f\"\
      Unsupported DATASET_TARGET value: {TARGET_ENV}\")\nE   RuntimeError: Unsupported\
      \ DATASET_TARGET value: generated\n=========================== short test summary\
      \ info ===========================\nERROR tests/Dataset/resource_test.py - RuntimeError:\
      \ Unsupported DATASET_TARG...\n!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during\
      \ collection !!!!!!!!!!!!!!!!!!!!\n1 error in 0.43s\n"
    elapsed_time_s: 1.573646
    avg_memory_mb: 35.49
    avg_cpu_percent: 101.1
    passed: 0
    failed: 0
    skipped: 0
    total: 1
    score_inputs_passed: 0
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 2
    score_inputs_failed_suite: true
    score_inputs_baseline_keys:
    - avg_cpu_percent
    - avg_memory_mb
    - resource_suite_time_s
    - resource_tests_total
    score_inputs_baseline_mem_mb: 55.98
    score_inputs_baseline_cpu_pct: 100.6
    score_inputs_actual_mem_mb: 35.49
    score_inputs_actual_cpu_pct: 101.1
  robustness:
    returncode: 0
    stdout: "......                                                              \
      \     [100%]\n============================== warnings summary ===============================\n\
      tests\\Dataset\\robustness_test.py:92\n  D:\\桌面\\RealAppCodeBench_generic_eval\\\
      tests\\Dataset\\robustness_test.py:92: PytestUnknownMarkWarning: Unknown pytest.mark.timeout\
      \ - is this a typo?  You can register custom marks to avoid this warning - for\
      \ details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.timeout(10)\n\
      \ntests\\Dataset\\robustness_test.py:102\n  D:\\桌面\\RealAppCodeBench_generic_eval\\\
      tests\\Dataset\\robustness_test.py:102: PytestUnknownMarkWarning: Unknown pytest.mark.timeout\
      \ - is this a typo?  You can register custom marks to avoid this warning - for\
      \ details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.timeout(10)\n\
      \ntests\\Dataset\\robustness_test.py:120\n  D:\\桌面\\RealAppCodeBench_generic_eval\\\
      tests\\Dataset\\robustness_test.py:120: PytestUnknownMarkWarning: Unknown pytest.mark.timeout\
      \ - is this a typo?  You can register custom marks to avoid this warning - for\
      \ details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.timeout(10)\n\
      \ntests\\Dataset\\robustness_test.py:135\n  D:\\桌面\\RealAppCodeBench_generic_eval\\\
      tests\\Dataset\\robustness_test.py:135: PytestUnknownMarkWarning: Unknown pytest.mark.timeout\
      \ - is this a typo?  You can register custom marks to avoid this warning - for\
      \ details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.timeout(10)\n\
      \ntests\\Dataset\\robustness_test.py:149\n  D:\\桌面\\RealAppCodeBench_generic_eval\\\
      tests\\Dataset\\robustness_test.py:149: PytestUnknownMarkWarning: Unknown pytest.mark.timeout\
      \ - is this a typo?  You can register custom marks to avoid this warning - for\
      \ details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.timeout(10)\n\
      \ntests\\Dataset\\robustness_test.py:163\n  D:\\桌面\\RealAppCodeBench_generic_eval\\\
      tests\\Dataset\\robustness_test.py:163: PytestUnknownMarkWarning: Unknown pytest.mark.timeout\
      \ - is this a typo?  You can register custom marks to avoid this warning - for\
      \ details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.timeout(10)\n\
      \n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n\
      6 passed, 6 warnings in 0.17s\n"
    elapsed_time_s: 1.521631
    avg_memory_mb: 32.24
    avg_cpu_percent: 98.8
    passed: 6
    failed: 0
    skipped: 0
    total: 6
    score_inputs_passed: 6
    score_inputs_failed: 0
    score_inputs_total: 6
    score_inputs_returncode: 0
    score_inputs_failed_suite: false
    score_inputs_baseline_keys:
    - robustness_suite_time_s
    - robustness_tests_total
  security:
    returncode: 0
    stdout: 'SECURITY_METRICS high_risk_count=0.0 files_scanned=3.0 total_loc=256.0

      .

      1 passed in 0.14s

      '
    elapsed_time_s: 1.574307
    avg_memory_mb: 31.65
    avg_cpu_percent: 95.8
    passed: 1
    failed: 0
    skipped: 0
    total: 1
    metrics:
      high_risk_count: 0.0
      files_scanned: 3.0
      total_loc: 256.0
    score_inputs_passed: 1
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 0
    score_inputs_failed_suite: false
    score_inputs_baseline_keys:
    - metrics
    - security_suite_time_s
    - security_tests_total
    score_inputs_baseline_high_risk_count: 0.0
    score_inputs_generated_high_risk_count: 0.0
  maintainability:
    returncode: 0
    stdout: 'MAINT_METRICS mi_min=20.0755 files_scanned=3.0 total_loc=256.0 max_cc=13.0

      .

      1 passed in 0.17s

      '
    elapsed_time_s: 1.637385
    avg_memory_mb: 32.07
    avg_cpu_percent: 100.0
    passed: 1
    failed: 0
    skipped: 0
    total: 1
    metrics:
      mi_min: 20.0755
      files_scanned: 3.0
      total_loc: 256.0
      max_cc: 13.0
    score_inputs_passed: 1
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 0
    score_inputs_failed_suite: false
    score_inputs_baseline_keys:
    - maintainability_suite_time_s
    - maintainability_tests_total
    - metrics
    score_inputs_baseline_mi_min: 7.7184
    score_inputs_generated_mi_min: 20.0755
    score_inputs_ratio_g_over_b: 2.6009924336650085
baseline_metrics:
  performance:
    performance_suite_time_s: 2.781374
    performance_tests_total: 2
  resource:
    resource_suite_time_s: 2.854474
    resource_tests_total: 2
    avg_memory_mb: 55.98
    avg_cpu_percent: 100.6
  functional:
    functional_suite_time_s: 4.552087
    functional_tests_total: 11
  robustness:
    robustness_suite_time_s: 2.083828
    robustness_tests_total: 6
  security:
    security_suite_time_s: 1.286846
    security_tests_total: 1
    metrics:
      high_risk_count: 0.0
      files_scanned: 6.0
      total_loc: 1131.0
  maintainability:
    maintainability_suite_time_s: 1.311875
    maintainability_tests_total: 1
    metrics:
      mi_min: 7.7184
      files_scanned: 6.0
      total_loc: 1131.0
      max_cc: 16.0
pytest_logs_dir: D:\桌面\RealAppCodeBench_generic_eval\results\Dataset\pytest_logs

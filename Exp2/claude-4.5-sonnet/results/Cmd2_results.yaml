project_name: Cmd2
task_file: D:\桌面\RealAppCodeBench_generic_eval\tasks\Cmd2\cmd2.yaml
generated_repo: D:\桌面\RealAppCodeBench_generic_eval\generation\cmd2
timestamp: '2025-12-31 13:29:05'
functional_score: 0.7273
non_functional_score: 0.4
non_functional_subscores:
  maintainability: 0.0
  security: 1.0
  robustness: 1.0
  performance: 0.0
  resource: 0.0
non_functional_weights:
  maintainability: 0.36
  security: 0.24
  robustness: 0.16
  performance: 0.12
  resource: 0.12
results:
  functional:
    returncode: 1
    stdout: "..FF....F..                                                         \
      \     [100%]\n================================== FAILURES ===================================\n\
      _______________________ test_echo_arguments_and_parsing _______________________\n\
      \napp = <functional_test._make_app_class.<locals>.SimpleApp object at 0x000001FD7ADEE820>\n\
      \n    def test_echo_arguments_and_parsing(app: Optional[Any]) -> None:\n   \
      \     if not _require_app(app):\n            return\n        output = run_command(app,\
      \ \"echo_args one two three\")\n>       assert \"one two three\" in output\n\
      E       AssertionError: assert 'one two three' in '\\n'\n\ntests\\Cmd2\\functional_test.py:276:\
      \ AssertionError\n_______________________ test_echo_arguments_with_quotes _______________________\n\
      \napp = <functional_test._make_app_class.<locals>.SimpleApp object at 0x000001FD7ADE3490>\n\
      \n    def test_echo_arguments_with_quotes(app: Optional[Any]) -> None:\n   \
      \     if not _require_app(app):\n            return\n        output = run_command(app,\
      \ 'echo_args \"hello world\" two')\n>       assert \"hello world two\" in output\n\
      E       AssertionError: assert 'hello world two' in '\\n'\n\ntests\\Cmd2\\functional_test.py:283:\
      \ AssertionError\n_____________________ test_multiple_commands_and_history ______________________\n\
      \napp = <functional_test._make_app_class.<locals>.SimpleApp object at 0x000001FD7AD87490>\n\
      \n    def test_multiple_commands_and_history(app: Optional[Any]) -> None:\n\
      \        if not _require_app(app):\n            return\n        commands = [\"\
      greet Alice\", \"greet Bob\", \"history\"]\n        output = run_commands(app,\
      \ commands)\n>       assert \"Hello Alice\" in output\nE       AssertionError:\
      \ assert 'Hello Alice' in ''\n\ntests\\Cmd2\\functional_test.py:321: AssertionError\n\
      =========================== short test summary info ===========================\n\
      FAILED tests/Cmd2/functional_test.py::test_echo_arguments_and_parsing - Asser...\n\
      FAILED tests/Cmd2/functional_test.py::test_echo_arguments_with_quotes - Asser...\n\
      FAILED tests/Cmd2/functional_test.py::test_multiple_commands_and_history - As...\n\
      3 failed, 8 passed in 4.17s\n"
    elapsed_time_s: 5.712376
    avg_memory_mb: 32.0
    avg_cpu_percent: 99.7
    passed: 8
    failed: 3
    skipped: 0
    total: 11
    score_inputs_passed: 8
    score_inputs_failed: 3
    score_inputs_total: 11
    score_inputs_returncode: 1
    score_inputs_failed_suite: true
    score_inputs_baseline_keys:
    - functional_suite_time_s
    - functional_tests_total
  performance:
    returncode: 1
    stdout: "F                                                                   \
      \     [100%]\n================================== FAILURES ===================================\n\
      _______________________ test_bulk_commands_performance ________________________\n\
      \n    def test_bulk_commands_performance() -> None:\n        app = PerfApp()\n\
      \        commands = [\"ping\"] * 2000\n    \n        start = time.perf_counter()\n\
      >       output = run_commands(app, commands)\n\ntests\\Cmd2\\performance_test.py:53:\
      \ \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _\n\napp = <performance_test.PerfApp object at 0x0000027A6715BE80>\n\
      commands = ['ping', 'ping', 'ping', 'ping', 'ping', 'ping', ...]\n\n    def\
      \ run_commands(app: PerfApp, commands: list[str]) -> str:\n        buffer =\
      \ io.StringIO()\n        saved_stdout = app.stdout\n        app.stdout = buffer\n\
      \        try:\n>           app.runcmds_plus_hooks(commands)\nE           AttributeError:\
      \ 'PerfApp' object has no attribute 'runcmds_plus_hooks'\n\ntests\\Cmd2\\performance_test.py:42:\
      \ AttributeError\n=========================== short test summary info ===========================\n\
      FAILED tests/Cmd2/performance_test.py::test_bulk_commands_performance - Attri...\n\
      1 failed in 0.45s\n"
    elapsed_time_s: 2.077555
    avg_memory_mb: 31.57
    avg_cpu_percent: 96.8
    passed: 0
    failed: 1
    skipped: 0
    total: 1
    score_inputs_passed: 0
    score_inputs_failed: 1
    score_inputs_total: 1
    score_inputs_returncode: 1
    score_inputs_failed_suite: true
    score_inputs_baseline_keys:
    - performance_suite_time_s
    - performance_tests_total
    score_inputs_baseline_time_s: 1.911562
    score_inputs_actual_time_s: 2.077555
  resource:
    returncode: 1
    stdout: "F                                                                   \
      \     [100%]\n================================== FAILURES ===================================\n\
      _____________________ test_memory_usage_for_many_commands _____________________\n\
      \n    def test_memory_usage_for_many_commands() -> None:\n        app = ResourceApp()\n\
      \        commands = [\"noop\"] * 3000\n    \n        process = psutil.Process()\n\
      \        base_mem = process.memory_info().rss\n    \n>       run_commands(app,\
      \ commands)\n\ntests\\Cmd2\\resource_test.py:57: \n_ _ _ _ _ _ _ _ _ _ _ _ _\
      \ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\napp = <resource_test.ResourceApp\
      \ object at 0x000002110254D670>\ncommands = ['noop', 'noop', 'noop', 'noop',\
      \ 'noop', 'noop', ...]\n\n    def run_commands(app: ResourceApp, commands: list[str])\
      \ -> str:\n        buffer = io.StringIO()\n        saved_stdout = app.stdout\n\
      \        app.stdout = buffer\n        try:\n>           app.runcmds_plus_hooks(commands)\n\
      E           AttributeError: 'ResourceApp' object has no attribute 'runcmds_plus_hooks'\n\
      \ntests\\Cmd2\\resource_test.py:44: AttributeError\n===========================\
      \ short test summary info ===========================\nFAILED tests/Cmd2/resource_test.py::test_memory_usage_for_many_commands\
      \ - Att...\n1 failed in 0.48s\n"
    elapsed_time_s: 2.069665
    avg_memory_mb: 33.36
    avg_cpu_percent: 98.4
    passed: 0
    failed: 1
    skipped: 0
    total: 1
    score_inputs_passed: 0
    score_inputs_failed: 1
    score_inputs_total: 1
    score_inputs_returncode: 1
    score_inputs_failed_suite: true
    score_inputs_baseline_keys:
    - avg_cpu_percent
    - avg_memory_mb
    - resource_suite_time_s
    - resource_tests_total
    score_inputs_baseline_mem_mb: 44.14
    score_inputs_baseline_cpu_pct: 100.9
    score_inputs_actual_mem_mb: 33.36
    score_inputs_actual_cpu_pct: 98.4
  robustness:
    returncode: 0
    stdout: "...                                                                 \
      \     [100%]\n============================== warnings summary ===============================\n\
      tests\\Cmd2\\robustness_test.py:129\n  D:\\桌面\\RealAppCodeBench_generic_eval\\\
      tests\\Cmd2\\robustness_test.py:129: PytestUnknownMarkWarning: Unknown pytest.mark.timeout\
      \ - is this a typo?  You can register custom marks to avoid this warning - for\
      \ details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.timeout(10)\n\
      \ntests\\Cmd2\\robustness_test.py:148\n  D:\\桌面\\RealAppCodeBench_generic_eval\\\
      tests\\Cmd2\\robustness_test.py:148: PytestUnknownMarkWarning: Unknown pytest.mark.timeout\
      \ - is this a typo?  You can register custom marks to avoid this warning - for\
      \ details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.timeout(10)\n\
      \ntests\\Cmd2\\robustness_test.py:165\n  D:\\桌面\\RealAppCodeBench_generic_eval\\\
      tests\\Cmd2\\robustness_test.py:165: PytestUnknownMarkWarning: Unknown pytest.mark.timeout\
      \ - is this a typo?  You can register custom marks to avoid this warning - for\
      \ details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    @pytest.mark.timeout(10)\n\
      \n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n\
      3 passed, 3 warnings in 0.16s\n"
    elapsed_time_s: 1.698665
    avg_memory_mb: 30.91
    avg_cpu_percent: 99.1
    passed: 3
    failed: 0
    skipped: 0
    total: 3
    score_inputs_passed: 3
    score_inputs_failed: 0
    score_inputs_total: 3
    score_inputs_returncode: 0
    score_inputs_failed_suite: false
    score_inputs_baseline_keys:
    - robustness_suite_time_s
    - robustness_tests_total
  security:
    returncode: 0
    stdout: 'SECURITY_METRICS high_risk_count=0.0 files_scanned=4.0 total_loc=399.0

      .

      1 passed in 0.15s

      '
    elapsed_time_s: 1.700812
    avg_memory_mb: 31.55
    avg_cpu_percent: 99.1
    passed: 1
    failed: 0
    skipped: 0
    total: 1
    metrics:
      high_risk_count: 0.0
      files_scanned: 4.0
      total_loc: 399.0
    score_inputs_passed: 1
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 0
    score_inputs_failed_suite: false
    score_inputs_baseline_keys:
    - metrics
    - security_suite_time_s
    - security_tests_total
    score_inputs_baseline_high_risk_count: 3.0
    score_inputs_generated_high_risk_count: 0.0
  maintainability:
    returncode: 0
    stdout: 'MAINT_METRICS mi_min=19.3312 files_scanned=4.0 total_loc=399.0 max_cc=14.0

      .

      1 passed in 0.19s

      '
    elapsed_time_s: 1.726639
    avg_memory_mb: 31.71
    avg_cpu_percent: 100.0
    passed: 1
    failed: 0
    skipped: 0
    total: 1
    metrics:
      mi_min: 19.3312
      files_scanned: 4.0
      total_loc: 399.0
      max_cc: 14.0
    score_inputs_passed: 1
    score_inputs_failed: 0
    score_inputs_total: 1
    score_inputs_returncode: 0
    score_inputs_failed_suite: false
    score_inputs_baseline_keys:
    - maintainability_suite_time_s
    - maintainability_tests_total
    - metrics
    score_inputs_baseline_mi_min: 0.0
    score_inputs_generated_mi_min: 19.3312
baseline_metrics:
  performance:
    performance_suite_time_s: 1.911562
    performance_tests_total: 1
  resource:
    resource_suite_time_s: 1.91871
    resource_tests_total: 1
    avg_memory_mb: 44.14
    avg_cpu_percent: 100.9
  functional:
    functional_suite_time_s: 3.166834
    functional_tests_total: 11
  robustness:
    robustness_suite_time_s: 1.488449
    robustness_tests_total: 3
  security:
    security_suite_time_s: 1.400502
    security_tests_total: 1
    metrics:
      high_risk_count: 3.0
      files_scanned: 21.0
      total_loc: 9076.0
  maintainability:
    maintainability_suite_time_s: 1.998324
    maintainability_tests_total: 1
    metrics:
      mi_min: 0.0
      files_scanned: 21.0
      total_loc: 9076.0
      max_cc: 69.0
pytest_logs_dir: D:\桌面\RealAppCodeBench_generic_eval\results\Cmd2\pytest_logs
